\documentclass[10pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. McNamara}}
\chead{\textsc{SDS/MTH 291: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\DeclareMathOperator{\Ex}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\X}{\mathbf{X}}
\DeclareMathOperator{\Hatmat}{\mathbf{H}}

\rhead{\textsc{April 21, 2016}}

\begin{document}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Time Series 
  \item Moving Averages
  \item Autoregressive models
  \item ARIMA models
  \item Putting it all together
\end{enumerate}


\paragraph{Time Series}

A Time series is a set of variables measured over time. Some examples are census data taken at equally spaced intervals with a consistency in the method of measurement and the variables measured, meteorological measurements, utility supply and demand measurements.

Before we proceed with modeling any time series data we need to understand the mechanism generating the time series. These include describing any sources of variations, seasonality, trends, cycles inherent in a time series.

\emph{Trend} Trend is a long term pattern in the series that is either decreasing or increasing. It can have a "zig-zag" pattern for shorter time periods within the series.

\emph{Seasonality} Seasonality is a recurring pattern for each period e.g. annual seasonal patterns have a cycle that is 12 periods long, with each month being a period. If the periods are quarters then the annual seasonal pattern is 4 periods long. It is desirable to get an estimate of the seasonal index to assess seasonality in the dataset. A seasonal index measures the extent of seasonal influence for a particular segment of the year i.e. it compares the expected value of each period to the overall or grand mean. In particular, we are interested in estimating how much each value for a period is above or below the grand mean. 

Note: Most time series data from US government websites is deseasonalized i.e. trends to seasonality factors have been removed.

\emph{Cycle} These are patterns in the data which are not due to seasonality.

Decomposition analysis to separate out the Seasonal (S), Trend (T) and Cycle (C) components along with Irregularity (I) and remove their influence from the data (X) before analysis.

$$ X_{t} = S_{t} T_{t} C{t} I
$$

Moving Averages are smoothing techniques that can help discern the various components above in the data.

\paragraph{Moving Averages}
In classical time series, Trend-Cycles are removed using "moving averages". A simple moving average of order 'm" is defined as:

$$
    	\widehat{T_{t}} = \frac{1}{m}{\sum_{j=-k}^{k}} X_{t+j}
  	$$

Other moving averages methods include weighted moving averages, and other complex smoothing techniques.


\paragraph{Autoregressive models}

In an autoregressive model we use multivariate regression to model the variable using a linear combination of the variable values over past time periods. The model for order p is:

Consider the addition of an \emph{interaction} term:
  	$$
  		x_t = c + \phi_1 x_{t-1} + \phi_2 x_{t-2} + .....+\phi_{p} x_{t-p}+ \epsilon_{t}
  	$$

See Time series lab for example of AR(1) and AR(2) models and conditions.



\paragraph{Autoregressive moving average (ARIMA) Models}

ARIMA models are widely used to model time series data. 

Most forecasting methods in time series also require certain stationarity conditions. Here are some commonly desirable conditions:

\emph{First Order Stationary}: If the expected value of variable X(t) is the same for all t then it is said to be First Order Stationary. This is typically achieved by detrending the data using methods such as differencing.

\emph{Second Order Stationary}: If X(t) is first order stationary and the covariance between X(t) and X(s) is a function of the length (t-s) then is is Second Order Stationary. In econometrics this is commonly achieved by taking a transformation e.g. taking square root of the variable.

Taking differences of two consecutive observations is one approach to making the time series stationary.  Seasonal differencing is used when differences are taken over consecutive periods. More on differencing in R is illustrated in the lab handout.

Maximum likelihood estimation is the method of choice (you used this in logistic regression) as it is similar to the least squares regression estimate in that minimizes the sum of square residuals. Similar to logistic regression, here we find the parameter estimates that maximize the log likelihood. R uses AIC to find the "best" model. Corrected AIC along with details on estimation and order selection can be found in Section 8.6 of Forecasting: principles and practice

\paragraph{Putting it all together}
Lets consider the material we have covered throughout this course. We have tried to focus mostly on applied ideas, some of which are computational (more useful in practice, hard to test) and some which are slightly more theoretical (needed so you understand what you can apply, easier to test).

Lets try to categorize some of the tasks we have learned how to perform. Where in the CFAU process do these fit?
\\ Interpreting multiple regression coefficients, nested F-tests, added variable plots, stepwise regression, calculating Cook's distance, calculating Fisher's LSD, calculating Tukey's HSD, the Bonferroni correction, randomization tests, bootstrap methods, fitting ANOVA, rewriting ANOVA models in different ways, checking ANOVA p-values, fitting a logistic regression model, translating from log-odds to odds, translating from odds to probability, calculating odds ratios, looking at p-values on logistic regression, interpreting logistic regression coefficients, considering forms of models. 
\\
Choose
\vspace{1in}
\\
Fit
\vspace{1in}
\\
Assess
\vspace{1in}
\\
Use
\vspace{1in}

Because interpreting coefficients is so important for both your project and your test, lets practice writing out the "script" for a variety of cases. 

\begin{enumerate}
\itemsep0.5in
\item Multiple regression: categorical variable
\item Multiple regression: numeric variable
\item Multiple regression: interaction term
\item ANOVA $\alpha_i$
\item ANOVA $\mu_i$
\item Logistic regression: categorical variable
\item Logistic regression: numeric variable
\item Logistic regression: interaction term
\end{enumerate}

\end{document}
