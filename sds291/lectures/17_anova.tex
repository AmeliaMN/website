\documentclass[10pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. McNamara}}
\chead{\textsc{SDS/MTH 291: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\DeclareMathOperator{\Ex}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}
\DeclareMathOperator{\X}{\mathbf{X}}
\DeclareMathOperator{\Hatmat}{\mathbf{H}}

\rhead{\textsc{November 15, 2016}}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Randomization warmup
  \item ANOVA
  \item Fisher's LSD
\end{enumerate}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(knitr)}
\hlstd{opts_chunk}\hlopt{$}\hlkwd{set}\hlstd{(}\hlkwc{size}\hlstd{=}\hlstr{'footnotesize'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

  \paragraph{Randomization} With your neighbor, discuss how you could use randomization to determine if the $R^2$ value of a simple linear model was greater than 0. Write out at least three steps that would be required (perhaps these steps would need to be repeated, as well).
  \vspace{1.5in}


\paragraph{ANOVA}

Recall that in the regression models we have considered thus far, we have always had at least one quantitative explanatory variable. How do we handle a model with \emph{only} a categorical explanatory variable? This technique is called ANOVA, for analysis of variance, but it is equivalent to regression with just a categorical variable. 

An ANOVA model is \emph{phrased} differently than a regression model. It may also be helpful to think of ANOVA as a generalization of two-sample $t$-test. 

Consider the usual ANOVA model
  $$
    y_{ij} = \mu_i + \epsilon_{ij}, \text{ where } \epsilon_{ij} \sim N(0, \sigma)
  $$
  for groups $i = 1,\ldots, I$ and individuals $j=1,\ldots,n_i$, with common standard deviation $\sigma$.
  
If we let $\mu = $ grand mean and $\alpha_i =$ the $i^{th}$ group effect, then we can write this for each of the $i=1,\ldots,I$ groups as
$$
    y_{ij} = \mu + \alpha_i + \epsilon_{ij}, \text{ where } \epsilon_{ij} \sim N(0, \sigma)
$$
Note that all we have done is to decompose the mean of the $i^{th}$ group ($\mu_i)$ into two parts: the grand mean ($\mu$), and the difference bewteen the mean of the $i^{th}$ group and the grand mean ($\alpha_i$). 

If we move $\mu$ to the left side of the equation, we get
$$
    y_{ij} - \mu = \alpha_i + \epsilon_{ij}\,,
$$
and now summing over $i$ and $j$ gives
$$
  		SST = SSG + SSE
$$
ANOVA gives us a way to test for the statistical significance of group means. The null hypothesis is $H_0: \alpha_1 = \alpha_2 = \cdots = \alpha_I = 0$ -- that is, all of the groups effects are in fact zero, and thus the group means are the same. The alternative hypothesis is that $H_A: \exists \,i$ s.t. $\alpha_i \neq 0$ -- that is, at least one of the group effects is not zero. We can't tell which one. 
\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(mosaic)}
\hlkwd{require}\hlstd{(Stat2Data)}

\hlkwd{data}\hlstd{(ThreeCars)}
\hlstd{a1} \hlkwb{=} \hlkwd{aov}\hlstd{(Price} \hlopt{~} \hlstd{CarType,} \hlkwc{data}\hlstd{=ThreeCars)}
\hlkwd{model.tables}\hlstd{(a1)}
\end{alltt}
\begin{verbatim}
## Tables of effects
## 
##  CarType 
## CarType
##     BMW  Jaguar Porsche 
##  -7.342  -5.619  12.961
\end{verbatim}
\end{kframe}
\end{knitrout}

These are the $\alpha_i$'s. The grand mean $\mu$ is:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{mean}\hlstd{(}\hlopt{~}\hlstd{Price,} \hlkwc{data}\hlstd{=ThreeCars)}
\end{alltt}
\begin{verbatim}
## [1] 37.57556
\end{verbatim}
\end{kframe}
\end{knitrout}

and the group means $\mu_i$ are:

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{mean}\hlstd{(Price} \hlopt{~} \hlstd{CarType,} \hlkwc{data}\hlstd{=ThreeCars)}
\end{alltt}
\begin{verbatim}
##      BMW   Jaguar  Porsche 
## 30.23333 31.95667 50.53667
\end{verbatim}
\end{kframe}
\end{knitrout}

\paragraph{Conditions for ANOVA}

The conditions are the same as for regression (minus Linearity), including \emph{equal variance among groups}. 

% Note that factors are evaluated as a group via $F$-test
    
To actually assess the Equal Variance condition among groups:
		\begin{itemize}
			\item Check residuals vs. fitted plot for similar spread across groups
			\item Check standard deviations among groups
			\item Check if $sd_{max} / sd_{min} <= 2$
		\end{itemize}
%		\item Importance on Randomization
%		\item Randomized Experiments allow causal inference
%		\item Observation studies permit limited inferences about populations

\paragraph{Equivalence of ANOVA and Regression}
Recall that one-way ANOVA is just a rephrasing of regression with a quantitative response variable and a single categorical explanatory variable. 

Let $X_i$ be the indicator (binary) variable corresponding to the $i^{th}$ group. Let $\mu_I$ be the overall mean of the $I^{th}$ group, and note that since every observation $y_{ij}$ has to be in some group, if it isn't in the any of the first $I-1$ groups, it has to be in group $I$. Call this the \emph{reference group}, and set $\mu_i = \mu_I + \beta_i \cdot X_i$. 

Then the ANOVA model above is equivalent to:
  $$
    y_{ij} = \mu_I + \beta_i \cdot X_i + \epsilon_{ij}, \text{ where } \epsilon_{ij} \sim N(0, \sigma)
  $$
  for $i=1,\ldots,I-1$. 
  
This is exactly what happens when you compute {\tt lm(y $\sim$ x)} in R, with {\tt x} being a categorical variable! The main difference is that now $\beta_i$ represents the size of the effect of being in group $i$, \emph{relative to group $I$}, whereas $\alpha_i$ represents the size of the effect of being in group $i$, \emph{relative to the grand mean}. 

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m1} \hlkwb{=} \hlkwd{lm}\hlstd{(Price} \hlopt{~} \hlstd{CarType,} \hlkwc{data}\hlstd{=ThreeCars)}
\hlkwd{coef}\hlstd{(m1)}
\end{alltt}
\begin{verbatim}
##    (Intercept)  CarTypeJaguar CarTypePorsche 
##      30.233333       1.723333      20.303333
\end{verbatim}
\end{kframe}
\end{knitrout}


The pooled standard deviation $s_p$, a weighted average of the standard deviations of the groups, is an estimate of $\sigma$, the unknown common standard deviation. This equal to the residual standard error. 

Note that the values predicted by both models are exactly the same!

\begin{knitrout}\footnotesize
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{sum}\hlstd{(}\hlkwd{predict}\hlstd{(a1)} \hlopt{-} \hlkwd{predict}\hlstd{(m1))}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\end{kframe}
\end{knitrout}

% Note also that it is clear that
% \begin{eqnarray*}
%   y_{ij} - \epsilon_{ij} &=& \mu_i \\
%   &=& \mu + \alpha_i = \\
%   \beta_0 + \beta_i &\Rightarrow& \mu + \alpha_i = \\
%   \beta_0 + \beta_i &\nRightarrow& \mu =\\
%   \beta_0, \alpha_i &=& \beta_i
% \end{eqnarray*}

So, there are three ways to express (and interpret!) the same mode: 
\begin{eqnarray*}
     y_{ij} &=& \mu_i + \epsilon_{ij}  \\
     y_{ij} &=& \mu + \alpha_i + \epsilon_{ij} \\
     y_{ij} &=& \mu_I + \beta_i \cdot X_i + \epsilon_{ij}
\end{eqnarray*}
\paragraph{Write out the three variations on the model} For this example using car types to predict price, write out the three models.
\vspace{3in}



\paragraph{Multiple Comparisons}
Once we have performed ANOVA, we often know that \emph{at least one} of our groups has a significantly different mean. Then, we often want to know \emph{which one}. This can lead to the problem of multiple comparisons. 
	\begin{itemize}
		\item Individual Error Rate (Type I error) vs. Family-wise error rate
		\begin{itemize}
			\item Individual Type I error: one specific false rejection of null hypothesis
			\item Family-wise Type I error: at least one false rejection of null hypothesis
		\end{itemize}
		\item Even when the probability of Type I error is low, if you are making many comparisons, then the probability of a family-wise Type I error is \emph{much} higher
		\item Recall the \href{https://xkcd.com/882/}{jelly beans comic} from xkcd
		\item Corrections for \emph{multiple comparisons} include:
		\begin{itemize}
			\item Bonferroni: divide the $\alpha$-level by the number of comparisons
			\item Fisher's LSD
			\item Tukey's HSD
		\end{itemize}
		\item These differ only in the choice of the critical value
	\end{itemize}

\paragraph{Corrections for Multiple Comparisons}
	\begin{itemize}
		\item Fisher's Least Significant Difference
		\begin{enumerate}
			\item Perform ANOVA
			\item If not significant, stop
			\item Compute the pairwise comparisons using the confidence interval
			$$
				\bar{y}_i - \bar{y}_j \pm t^* \sqrt{MSE \left( \frac{1}{n_i} + \frac{1}{n_j} \right) }
			$$	
		\end{enumerate}
		\item Fisher's LSD: $t^*$ chosen according to $\alpha$ and $n-K$ d.f.
		\item Bonferroni: $t^*$ chosen according to $\alpha/m$ and $n-K$ d.f., where $m = \binom{K}{2}$
		\item Tukey's Honest Significant Difference: critical value $=\frac{q}{\sqrt{2}}$, where $q$ depends on \emph{studentized range distribution}
	\end{itemize}


\end{document}
