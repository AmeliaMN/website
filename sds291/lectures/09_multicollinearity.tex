\documentclass[10pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. McNamara}}
\chead{\textsc{SDS/MTH 291: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\DeclareMathOperator{\Ex}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}

\rhead{\textsc{October 18, 2016}}

\usepackage{Sweave}
\begin{document}
\input{09_multicollinearity-concordance}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Multicollinearity and variance inflation factor
  \item More examples of multiple regression
  \item Regression summary lab?
\end{enumerate}

\paragraph{Multicollinearity}

Sometimes explanatory variables are highly correlated. This can cause oddities in regression output, since the effect of one variable may be confounded by another with which it is highly correlated.

Lets consider an example. The predictors \texttt{read} and \texttt{write} are both highly correlated with \texttt{math}. But, they are also correlated with each other.




\begin{Schunk}
\begin{Sinput}
> m2 <- lm(math~read+write, data=hsb2)
> summary(m2)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = math ~ read + write, data = hsb2)

Residuals:
     Min       1Q   Median       3Q      Max 
-20.8478  -4.6996   0.1016   4.4756  16.0483 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 12.86507    2.82162   4.559 9.00e-06 ***
read         0.41695    0.05648   7.382 4.29e-12 ***
write        0.34112    0.06110   5.583 7.76e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.555 on 197 degrees of freedom
Multiple R-squared:  0.5153,	Adjusted R-squared:  0.5104 
F-statistic: 104.7 on 2 and 197 DF,  p-value: < 2.2e-16
\end{Soutput}
\begin{Sinput}
> m3 <- lm(math~read+write+read*write, data=hsb2)
> summary(m3)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = math ~ read + write + read * write, data = hsb2)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.463  -4.376  -0.280   4.464  16.059 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)   
(Intercept) 41.003933  14.390128   2.849  0.00485 **
read        -0.164643   0.297075  -0.554  0.58006   
write       -0.183184   0.269902  -0.679  0.49813   
read:write   0.010628   0.005331   1.994  0.04759 * 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 6.506 on 196 degrees of freedom
Multiple R-squared:  0.5249,	Adjusted R-squared:  0.5177 
F-statistic: 72.19 on 3 and 196 DF,  p-value: < 2.2e-16
\end{Soutput}
\end{Schunk}

\begin{enumerate}
  \itemsep0.5in
  \item What happens if we include their interaction term in a model?
\end{enumerate}

\paragraph{Variance inflation factor}
Geometrically, if two vectors are strongly correlated, then they point more or less in the same direction, and the plane through those vectors will be wobbly.

How do we know if we have multicollinearity? Define
 		$$
 			VIF_i = \frac{1}{1-R_i^2},
 		$$
 		where $R_i^2$ is the $R^2$ for a regression of $X_i \sim \sum_{j \neq i} X_j$. A common rule of thumb is that $VIF_i > 5 \rightarrow R_i^2 > 0.8$ implies multicollinearity.

Remedies:
 		\begin{enumerate}
 			\item Drop some predictors
 			\item Combine some predictors (e.g. survey questions)
 			\item Discount the coefficient $t$-tests
 		\end{enumerate}


\begin{Schunk}
\begin{Sinput}
> require(car)
> Credit <- read.csv("Credit.csv")
> m4 <- lm(Balance~Age+Rating+Limit, data=Credit)
> summary(m4)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = Balance ~ Age + Rating + Limit, data = Credit)

Residuals:
    Min      1Q  Median      3Q     Max 
-729.67 -135.82   -8.58  127.29  827.65 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -259.51752   55.88219  -4.644 4.66e-06 ***
Age           -2.34575    0.66861  -3.508 0.000503 ***
Rating         2.31046    0.93953   2.459 0.014352 *  
Limit          0.01901    0.06296   0.302 0.762830    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 229.1 on 396 degrees of freedom
Multiple R-squared:  0.7536,	Adjusted R-squared:  0.7517 
F-statistic: 403.7 on 3 and 396 DF,  p-value: < 2.2e-16
\end{Soutput}
\begin{Sinput}
> vif(m4)
\end{Sinput}
\begin{Soutput}
       Age     Rating      Limit 
  1.011385 160.668301 160.592880 
\end{Soutput}
\begin{Sinput}
> Credit %>%
+   select(Age, Rating, Limit, Balance) %>%
+   cor()
\end{Sinput}
\begin{Soutput}
                Age    Rating     Limit     Balance
Age     1.000000000 0.1031650 0.1008879 0.001835119
Rating  0.103164996 1.0000000 0.9968797 0.863625161
Limit   0.100887922 0.9968797 1.0000000 0.861697267
Balance 0.001835119 0.8636252 0.8616973 1.000000000
\end{Soutput}
\begin{Sinput}
> # cor(Credit[,c("Age", "Rating", "Limit", "Balance")]) #this also works
\end{Sinput}
\end{Schunk}

\begin{enumerate}
  \itemsep0.5in
  \item Which variables are the most highly correlated?
  \item Which terms in the model have the highest VIF?
  \item Which term(s) would you drop from the model to try again?
  \vspace{0.5in}
\end{enumerate}

\paragraph{Scales of variables}
The scale of variables makes a difference to your model interpretation. 

\begin{Schunk}
\begin{Sinput}
> require(mosaic)
> data(Salaries)
> head(Salaries)
\end{Sinput}
\begin{Soutput}
       rank discipline yrs.since.phd yrs.service  sex salary
1      Prof          B            19          18 Male 139750
2      Prof          B            20          16 Male 173200
3  AsstProf          B             4           3 Male  79750
4      Prof          B            45          39 Male 115000
5      Prof          B            40          41 Male 141500
6 AssocProf          B             6           6 Male  97000
\end{Soutput}
\begin{Sinput}
> m1 <- lm(yrs.service~yrs.since.phd + salary, data=Salaries)
> summary(m1)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = yrs.service ~ yrs.since.phd + salary, data = Salaries)

Residuals:
     Min       1Q   Median       3Q      Max 
-22.6297  -2.2685   0.8793   3.7076  19.1558 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -6.445e-01  1.050e+00  -0.614   0.5398    
yrs.since.phd  9.420e-01  2.308e-02  40.806   <2e-16 ***
salary        -2.428e-05  9.822e-06  -2.472   0.0138 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.375 on 394 degrees of freedom
Multiple R-squared:  0.8301,	Adjusted R-squared:  0.8292 
F-statistic: 962.5 on 2 and 394 DF,  p-value: < 2.2e-16
\end{Soutput}
\end{Schunk}

\begin{enumerate}
  \itemsep0.5in
  \item Write out the regression equation, paying attention to the scale of the variables. 
  \item Interpret the coefficient on \verb#salary#
  \item Does this model make intuitive sense?
  \item Predict the number of years of service the model would expect for a professor 5 years out of their PhD making \$80,000. 
  \vspace{0.5in}
\end{enumerate}

\begin{Schunk}
\begin{Sinput}
> Salaries = Salaries %>%
+   mutate(salaryThou = salary/1000)
> head(Salaries)
\end{Sinput}
\begin{Soutput}
       rank discipline yrs.since.phd yrs.service  sex salary salaryThou
1      Prof          B            19          18 Male 139750     139.75
2      Prof          B            20          16 Male 173200     173.20
3  AsstProf          B             4           3 Male  79750      79.75
4      Prof          B            45          39 Male 115000     115.00
5      Prof          B            40          41 Male 141500     141.50
6 AssocProf          B             6           6 Male  97000      97.00
\end{Soutput}
\begin{Sinput}
> m2 <- lm(yrs.service~yrs.since.phd + salaryThou, data=Salaries)
> summary(m2)
\end{Sinput}
\begin{Soutput}
Call:
lm(formula = yrs.service ~ yrs.since.phd + salaryThou, data = Salaries)

Residuals:
     Min       1Q   Median       3Q      Max 
-22.6297  -2.2685   0.8793   3.7076  19.1558 

Coefficients:
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -0.644528   1.050220  -0.614   0.5398    
yrs.since.phd  0.941976   0.023084  40.806   <2e-16 ***
salaryThou    -0.024281   0.009822  -2.472   0.0138 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 5.375 on 394 degrees of freedom
Multiple R-squared:  0.8301,	Adjusted R-squared:  0.8292 
F-statistic: 962.5 on 2 and 394 DF,  p-value: < 2.2e-16
\end{Soutput}
\end{Schunk}

\begin{enumerate}
  \itemsep0.5in
  \item Write out the regression equation, paying attention to the scale of the variables. 
  \item Interpret the coefficient on \verb#salaryThou#
  \item Predict the number of years of service the model would expect for a professor 5 years out of their PhD making \$80,000. 
  \item How do the p-values and predictions compare to the unscaled version?
\end{enumerate}




\end{document}
