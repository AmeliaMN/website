\documentclass[10pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. McNamara}}
\chead{\textsc{SDS/MTH 291: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\DeclareMathOperator{\Ex}{\mathbb{E}}
\DeclareMathOperator{\Var}{\text{Var}}

\rhead{\textsc{September 20, 2016}}

\begin{document}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Interpreting simple linear regression coefficients
  \item Residuals
  \item Conditions for regression
  \item Outliers-- leverage and influence
\end{enumerate}


% \paragraph{Predictions}
% 
% Use the function \texttt{predict} to use your model to make predictions for new observations. 

\paragraph{Interpreting coefficients}
Let's talk about the homework questions due last night. 

<<message=FALSE, warning=FALSE>>=
require(mosaic)
require(Stat2Data)
data(Cereal)
m1 <- lm(Calories~Sugar, data=Cereal)
coef(m1)
@

How can we interpret the intercept in this model? The slope?
\vspace{1in}

\paragraph{Residuals}
  \begin{itemize}
		\item Residuals: $y - \hat{y}$
		$$
			SSE = \sum_{i=1}^n (y - \hat{y})^2, \qquad \hat{\sigma}_\epsilon = \sqrt{\frac{SSE}{n-2}}
		$$
		\item Least Squares: technique for minimizing SSE
		\item Finds \emph{unique} straight line between scatterplot of points
	\end{itemize}
	




\paragraph{Conditions for regression}

\begin{itemize}
  \itemsep0.75in
  \item \textbf{L}inearity:
  \item \textbf{I}ndependence:
  \item \textbf{N}ormality of Residuals:
  \item \textbf{E}qual Variance of Residuals:
  \vspace{0.5in}
\end{itemize}

The LINE conditions are the same as the conditions from the book, just differently stated. Lets look at some examples of the conditions failing to be met in the residuals lab. 

\paragraph{Lab}
Some code from the residuals lab
<<eval=FALSE>>=
classdata %>% slice(12)
classdata %>% slice(10)

mod = lm(num_languages~num_countries, data=classdata)
summary(mod)
xyplot(num_languages~num_countries, data=classdata, type=c("p", "r"))

new_ds = data.frame(num_countries = c(3, 11))
predict(mod, newdata = new_ds)
1 - 2
4 - 2

residuals(mod)
sum(residuals(mod)^2) 
mean(residuals(mod))

plot(mod, which=c(1,2))
histogram(~residuals, data=mod, fit="normal")
@

\paragraph{Transformations}
What if the assumptions for regression are not met? Apply transformations! This process is as much art as science... The most common transformation to apply is a log transformation. 

\paragraph{Leverage and influence} Not all outliers have high leverage!

% \paragraph{Outliers}
% What about points with large residuals? How large? Recall that if $X \sim N(\mu, \sigma)$, then $Z = \left(X - \mu \right)/\sigma$ follows a $N(0,1)$ distribution (e.g. a standard normal distribution). Thus, we define the \emph{standardized} residuals:
% 		$$
% 			e^{standard}_i = \frac{e_i}{\hat{\sigma}_\epsilon} = \frac{y_i - \hat{y}_i}{\hat{\sigma}_\epsilon}, \qquad \text{ note } \bar{e} = 0
% 		$$
% 
% Recall the 68-95-99.7 Rule! Any standardized residuals larger than 3 may be considered outliers. 
% 
% 
% \paragraph{Leverage}
% 
% Points that have extreme $x$ values can have a disproportionate influence on the slope of the regression line. More on this in Chapter 4.3.


% \newpage
% 
% \section{Instructor's Notes}
% 
% 
% \paragraph{Conditions}
%   	Assumptions made in SLR
% 	\begin{enumerate}
% 		\item Linearity: the two quantities are linearly related
% 		\item Zero mean residuals: guaranteed by OLS
% 		\item Constant variance of residuals: homoskedasticity
% 		\item Independence
% 	\end{enumerate}
% 	For inference
% 	\begin{enumerate}
% 		\item Randomness
% 		\item Normality
% 	\end{enumerate}
% 
% 
% \paragraph{Assessing Conditions}
% 	\begin{itemize}
% 		\item Residuals vs. fitted plots
% 		\begin{itemize}
% 			\item Show Linearity \& Constant Variance
% 		\end{itemize}
% 		\item Normal plots
% 		\begin{itemize}
% 			\item Shows Normality
% 		\end{itemize}
% 	\end{itemize}
% 	
% 	
% 	\paragraph{Transformations}
% 	\begin{itemize}
% 		\item What do to if assumptions are not met?
% 		\item Non-linear relationship?
% 		\item Non-constant variance?
% 		\item Non-normality?
% 		\item Apply transformations! As much art as science
% 		\item Do \verb#MetroHealth83# and \verb#SpeciesArea# examples from book
% 	\end{itemize}
% 
% 
% \paragraph{Outliers}
% 	\begin{itemize}
% 		\item What about points with large residuals? How large?
% 		\item Standardized residuals
% 		$$
% 			rstandard(y) = \frac{r}{\hat{\sigma}_\epsilon} = \frac{\hat{y} - y}{\hat{\sigma}_\epsilon}, \qquad \text{ note } \bar{r} = 0
% 		$$
% 		\item Studentized (deleted-$t$)
% 		\begin{itemize}
% 			\item Remove the $i^{th}$ point and do regression again
% 		\end{itemize}
% 		\item Recall the 68-95-99.7 Rule!
% 		\item Do demo to compute these
% 	\end{itemize}
% 
% 
% \paragraph{Leverage}
% 	\begin{itemize}
% 		\item Points that have extreme $x$ values can have a disproportionate influence on the slope of the regression line
% 		\item \url{http://www.rob-mcculloch.org/teachingApplets/Leverage/index.html}
% 	\end{itemize}




\end{document}
