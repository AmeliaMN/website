[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Recent posts",
    "section": "",
    "text": "Guidelines I follow when giving talks\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2020\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nDeleting Facebook\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2019\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nKey Attributes of a Modern Statistical Computing Tool\n\n\n\n\n\n\n\npapers\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2019\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nA tidy dress\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2019\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nWikipedia in the classroom: Gender, argh\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2018\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nDagstuhl reflections\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nMore guys\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nScientists Programming\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nR syntax comparison\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2018\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nOn microaggressions\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2017\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nWhat are the chances my name is Amanda?\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2017\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nCensus data: A rant\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nStatistics graduate school advice\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nWorth adding to your inbox\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nOpenVisConf talk transcript\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nDo you know Nothing when you see it?\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s wrong with being data-collecting pigeons?\n\n\n\n\n\n\n\ndata\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2016\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nContextual notes\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2015\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\n20th New England Isolated Statisticians Meeting (NEISM)\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2015\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nFitbit colors\n\n\n\n\n\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nJan 14, 2015\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nFabric posters\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nNew tools for data analysis and journalism\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nKid Pix\n\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nData Science LA\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nICOTS discussion on technology in statistics education\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nYes, and…\n\n\n\n\n\n\n\nteaching\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nCode + Ed\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nJul 22, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nSweater variation; or grey != black and white\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nThat paper I was telling you to read\n\n\n\n\n\n\n\npapers\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nRepping Hadley at DataFest\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nThoughts from NICAR 2014: process and content\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nThoughts from NICAR 2014: data, where does it come from?\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nMigrated posts\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nThoughts from NICAR 2014: taking action\n\n\n\n\n\n\n\nconferences\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2014\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nProductivity\n\n\n\n\n\n\n\ntools\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2013\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nWhat’s the shashtistic on me?\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2012\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nThe Postal Service is into data visualization\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2012\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nBad words\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2012\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nStochastic as shit\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2010\n\n\nAmelia McNamara\n\n\n\n\n\n\n  \n\n\n\n\nA counting problem\n\n\n\n\n\n\n\nmisc\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2010\n\n\nAmelia McNamara\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Here are some links to selected presentations I’ve given and workshops I’ve led. For a more complete list, see my CV.\n\nPresentations\nComputing in the Statistics Curriculum: Lessons Learned from the Educational Sciences, with Andrew Zieffler, Matthew Beckman, Chelsey Legacy, Elle Butler Basner, Robert delMas, & V.N. Vimal Rao. JSM 2021, virtual. (slides, video)\nSpatial Gerrymandering: Can it be Avoided?, September 2020. Thinking Spatially: Mapping Politics and Polarization, virtual. (slides)\nHow (not) to bring Wikipedia into the classroom, August 2020. JSM 2020, virtual. (slides)\nSpeaking R (keynote), July 2020. useR! 2020, virtual. (slides, video).\nMaking a Tidy Dress, January 2020. rstudio::conf, San Francisco, CA. (slides, video)\nWhy the Tidyverse? (keynote), August 2019. noRth conference, Minneapolis, MN, (slides)\nData Science as a Superpower (plenary), July 2019. QUBES Evolution of Data in the Classroom: From Data to Data Science, Williamsburg, VA. (slides)\nData Journalism as a Liberal Art (keynote), June 2019. LACOL Data Science in the Liberal Arts. (slides)\nTeaching Data Communication, May 2019. SDSS (slides)\nWrangling Categorical Data in R Without Losing your Mind, January 2019. rstudio::conf (slides, video)\nHow Spatial Polygons Shape Our World: Geometry, Data, and Perceptions of Truth, September 2018. University of Minnesota Institute for Advanced Study Minneapolis, MN. (slides, video)\nHow Software Affects Humans’ Conceptions of Data: A Case Study in R Syntaxes, August 2018. JSM 2018, Vancouver BC. (slides)\nImagining the future of statistical education software, July 2018. ICOTS 10, Kyoto, Japan. (slides)\nMaking spatial aggregation more transparent, April 2018. 2018 CGA Conference: Illuminating Space and Time in Data Science, Harvard University, Cambridge, MA. (slides)\nData Science as a Superpower, April 2018. Introduction to Data Science Big Data Day, Los Angeles, CA. (slides)\nScientists Programming, February 2018. Evidence about Programmers for Programming Language Design. Dagstuhl Seminar, Wadern, Germany. (slides, writeup)\nAlgorithmic Accountability, November 2017. Smith College Otelia Cromwell Day, Northampton, MA. (slides, video)\nInterfacing with Data (plenary), September 2017, Data Literacy Conference, Aix-en-Provence, France. (slides)\nHow Spatial Polygons Shape Our World, April 2017, OpenVisConf, Boston, MA. ( video, slides and links)\nIntegrated R labs for high school students, useR 2016, Stanford University, Stanford, CA. (video, slides and resources)\nThe gap between tools for learning and for doing statistics. May 2016, Electronic Conference on Teaching Statistics (eCOTS). (video and slides)\nDo you know Nothing when you see it? OpenVisConf April 2016, Boston MA. (video links, transcript)\nEyeo Highlights. Datavis LA Meetup. August 2014, Los Angeles CA. (video and slides)\nTeaching Data Science to Teenagers (with M. Hansen). International Conference on Teaching Statistics. July 2014, Flagstaff, AZ. (slides)\nTeaching R to High School Students (with J. Molyneux). useR! The R User Conference. July 2014, Los Angeles, CA. (slides)\n\n\nWorkshops\nExploring the Tidyverse in R, with Aaron Kessler. NICAR March 2020, New Orleans, LA. (materials)\nIntroduction to Data Science in the Tidyverse, with Hadley Wickham. rstudio::conf. January 2020, San Francisco, CA. (materials)\nData Carpentry, with Kelsey Gonzalez. Bryn Mawr College. January 2020, Bryn Mawr, PA. (materials)\nData Science in the Tidyverse, with Hadley Wickham. rstudio::conf. January 2019, Austin, TX. (materials)\nIntro to R & RStudio, January 2018. rstudio::conf, San Diego, CA. (materials)\nData Science for Statisticians, March 2017, ENAR, Washington, DC. (materials)\nIntroduction to R. With Coulter Jones. NICAR. March 2016, Denver CO. (code)"
  },
  {
    "objectID": "blog/2016-05-16-OpenVisConfTranscript/index.html",
    "href": "blog/2016-05-16-OpenVisConfTranscript/index.html",
    "title": "OpenVisConf talk transcript",
    "section": "",
    "text": "During OpenVisConf, I hurriedly posted some links from my talk so people could follow up with resources I had mentioned. I kept meaning to write up a fuller blog post, but of course life comes along (in the interim, Smith has finished finals, I’ve graded a ton of projects, gone out of the state twice, and attended commencement).\nSo, when the official transcripts from the conference were posted, I knew I’d struck gold. Amanda Lundberg was the transcriptionist, and she did a fantastic job. The transcripts are provided with a creative commons license, so I grabbed the text from my talk, edited it a bit, and stuck in some images.\nIt was quite interesting to read the transcripts as-is. I know Amanda edited out a lot of “um”s and “uh”s as everyone was speaking, but a person’s verbal tics still get through. In my case, I say “that”, “so”, and “really” a LOT. Some of those have been removed here for ease of reading.\nOn a more interesting note, the organizers used TF-IDF to identify unique words and bi-grams from each talk. Mine make it pretty clear I’m a statistician. Words like “distribution”, “statistic”, “observed difference” and “bootstrap” all appeared more in my talk than others.\n\n\n\ntf-idf\n\n\n\nTalk\nProbably the easiest way to tell what the heck I’m talking about is to watch the video of my talk.\n\n\nBut, if you’re video-resistant (I usually am) this will probably give you the flavor.\n\n\nTranscript:\nThe title of my talk is “Do you know Nothing when you see it?”.\n\n\n\n\n\nWhen I say nothing, I don’t mean nothing like television static, because that’s actually not nothing, that’s something. It’s the image, it just got garbled when it was going through the air. I mean something more like this,\n\n\n\nrandom noise\n\n\nbut not really even this, because this is random noise that I generated with my computer. It’s pseudo-random, and probably your awesome human brain is finding patterns like DeepDream already. You can see the bunnies and hearts and clouds in my random data.\nOur goal is to be able to identify when something we created is nothing. And in order to do that, I’m going try statistics.\n\n\n\nStand back, I’m going to try… statistics\n\n\nI am a statistics professor. That means whenever I get in front of a group of people, I try to sneakily teach them some statistics.\nThe question is, what is statistics about? And it’s about many things. It’s about variation, which is one of the main themes we’re going to talk about here. Sometimes there’s modeling– I’m going to ignore modeling almost completely. And when you take an intro stat class or if you’re doing basic science, you’re often asking are these two numbers different? Another way of saying is, is this one number, the difference, different than zero? Sometimes you want to know, this number, what are some other reasonable numbers we could have seen if we hadn’t seen this one?\nIn order to answer those questions, we need context about the variation. The first example that came to mind when I was preparing this talk was ants. I don’t know why. Imagine you had some ants, and you had some big-looking ants and little-looking ants.\n\n\n\nBig ant\n\n\n\n\n\nSmaller ants\n\n\nAnd you thought maybe I got unlucky– I got five big-looking ants and five little-looking ants, but really they’re both draws from the same population. And overall the groups don’t have any difference in size. But, you observed differences in the group sizes of three-quarters of an inch.\nYou and I know something about ant size variation. That sounds very large, and we’ll see that it actually was.\nIn another context, imagine you had some a tug of war game set up, you had two teams. There’s men and women split up into the blue and the pink team.\n\n\n\nBlue team\n\n\n\n\n\nPink team\n\n\nWe mapped their heights, we found the average height, and we learned the average height between the groups was different. Again, we observed a difference of about three quarters of an inch.\nWe want to know, is that significant? Are those two teams really different, or did we just observe a difference by nature of the selection process? The random generation we’re assuming is happening behind the scenes?\nIf you’ve taken a standard statistics class, you’re now thinking about confidence intervals. You’re thinking “point estimate, plus or minus the standard error times something that has to do with some distribution”.\n\n\n\npoint estimate, plus or minus the standard error times something that has to do with some distribution\n\n\nAnd maybe you’re thinking about some pages in your textbook that had standard error calculations, with a bunch of square roots and fractions.\n\n\n\nStandard error forumas\n\n\nWe’re thinking about a difference of means. So, we think, we can just pick that first formula. But really, it’s worse than that. There’s all these different proofs about if you have different group sizes, and if the variances are different.\n\n\n\nStandard error formulas\n\n\nSo then you need to pick that formula out. And then it is worse than that, because once you have the standard error calculation, you have to know the degrees of freedom you’re going to look at in your distribution.\n\n\n\nStandard error formulas\n\n\nAnd even I– I have a Ph.D. in statistics– I get the heebie-jeebies if you ask me to come up with the right standard error computation. Maybe you’re better at statistics than me, and came up with the right standard error, found the degrees of freedom. Now you’re going to look at some idealized distribution.\n\n\n\nT distribution\n\n\nWe call this a sampling distribution. It’s not a distribution of data, it’s a distribution of statistics. Statistics are numbers computed from other numbers. In this case we had the average heights of the two tug of war teams and we’re looking at their difference.\nWith your standard error and your degrees of freedom and the distribution, you could come up with some confidence interval and you could see if there was really no relationship between the team and the height, how often would we observe a difference of three-quarters of an inch?\nI’m going to argue that’s not the way you should do that problem. Instead, you should use randomization. And randomization is just what it sounds. You have two things you think might have a relationship. You want to come up with the sampling distribution, and you want it to be the null distribution. The distribution of, essentially, nothing.\nSo what you’re going to do is you’re going to take the values, the labels here, and you’re going to mix them up. You’re going to compute the group height means for those different groups in the mixed-up data and compute the difference in the heights.\nSo you can see already sometimes you’re getting a positive difference, sometimes you’re getting a negative difference. I think one of those turned out to be zero. We’re going to do this like a thousand times and then we can look at the distribution of that statistic.\n\n\n\nRandomization distribution\n\n\nThis distribution is centered around zero because it’s a null distribution. And then we can calculate where is 95% of the data? What’s the middle 95%? And then sort of say “if there really was no difference between the heights of these two tug of war teams, what sorts of differences might we observe?”\nAnd so in the case of the tug of war teams, if there was really no difference, we could observe height differences that were negative four inches to positive four inches different. Our observed difference of three quarters of an inch is so tiny we think it is nothing. Even though we saw a difference of three-quarters of an inch, that was nothing.\nFor the ants, same thing, but the distribution looks different.\n\n\n\nRandomization distribution\n\n\nRandomization distributions are not always symmetric. And they’re not always smooth. They can have these lumps. But, again, we can compute where the middle 95% of the data is. So for the ants it goes from negative 0.5 to 0.5. The observed difference of three-quarters of an inch, that would be pretty weird to see if there was no difference between my two different ant size groups.\nI’m a statistician, so the open source programming language of my choice is R. If you wanted to do this yourself in R, this is the code you would use.\nlibrary(mosaic)\ndiff(mean(~height|team, data=tugowar))\n\nbootstrap &lt;- do(1000) * diff(mean(~height|team, data=resample(tugowar, groups=team)))\nSo if I wanted to compute one difference in means, it’s the first piece of code. And I wanted to do a thousand of them, I could use the second chunk.\nBut there’s another technique we might want to use for assessing whether the number we got is sort of reasonable or what some other possible numbers we could have observed could be. That’s bootstrapping. In bootstrapping you take the data you have, kind of like pulling yourself up by your bootstraps. Making something from nothing. That’s not really possible, so we’re going to make data from our old data. We’re going to sample with replacement. Pull out data points. Here we’re not breaking the relationship between the two variables. We’re just pulling them out directly.\nAnd you can see sometimes I get the same data point more than once in my bootstrap sample. But now I have new data, and I can treat that as my current data. And compute the mean heights, and then the possible difference in heights. And so if that observed tug of war game we saw was really representative of the world, other than 0.75 I might have observed 0.6637.\nI can come up with a bootstrap distribution, much in the same way I can with randomization.\n\n\n\nBootstrap distribution”\n\n\nThe bootstrap distributuon might not be symmetric. It is centered around the estimate from the real data. In this case, centered around the 0.75. And then it shows us some possible numbers we could have observed.\nSo, again, if that was our data and we were generating bootstrap samples, we might have seen differences from negative 3 to positive 4, and again we think ours is not different from nothing or zero.\nAnd with the ants, all the observed values we could have seen, they are all negative.\n\n\n\nBootstrap distribution\n\n\nSo, we think there is a relationship. One of those groups is bigger than the other.\nIf you want to know more about randomization and the bootstrap, you could look at this open source textbook. What? It was written by some really cool statisticians. The source is all on GitHub. You can download the PDF for free. If you like physical books, you can buy this statistics textbook that I used in my class for $9 on Amazon. So it’s really cool and good.\n\n\n\nOpen Intro\n\n\nIf you want other resources, Jonathan Stray has a five minute lighting talk from NICAR, “Solve Every Statistics Problem with One Weird Trick”, and that one weird trick is randomization, so you kind of know about that.\n[](http://jonathanstray.com/me\nAnd Tim Hesterberg, has a paper, “What Teachers should know about the Bootstrap”. I think it really should be called what everyone should know about the bootstrap.\n\n\n\nTim Hesterberg\n\n\n\n\n\nWhat does this have to do with visualization?\n\n\nSo what does this all have to do with visualization? We’re going to try to do the same thing with visualization as we did with numbers. Try to say is this different than nothing? What are other things we could have observed? And this is going to help us try to not make visualizations that don’t show anything.\nWhen I say that, I don’t mean visualizations of the type you see on WTF-viz,\n\n\n\nLingerie chart?\n\n\nalthough I like making fun of those too. And I don’t mean showing nothing in the way Darrell Huff is talking about in the 1950s with How to Lie with Statistics.\n\n\n\nDarrell Huff\n\n\nInstead, I’m talking about using techniques like randomization. There’s an awesome paper by Hadley Wickham, Andreas Buja, Di Cook and Heike Hofmann.\n\n\n\n\n\nHadley Wickham\n\n\n\n\n\n\nAndreas Buja\n\n\n\n\n\n\nDi Cook\n\n\n\n\n\n\nHeike Hofmann\n\n\n\n\nIt’s called Graphical Inference for InfoVis. I think it’s really worth checking out. One of the techniques they suggest in the paper is called the lineup.\n\n\n\nLineup\n\n\nThe idea of the lineup is you put your data, your plot that you think shows something real, in a lineup with a bunch of innocent plots. If you can pick your accused plot out of the lineup of innocent plots, that means that it’s somehow different than nothing.\nTo illustrate this, I would break the relationship between X and Y and look at what other plots I could have gotten from the same data with that relationship broken.\n\n\n\nRandom plot\n\n\nInstead of looking at them all in sequence, I want to look at them all together. Let’s take an example. This is some data about loans. And I’m plotting the balance of a loan against the income. And I’m coloring by whether or not a person defaulted on their loan.\n\n\n\nLoan defaults\n\n\nAnd my human brain sees a pattern. I say, it looks like people who default on loans are the ones that have really high loan balances. It doesn’t matter if they’re rich or poor, it’s just those high loan balances.\nIs it possible I just sort of made that up? This is what it would look like if you use the protocol in the graphical inference paper and look at the plots.\n\n\n\nLoan defaults in lineup\n\n\nThere’s 20 plots, because we often use a standard p-value cutoff of 0.05, which is the same as one out of 20. If you guessed randomly, you’d get the right one 1 out of 20 times even guessing at random.\nSo with this one I think you can probably identify which is the real plot. Part of it is I showed you the real one before. You might have recognized it.\nIn practice you shouldn’t look at the accused plot alone before you look at it in the lineup. So either you should build this into your workflow, looking at a lineup of plots every time you make a visualization, or more realistically, if you make a plot and think it might sort of show nothing, make a lineup and show it to someone who hasn’t seen the original plot.\nlibrary(ggplot2)\nggplot(Default)+\n  geom_point(aes(x=balance, y=income, col=default))\n\nlibrary(nullabor)\nggplot(lineup(null_permute('default'), Default))+\ngeom_point(aes(x=balance, y=income, col=default)) +\n facet_wrap(~.sample)\n\ndecrypt(\"OlCE bQTQ Aw GWPATAWw d\")\nIf you’re an R person, this is familiar. This is the ggplot code to make a scatterplot. And the bottom is to randomize the data to make null plots. I’m doing a null permute on the label of default versus not default and wrapping those facets into an array of plots. When you run the code, they have done a clever thing, it will make a plot in your plotting window, but also prints this text in the console.\nIt says decrypt and then it’s a bunch of nonsense characters. And that’s where they hid the answer. So if you were doing this on your computer and you wanted to not know ahead of time which was the real plot, but figure it out afterward, this is how to do it. You take the piece of code, and run the code, tells you where the true data was. So in this case we got it right.\ndecrypt(\"OlCE bQTQ Aw GWPATAWw d\")\n[1] \"True data in position 14\"\nYou can use the same code for the t-test. This is the tug of war teams again. Somewhere in there I’ve hidden the real plot of the different heights. The crosses represent the mean heights for each of the groups. The question is if you can pick out the real data. So in your head, make your guess.\n\n\n\nVisual t-test\n\n\nIt says decrypt, if you’re good at decrypting quickly, you could see what it is. But the true data is in position 5.\ndecrypt(\"OlCE bQTQ Aw GWPATAWw J\")\n[1] \"True data in position 5\"\nThat’s not the one I picked out. Certainly not the most extreme of the possible plots. What this is telling me is the same thing as the permutation test I did with the statistics and the distribution at the beginning of the talk. There really isn’t a difference between these two groups.\nSo I’ve given you a positive example and a negative one. But this one, I don’t think you really needed visualization to tell you that answer. You could have used standard statistics. The power of the graphical inference technique, is it’s generalizable. You can use it for anything.\nOne thing where I think humans are great at finding patterns in noise is in time series analysis. This is about the steps I take. So it’s from my FitBit. And sometimes I like to make up stories, like the variance is going up. Or the mean seems like it’s changing. Or something happened in December. I’m really good at making up those stories about the distributions.\n\n\n\nTime series\n\n\nSo again, the real data is in there. Again, look at it and try to see if you can guess which one it is. And now we’re going to try and decrypt. So there’s the string again.\ndecrypt(\"OlCE bQTQ Aw GWPATAWw y\")\n[1] \"True data in position 4\"\nIt says the true data is in position 4. Again, that wasn’t the one I had picked out. But if I had just shown you the one time series plot, you would have believed my stories about what was interesting about that plot.\nWith the statistics analog, we’ve been doing the “is this different than zero?” task. Now I want to switch to the “what are some other reasonable values we could have gotten for this?” task. So with the numbers, that was what are some other reasonable values we could have gotten.\nAnd then I’m going to talk about that in terms of plots. So some of this comes from joint work with a colleague of mine, Aran Lunzer, we worked together at the Communications Design Group. This is the highest quality image of Aran I could find.\n\n\n\nAran Lunzer\n\n\nI guess it’s almost life size in the theater, so maybe that’s fine.\nWe worked on a tool, a prototype tool, and we called it LivelyR. This is real in the sense that if you go on GitHub and download the code you could run it yourself. I don’t recommend it. It’s very buggy. But it really has R on the back end and LivelyWeb and javascript on the front end.\n\n\n\n\n\nLivelyR introductory demo from Aran Lunzer on Vimeo.\n\nIt lets you play with the bin width and bin offset of a histogram. Many things let you do this, but this also lets you overlay a sweep of parameter values. So you can see a variety of histograms with the same bin width and slightly different bin offsets. And they form what I like to call a histogram cloud. Which is like a kernel density estimate, but easier for people to understand.\nThere’s one more feature we built into this, which is you can call out the individual histograms if you want to. Instead of seeing them overlaid as a cloud, you could do small multiples. And it’s actually a two-hand interaction here. There’s an iPad and you’re controlling the one screen with your right hand and the other with your left hand. So it’s a little bit buggy. But you can see the small multiples of the different possibilities of histograms you could have seen.\nAgain, I think it’s really easy with histograms to use the default algorithm, the Sturges algorithm or whatever it is in your favorite tool, which will make the bin widths for you. If you’re using ggplot2, it will give you a warning, like, I chose a default but you should come up with something better. But, many people don’t. I know that you all do, because you’re visualization professionals.\nBut it’s not always obvious to non-professionals that those defaults make a huge difference. You might use the defaults and find what looks like a pattern, but it’s really just the result of the parameter values you chose. So giving people the opportunity to play with the parameters is really powerful.\nAfter Aran and I did this work in one dimension, looking at the histograms. I started thinking how could we do this in two dimensions? Which got me thinking about maps.\nThe Modifiable Aerial Unit Problem says that if you aggregate spatial data in different polygonal shapes, you’re going to get different spatial patterns.\n\n\n\nModifiable Areal Unit Problem\n\n\nThis can happen with gerrymandering.\n\n\n\nGerrymandering\n\n\nIt can happen with different county and zip code data, these different levels of polygons. And there are a lot of statistical problems that go along with that.\nAran Lunzer and I worked together on this next tool, which doesn’t really have a name.\n\n\n\nSpatial aggregation\n\n\nIt takes point data about earthquakes in southern California and aggregates them into polygons. But instead of making fixed polygons, you can scale and rotate and move the polygons to see some other possible values of the visualization you could have gotten. So you start with your default values and maybe you think there are very obvious trends everyone should be aware of. You can tell some great story about why there is a hot spot of earthquakes in this one place.\nBut then when you start manipulating the polygons, you can see there are many other possible patterns you could have created.\n\n\n\nNothing, AZ\n\n\nWhat I want you to take away from this talk is that there are statistical tasks and very analogous visualization tasks. In statistics we want to know if two numbers are different, which is like knowing if one number is different than zero. We could use tools like randomization to answer that question. In the visualization world we can use randomization in a different way to answer the question, is this plot different than nothing?\nIn statistics we might want to ask, what are some reasonable, other possible values for this number? What other differences in heights could we have observed? And you can use something like the bootstrap to show you other possible reasonable values. In the visualization space, using parameter manipulation, or giving users the ability to see how your parameter choices impact the visual story they are seeing can be really powerful."
  },
  {
    "objectID": "blog/2014-05-05-Repping-Hadley/index.html",
    "href": "blog/2014-05-05-Repping-Hadley/index.html",
    "title": "Repping Hadley at DataFest",
    "section": "",
    "text": "This weekend, we had our fourth annual DataFest event at UCLA. DataFest is a weekend-long data hackathon, where we give students an interesting dataset, plenty of snacks, and a space to work, and they try to come up with insights. This was the third year I attended, and it seems to just get bigger and better every time. We had about 140 undergraduates from all around Southern California this year. Groups came from UCLA, USC, Cal Poly SLO, UC Riverside, and Pomona College. Our data sponsor was GridPoint, an energy-management company. They provide a service that is similar to the Nest (my simplistic interpretation), and they gave us tons of data for the students to analyze. I signed an NDA, so I shouldn’t be very specific about the contents of their proprietary dataset, but students were able to find tons of cool insights.\n1DataFest 2014 students Our 2014 DataFest contestants\nMy personal favorite project (and Best Visualization prize winner) was a group who found and analyzed a trend in misusing AC, which they defined as running the air conditioner when outside temperatures were below some cutoff, I believe 60 degrees. Really, all the projects were fantastic.\nAs a graduate student “consultant” at DataFest, my responsibility was to float around and help the contestants. Since most of the UCLA participants have had me as a TA in a class where we’re using R, I got a lot of programming questions. Invariably, when students were really stuck on something it was a hard problem (which was bad for me looking like a superhero, but means that our students are getting the hang of R!).\nDuring the weekend, I realized that I was repping Hadley Wickham’s packages even harder than usual. For example, students kept coming to me with incredibly hairy code, filled with multiple for loops using explicit indices and making matches over many factor levels, asking for help debugging why it wasn’t doing what they wanted. Although I would attempt to talk through their code with them, it was usually beyond me getting up to speed quickly enough to catch what was probably a small typo somewhere. Instead, I constantly suggested plyr, and actually became pretty good at it over the weekend. ddply() is such a heavy-hitter! There were definitely a few “whoa”s from students who hadn’t seen the power of plyr before. Most of my students already know ggplot2, but they haven’t gone beyond the basic geoms. I helped another group use geom_polygon() to show an additional layer of data in their plot. Finally, the data was all time stamped, and although the base R functions like as.POSIX() would work, I saw a lot of code trying to make character matches to pull months or years from the data. lubridate to the rescue!\nIn fact, I must have a standard line on Hadley, because at one point a group of students called me over to join a conversation with some of our sponsors from Summit Consulting. “Amelia, what do you think about Hadley Wickham?” they asked. “He’s great!” I said. “He writes all the best packages!” This got a big laugh, to the point that I think they’d made a bet on what I’d say.\nRStudio was a generous sponsor of the event, but I think next year they should send some visitors! At the very least, we need to bring back our mini-seminar series during DataFest, and do demos of some of these useful packages.\n\nFor more on DataFest, check out our twitter (and the hashtag #datafest), as well as this great article from FiveThirtyEight. The article mentions that there were three other DataFests this year, at Duke, the Five Colleges and Princeton. Next year, we’re hoping to go even more national, with support from the American Statistical Association. I can’t wait to see where it goes in the future!"
  },
  {
    "objectID": "blog/2016-10-19-GradSchool/index.html",
    "href": "blog/2016-10-19-GradSchool/index.html",
    "title": "Statistics graduate school advice",
    "section": "",
    "text": "It’s that time of year again, the time where I find myself meeting with students thinking about graduate school in statistics. Since I often end up sending people the same things, I figured I’d pull them together into a blog post. You probably already know about the grad cafe, the professor is in, and PhD comics. These other links might not be as common.\n[](&lt;http://www.phdcomics.com/comics.php?f=1286&fb_action_ids=148470391984848]\n\nQ: What is grad school in statistics like?\n\n\nA: Here are some resources to help you get a taste:\n\nWhen I was still in graduate school, I wrote a three-part blog series for datascience.la on graduate school: Statistics grad school– what is it really like?, Part 2 of what statistics grad school is like, and Part 3: Paying for statistics graduate school.\nI gave some additional advice in this recent interview in AmstatNews.\nMichael Lopez also has a piece called So you want a graduate degree in statistics?, with sections on deciding on a program, thriving once you are there, and much more. He goes in to the difference between biostatistics and statistics, a PhD and an MA, and how to evaluate programs.\n\n\n\nQ: Where should I go for grad school?\n\n\nA: Wherever seems like a good fit!\nI really think statistics is such an in-demand field that you can get a job with a degree from virtually any school. These show my personal research interests and biases, but a couple of my favorite programs to recommend to students are:\n\nStatistics at Iowa State. This is where Hadley Wickham and Yihui Xie got their PhDs, and current students include Carson Sievert, Maggie Johnson, Andee Kaplan, Samantha Tyner, and Eric Hare. Notable faculty include Hieke Hofmann and Ulrike Genschel. [Update, 11/27/17, I believe all of those students now have PhDs!]\nBiostatistics at Johns Hopkins. This is where Hilary Parker, Alyssa Frazee and Jenna Krall got their PhDs. Notable faculty include Roger Peng, Jeff Leek, and Brian Caffo (of Coursera data science specialization fame).\nUniversity of Washington Interactive Data Lab. Not exactly a grad program, because it involves people from both computer science and the school of information, but does awesome work. Notable students include Arvind Satyanarayan [Update 11/27/17, he has his PhD!], and faculty include Jeff Heer of the computer science department and Jessica Hullman in the iSchool.\n\nFun fact I recently learned– Iowa State was the first US university with a department of statistics, and Johns Hopkins the first (in the world?) to offer biostatistics!\n\n\nQ: How do I get in to grad school?\n\n\nA: You need lots of things to be a strong candidate, including:\n\ngood (but not necessarily perfect) grades\na decent GRE score (particularly on the quantitative section of the regular GRE)\na compelling story in your personal statement (email me if you want to read mine, I don’t feel like posting it on the web)\nsolid letters of recommendation, particularly from faculty members who know you well from a variety of contexts, like class, research, etc.\nI also always recommend reaching out to people from the program you are applying to, whether current grad students, alumni, or professors to get a better sense of a program and whether you would be a good fit. Female Science Professor has a great post about what to say (and not to say) in an email to a professor.\n\n\n\nQ: How do I succeed in grad school?\n\n\nA: I recommend the following:\n\nHow to Prep for Grad School if You’re Poor- a wiki with some information about the application process, but mostly the “cultural capital” you might not have that could be useful when you’re in grad school.\nDoing research- how to do research (the short story: do a little every day).\nA thesis proposal is a contract. Or at least, it should be.\nWrite the paper first- an idea about how to frame your work by writing the paper first.\nOral exams- this is specifically about advancing to candidacy at UCLA in statistics, but I’ve sent it to friends in other programs who don’t have similar guidelines.\nLeek Group Guides to reviewing papers, data sharing, writing your first paper, or reading papers.\n\n\n\nQ: How can I learn more?\n\n\nA: I recommend getting involved in some online communities.\n\nThere is a ton of activity in statistics and data science on twitter. Most of the faculty and students I mentioned above have twitter accounts (some linked above, the others easy to find), and if you are into R you can get a taste by looking at the #rstats and #tidyverse hashtags.\nRoger Peng and Jeff Leek (mentioned above) and Rafa Irizarry have a blog called Simply Statistics with all sorts of interesting statistics discussions.\nHilary Parker and Roger Peng (mentioned above) have a podcast called Not so standard devations, available on SoundCloud, iTunes, and many more podcast distributors.\n\nWhat did I miss? What are other questions you have?"
  },
  {
    "objectID": "blog/2014-08-06-ICOTS-Panel-Transcript/index.html",
    "href": "blog/2014-08-06-ICOTS-Panel-Transcript/index.html",
    "title": "ICOTS discussion on technology in statistics education",
    "section": "",
    "text": "One of the most interesting sessions I attended at ICOTS was the panel session, Future trends for technology in statistics education. The panel was chaired by Chris Wild of Auckland, and featured\n\nDeb Nolan, UC Berkeley\nNick Horton, Smith College\nBill Finzer, Common Online Data Analysis Platform (CODAP)/Fathom\nWebster West, Texas A&M/StatCrunch.\n\nBecause I was so fascinated by the discussion, I ended up creating an almost-complete transcript of the discussion (biased by my personal interests, of course). I thought it was possible that someone else might be interested in my transcript, so I’m posting it here. Beyond my own interest bias, I’m sure that I’ve mis-attributed an idea here and there. Please let me know if there’s something I should change to be closer to the truth!\n\n\n\nThe panel\n\n\nThe panelists helping attendees find the last few seats (eventually, it was standing-room only)\nThe first question posed to the panel by Chris Wild was\n\nwhat do you consider the most exciting developments in technology for statistics education?\n\nWW: Student centered data collection.\nNH: Open source tools, giving more students access.\nBF: The recognition that data science as essential, and technology making it possible to integrate it into the whole curriculum.\nDN: The popularity of data science and interest by students.\nFrom that question, the discussion really just moved to a more fluid form, where panelists fed off each other’s comments and moved the conversation forward. Here are some of the comments that caught my ear.\nNH: There isn’t going to be one “intro course,” there will be many courses. A second course needs to cover thinking with data, data wrangling, data munging (or whatever you call it). Hadley’s stuff is good, and uses simple verbs. Students should have the ability to do something with real world data.\nDN: Students need to be exposed to more than one technology. Don’t just turn the R crank. [At some point in the discussion, DN shared that students often call her statistical computing course “the R course,” which makes her cringe.]\nWW: I struggle to come up with the right verbs (for describing the tasks students need to be able to do, especially in terms of code). But, we all do. Maybe it’s better to start with examples. Visualize point A, point B, see how to do it, and then determine what skills are needed.\nBF: “Data immersion” is an experience everyone should have, regardless of discipline.\nAnother question arose here, perhaps raised by Chris Wild. &gt; What are the advantages and disadvantages of menu-driven paradigms versus programming?\nWW: I write code that helps people do menu-driven things. Many teachers are unreachable when it comes to code, and menu-driven can help.\nDN: There’s a difference between coding and scripting. Scripting is simpler, coding is more complex. To code, you need some understanding of control flow, etc. Scripting is more interacting at the console.\nBF: In Fathom, you have a collection, and you can sample from it. That’s essentially a loop. If you’re going to be a data scientist, you need to think computationally.\nNH: We need to think developmentally. Think about when parents are upset because they can’t toilet train their baby. You say, I’m confident by the time they’re 18, they’ll be okay. Then work backward from there. [Ed: I’ve lost what this analogy was supposed to be about. Is the baby a statistics student, and the toilet-training teaching them to code? Insights on this one would be appreciated].\nCoding lets you document your workflow. It’s much harder to do this in an interface. This helps with reproducibility, which we’ve basically decided is a good thing. Down the line, you want peer reviewers to be able to look at your data and code. Biologists have lab notebooks, that helps them be reproducible, and they see this in courses.\nBF: Ubiquitous data computing is coming. Imagine your Personal Data Calculator. Just like you always have something to write with, you will have a PDC. Technology for working with data will be just as pervasive as other technologies.\nNH: Shiny lets me quickly make an applet that would take 6 months for a Java developer. There’s a speed up in the creation of tools.\nWW: In StatCrunch, I want to let textbook authors script their own applets (versus scripting artisanal applets for each author individually). This is coming down the road.\nDN: In the past, I’ve tried making education games. One was a hospital that you tour around in. This effort was in Java, but we didn’t have the manpower. I had ideas about interactive dynamic documents, same thing. Shiny is making a difference, although I would prefer to be “down one level.” There’s a difference between dynamic and interactive– dynamic means that output is automatically created for you from code. Interactive means that a reader can play with sliders, etc.\nBF: [Paraphrasing DN] “So, a learning technology is something that will be out of date in 10 years?”\nFrom Chris Wild, &gt; there’s a design problem (that’s always the same) and an implementation problem.\nNH: Michael Bulmer’s “Island”. It’s a big thing to take on.\nDN: You can bring design in through simulation studies.\nNH: Looking at health services research: they use non-random data (e.g. from billings) to find out things about health. This is much the same as the problem we’re facing now. This is what we should be aiming for.\nDN: There’s a lot of room to grow in computer science at the high school level. This will change what we do at the college level. “Have you heard about R.N. Jesus?” Stands for Random Number Generator, but you pray to it to get good numbers in a game. WoW is a super polished game. We don’t have the skills to build something polished like that.\nSome time around here, the panel was opened up to questions from the audience. I’m not sure exactly what point that was. Nonethless, there was another question for the panel– &gt; what will we be talking about at ICOTS 10 in 2018?\nBF: There will be learning log data available. I hope statistics ed people will be using this data.\nThen, someone (maybe Rob Gould) proposed a definition for Data Science. From my understanding of the comment, I sketched a Venn diagram in my notes which looked something like this: \nAn audience member suggested CoderDojo as a resource for students, which makes it cool to code, says every kid can code. [Ed: I think these types of claims are usually very limited to kids with a lot of privilege]\nRandy Pruim asked the panel, &gt; is there a dark side to this? Now, anyone can make a pretty graph, but it might not mean anything. There are thinking skills that are necessary.\nDN: We’ve always had this problem.\nWW: We should be doing things like text analysis in intro classes. Students get out of intro classes, they think data is just numbers.\nBF: Personal data should be meaningful to individuals. It should be able to be worked with.\nAnd with that, my transcript ends. See, I told you it might only be interesting to me! Those who were there, what insights did I neglect to capture?"
  },
  {
    "objectID": "blog/2012-01-30-Bad-words/index.html",
    "href": "blog/2012-01-30-Bad-words/index.html",
    "title": "Bad words",
    "section": "",
    "text": "As a budding statistician, I struggle with the issue of terminology. Since I’m no longer just a member of the public, I’m expected to have a clearer understanding and use of technically specific words, but I’m just as biased by the popular uses of them! A few words/phrases I’m working on:\n“The data is” – I know it is “the data are,” but that still sounds so weird.\n“Random” – I still say this when someone says something unexpected, if I run into a long-lost friend at the mall, or if things appear to have no cause. Random is really quite a specific word, and almost every time I say it, the phenomenon is not random. That brings me to my next word,\n“Phenomena” – this sounds like the singular, not the plural.\n“Chaos” – Similar to random, it has a technically specific meaning that I am almost never using correctly. “It was chaos! Everyone was talking at once!” really doesn’t describe a chaotic situation at all.\nTwo of these four errors have been unfortunately been documented in slides of presentations. Oops. Don’t judge me til I have my PhD… I’m working on them!"
  },
  {
    "objectID": "blog/2014-03-04-NICARthoughtsPt1/index.html",
    "href": "blog/2014-03-04-NICARthoughtsPt1/index.html",
    "title": "Thoughts from NICAR 2014: taking action",
    "section": "",
    "text": "This past weekend, I attended the IRE NICAR conference in Baltimore, MD.\nIt took me a while to figure out what those acronyms stand for, but they’re Investigative Reporters and Editors and National Institute of Computer-Assisted Reporting.\nNow, I am not a reporter, editor, or computer-assisted journalist, but I found the entire conference fascinating and learned a lot. I thought I’d write up some of my impressions to keep them fresh for myself, and in case anyone might be interested in what a statistician thought of a journalism conference.\nI had a lot of thoughts, so I’ll be breaking this into a couple pieces. The first thing I’ll say is that I’m taking Matt Waite’s advice, and putting my learning into action immediately following NICAR. That’s what this blog is! I’ve been meaning to learn jekyll for months now, and writing about NICAR provided the perfect motivating project to learn the tool. I’ve already switched my entire website to jekyll, and I’m learning how to build blogs (what jekyll is really good at ) now.\nSome resources that have been helpful to me on this journey include:\nsudo xcodebuild -license\nThis turns out to be one of the things that was hanging up my jekyll install. I kept thinking it was a problem with my ruby version, or with some other dependency, but once I had run that and agreed to the xcode user agreement, doing\ngem install jekyll\nworked right out of the box.\nOnce I had jekyll running, I referred to a bunch of other peoples’ blog posts and github repositories, notably:\n\nAnna Debenham’s blog on getting started with github pages (plus bonus jekyll)\nJosh Branchaud’s blog on running your jekyll blog from a subdirectory\nThe snaptortoise github repository on jekyll rss feeds (found via Drew Inglis’ post on adding RSS to jekyll)\nDaring Fireball guide to markdown syntax\nAnd of course, the jekyll documentation on things like writing posts.\n\nObviously, this is a work in progress. But I’m feeling good about jumping right on something that I’ve been wanting to try for ages, especially since it came up so much at the conference. Separation of presentation and content. It is good.\nThis is the first in a series of three posts about NICAR. The second is here and the third is here."
  },
  {
    "objectID": "blog/2014-12-12-FabricPosters/index.html",
    "href": "blog/2014-12-12-FabricPosters/index.html",
    "title": "Fabric posters",
    "section": "",
    "text": "Don’t you hate having to take a poster to conferences? The tube is bulky, it’s easy to forget in the airport (or on the plane!), sometimes you get charged extra for your “additional carry-on” and often the poster doesn’t look great once it arrives. Plus, it’s expensive to print a poster, and you usually only get one use out of it.\nEnter, the fabric poster!\nI learned about this wonderful option via twitter, before the Women in Statistics Conference. I was one of three women to bring fabric posters. Shown here are the other two, demonstrating one of the fantastic things about fabric posters– they’re flexible!\n\n\nSuperhero statisticians to the rescue! @kralljr @HelenPowell01 #WiS2014 pic.twitter.com/uuVl05d40z\n\n— Stephanie Hicks (@stephaniehicks) May 16, 2014\n\n\nBetter than that, you can fold them up in your suitcase and they don’t wrinkle (or if they do, you can iron them!). Plus, they have the same DPI as paper, they’re cheaper, and you can easily re-use them (either as a poster or as a superhero cape). Still not sold? Check out these photos from IEEE Vis in Paris. The conference organizers needed to take down the posters quickly, and almost anyone with a paper poster ended up with something ripped, wrinkled, and taped to someone else’s poster.\n\n\n@AmeliaMN Sad paper posters, happy (folded) fabric poster. #ieeevis pic.twitter.com/VFZj2HdOuH\n\n— Amelia McNamara (@AmeliaMN) November 14, 2014\n\n\nOkay, so how do you do it? This post is fantastic, and it’s where I learned how to print a fabric poster. For whatever reason, that link keeps changing, so I’ll summarize the main points:\n\nuse the website spoonflower.com\nchoose the “performance knit” fabric, which should cost around $25\nupload your poster as a png (ignore at your peril!)\nmake sure it looks right, and order!\n\nPrinting and shipping is variable, but Spoonflower has a data-driven page that will tell you how many days production is currently taking. If you need your poster quickly, you can pay for rush shipping, but either way the poster will come folded up in one of those soft plastic envelopes that fits easily into any mailbox. What are you waiting for?"
  },
  {
    "objectID": "blog/2014-04-16-NICARthoughtsPt3/index.html",
    "href": "blog/2014-04-16-NICARthoughtsPt3/index.html",
    "title": "Thoughts from NICAR 2014: process and content",
    "section": "",
    "text": "One of the things I was most interested in learning about at NICAR was the processes and tools that journalists use when they’re doing data analysis. And the conference did not disappoint. I feel like I came away with a bunch of insight about how things are typically done, and a couple great places where crossover could be even more useful.\nData diaries\nThroughout the conference, I heard several people emphasize the need for a “data diary” to document the process of creating data-driven journalism. This is great! We should all strive to document our work to capture our own process and remember what we do. However, it seems like journalists have a workflow something like, as you do your analysis in Excel, write down in Word what you did. Others write longhand on a piece of paper. But, it would be much better to have the documentation of the process actually be the process. For example, if you do your analysis in a language like python or R, you don’t have to write out “I edited the third row, second column to say 10 instead of 10000.” Rather, the code both does the action and documents what happened. Obviously, commenting is important so that you remember what all your hacks were doing, but using a language rather than a graphical user interface makes the process so much more documented. In fact, you can wrap your code and comments in a narrative, using tools like the iPython notebook or R Markdown.\nIn fact, one of my dreams for data journalism has been that all data-driven stories would be accompanied by a version of the data cleaning and analysis process. This documentation would be available to readers, who could modify and “audit” the code/process to see what their changes do. In my vision, you could also view the raw data, to see where everything started. However, hearing it from the journalistic prospective, that’s probably an unreasonable vision. Sarah Cohen was saying that if you wanted to open-source data, you would need to verify every cell in the data set, and have an editor go through it, too. It’s hard for me to imagine “editing” a data set like that, but I can see why it’s necessary. If you did something wrong in your data set, it may not make any impact on the final result (that one value probably isn’t throwing off your mean or plot if it’s not an outlier) but if the public could inspect the data cell by cell, it could be a big deal to have a number wrong. I need to think more about this and see if there’s a solution. If you think of one, let me know in the comments!\nWorking headlines\nAnother piece of process that I heard referenced several times in the sessions on “The Data-Driven Story” was the conversation starter, “Tell me what your working headline is. Tell me what your first two paragraphs are.” I work with a lot of education experts, who like the strategy of “sentence starters” which are phrases that help get students speaking in the language of the discipline, while guiding the conversation. This is similar. It’s a simple strategy, but it would be easy for an editor, even one not experienced with data, to understand the response from their journalist. It takes the conversation out of the realm of the data and back into the context. I think it could be a useful conversation (or rubber-duck debugging topic) for statisticians and data analysts to work through.\nLightning talks\nOn a more meta level, I thought the exercise of lightning talks was extremely effective. The space was large enough for almost every attendee of the conference to be included, the talks were short enough that they could only tantalize, never bore, and the material was surprisingly complex. The event was kicked off with some stand-up comedy by Andy Boyle, who I’ve come to discover is just as funny on twitter talking about journalism.\nFor me, the best three lighting talks were by Lena Groeger, Chase Davis and Ben Welsh. Lena talked about “wee things,” which I thought was a nice contrast to “big data.” Some of the wee things she discussed were small multiples, tiny text, mini maps, iity-bitty images, and sparklines. The entire talk seems to be available online if you want to check it out yourself. She also recommended the book The Information which I’m really looking forward to getting into. Chase essentially ran through my entire statistics graduate education in 5 minutes, explaining five algorithms, including Latent Dirichelt Allocation and the Naive Bayes Classifier. The code is on github, although the presentation isn’t working for me. He apparently gave a similar talk the previous year, so if you want to catch up with everything I’ve learned in my Phd, check out his code and learn about Principal Component Analysis, Random Forests, and more. Ben did a talk called “You Must Learn,” although what stuck with me the most (more than what I was supposed to learn, oops) was the graphic styling of his presentation. Each slide was like a comic encapsulation of what he was talking about, and not in a dumb clipart way. The video of the talk is up too.\nI’m on the planning committee for useR! 2014, and I’m hoping that our lightning talks can be even half as awesome as the ones from NICAR. You heard me, statisticians, step it up!\nLyra\nMore of a tool than a process, but I was thrilled to be able to attend a live demo of Lyra with Arvind Satyanarayan, one of the creators of the project. Lyra is fascinating to me because it’s open source, it’s working with an existing framework, and it just makes extremely powerful data visualization tools available to people without much training necessary. If you haven’t already seen it, check out the gallery of plots they’ve been able to reproduce with their tool.\n*This is my third (and final) post about NICAR 2014. The other two are here and here."
  },
  {
    "objectID": "blog/2014-08-14-DataScienceLA/index.html",
    "href": "blog/2014-08-14-DataScienceLA/index.html",
    "title": "Data Science LA",
    "section": "",
    "text": "I’m now involved in the recently-launched datascience.la blog. While you’ll probably be drawn to the site for the useR! conference session videos, interviews with data scientists, and meetup announcements, there’s going to be much more!\n\n\n\nDataScienceLA\n\n\nI’ve posted Data Science Goes to College with DataFest, yet another summary of the yearly DataFest we hold at UCLA and that’s going national.\nThen, I wrote Statistics Grad School– What is it Really Like? describing some of the experience of getting a PhD in statistics at UCLA.\nFinally, I’ll be giving a talk at this month’s DataVis LA meetup, Eyeo Festival Highlights on August 26th. Come by Cornerstone OnDemand to hear me talk about the things that inspired me the most from the eyeo festival.\nIf you want to keep up with datascience.la, you can follow along via twitter, youtube, add us to your favorite rss reader, or just drop by the site. I’m excited to be a part of it!"
  },
  {
    "objectID": "blog/2018-03-20-Wikipedia-in-the-classroom/index.html",
    "href": "blog/2018-03-20-Wikipedia-in-the-classroom/index.html",
    "title": "Wikipedia in the classroom: Gender, argh",
    "section": "",
    "text": "The data journalism course I’m teaching this semester is a new offering for me. I may write up more of my reflections later, but for now I wanted to get some thoughts down about the experience of using Wikipedia as a pedagogical tool. At this point, I feel like it was too real-world of an experience for the class, and I inadvertently exposed my students to some gendered bias.\nFor their first writing assignment in the class (I am nothing if not ambitious!) I had the students author Wikipedia articles for people who did not have them. You can look at my assignment guidelines, but essentially I wanted them to research and write an article, but I gave credit for completion, rather than quality (particularly by Wikipedia’s metric). I generated a list of folks I thought were notable enough for articles, but told students they could choose someone else if they had an idea they wanted to pursue.\nIt turns out that not everyone I thought was notable actually had enough material on the web to meet the Wikipedia notability standards. For example, Charlotte Wickham might fall in that category. She seems notable to me, but there’s only a limited amount of press on her. I also learned a little too late that journalists often do not count as notable, because Wikipedia has not accepted a specific notability standard for them. So, people like Sisi Wei and Ryann Jones might fail to meet the standard, even though there is quite a bit of material on the web about them.\nOf the 20 or so articles my students undertook, seven of them are now live on Wikipedia. I’ll admit to being a little confused about how articles end up in the main namespace– some things that I’ve written have just ended up there right away, while others get stuck with an “awaiting review” flag, and others are just in the draft namespace with no such tag. Oliver Keyes and Mikhail Popov offered me some guidance on the Friendly Tech Space Slack (it lives up to its name!) and the most recent conclusion is that I conincidentally had students do this assignment during an Autoconfirmed article creation trial at Wikipedia.\nThose seven articles are:\n\nLynn Cherny (saved from deletion from a friendly editor with a female-sounding username)\nJanaya Khan (probably the best article to come out of this class, and one of the names the students came up with themselves!)\nJeff Leek\nGiorgia Lupi (may not actually have been reviewed yet, perhaps I just dropped it into the main namespace?)\nStefanie Posavec (also may not have been reviewed)\nVictoria Stodden\nJer Thorp\n\nI think those articles are fantastic, and I’m glad to see them added to Wikipedia! But, that means the vast majority of the articles my students wrote are either waiting for review, or have been rejected. The queue of articles waiting for review is long, and the flag says they are “reviewed in no specific order” but that it may take 8 weeks or more to get a review.\nI don’t have enough data here to back this up, but it seems that of the articles my students wrote that have already been reviewed, the ones about men (Jeff Leek and Jer Thorp) were reviewed quickly, given the benefit of the doubt (see the charitable comments on the article about Jeff Leek, even though the draft might not have been of the highest quality), and accepted. The ones about women are either still waiting to be reviewed, or have been rejected on grounds that I think are pretty weak (see the comment on the article about Amanda Cox that rejects her article because it says she served on the Federal Reserve Board, rather than that she worked at the Fed).\nHere are the rejections that are getting me particularly riled up:\n\nJenny Bryan\nAmanda Cox\nKim Rees\nJulia Silge\n\n[Edit, May 2019: Jenny, Amanda, and Kim all have Wikipedia pages! Thanks to Jessamyn West for using her Wikipedia status to get Amanda and Jenny’s articles pushed through.]\nI think those people (women) are extremely notable, and we should work to get their pages up to snuff. Will you help me?\nAnother place you could jump in is trying to improve the articles we have that are still in the review queue (or that I’ve re-submitted after rejection), to hopefully forestall any issues:\n\nKennedy Elliot\nMeredith Broussard\nBeatrice Fischel-Bock (another student-generated idea!)\nLena Groeger\nMark Hansen\nSisi Wei\nCharlotte Wickham\n\n[Edit, May 2019: Meredith, Lena, and Mark now have wikipedia pages!]\nAnd, if you’re looking for folks to write articles about, I’m giving away ideas for free! How about:\n\nMara Averick\nNadieh Bremer\nKarl Broman\nJen Christiansen\nJessica Hullman\nMichelle Minkoff\nHilary Parker\nRoger Peng\nOlga Pierce\nDavid Robinson\nIrene Ros\nKara Woo\nNathan Yau"
  },
  {
    "objectID": "blog/2013-08-10-Productivity/index.html",
    "href": "blog/2013-08-10-Productivity/index.html",
    "title": "Productivity",
    "section": "",
    "text": "Like most people, I don’t feel like there are enough hours in a day to do everything I want to. And, I’ve been getting caught up in the small stuff because it’s so much easier to renew that library book than start writing my dissertation. Here are a few things that have been helping me become more productive lately:\n\nInbox Zero. There are many explanations of inbox zero floating around out there, but this is the one I’ve implemented. Now, the only things I see when I log in to gmail are emails that haven’t been touched before, and if I want my “to-do list” emails, they’re in the starred filter. On the same tack, I’ve started filtering my work emails out of my personal inbox so that’s not the first thing I see when I look at my phone in the morning. I only work half time, so this helps me to delineate my time better.\nCoffitivity. Again, there are many tools that give you ambient noise (my old standby was iSerenity) but this one has a modern sound. In fact, I swear it incorporates the exact buzz that my phone makes against the desk. Maybe that’s not a good feature…\nRescueTime. This app helps me quantify how productive I’ve been. The weekly reports are always a little depressing (I need to adjust my goals so I sometimes meet them), but they give me a little jolt of energy to change my habits or an affirmation that I’m on track.\ntoggl. When I need to actually keep track of hours for something (freelance projects or general hours accounting) I use toggl. Social media on the bus, only. It’s so tempting to look at facebook, twitter, or blogs during the day, but it’s so counterproductive to my dissertation goals. I’ve been trying to limit social media consumption to my twice-daily bus ride, when there’s too much stimulus for me to read something serious but I’ve got a solid block of time to fill.\n\nThen, there are some things that I know would help me become more productive that I need to implement (or get better about):\n\nTurn off email notifications on my phone and probably on my computers as well. Do I really need a ping every time I get a new email? As I work to filter things out so they don’t hit my inbox (and instead are filtered for later consumption), maybe the ping will become more meaningful, but right now it’s probably a distraction.\nLeave myself a note with an action-item for the following morning. I usually have my highest energy levels right when I arrive at the office, so that’s not the time for email triage (are you sensing a trend, here? Email is the bane of my existence). If I left a Post-It with my first concrete task for the day, I could jump right into that.\n\nMany of these ideas came from adulting, smartprettyandawkward, and hacks for grad students, so credit where credit is due."
  },
  {
    "objectID": "blog/2014-06-12-Sweater_Variation/index.html",
    "href": "blog/2014-06-12-Sweater_Variation/index.html",
    "title": "Sweater variation; or grey != black and white",
    "section": "",
    "text": "I have a sweater that I’ve loved to death (if you’ve encountered me anytime in the last 5 years, you’ve probably seen it) and after many frustrating attempts to replace it with something similar, I took to eBay to try to find an exact replacement. This is something I’ve had success with in the past– put in some information about the item, and make a saved search that emails me every day with newly-listed results. But I’ve learned that eBay sellers only know about the mean, and not about variation.\nMy eBay saved search goes something like this– “H&M cardigan Color:Gray” but I’ve found that I often get results like this:\n1Black and white sweater\nPart of the problem is one of classification– eBay provides the standard six colors (primary and secondary), and a few descriptors like beige, ivory and brown, but probably the best description of the sweater above is “Multi-Color” which doesn’t really capture it. Probably eBay should provide a “Black and White” option.\nWhat I think is funny about this example is that the seller is partially right– the typical color of this sweater is gray. But I’m not interested just in the typical color. I’m looking for a sweater with limited color variation (consistency across the garment). I’m looking for a unimodal color distribution with a small standard deviation, not a bimodal one!\nThis is abusing terminology a bit, but I think it illustrates an important point about how we don’t always care about the typical value, and when we talk about distributions (in any sense) we also need to take into account the variability."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html",
    "href": "blog/2020-04-16-presentations/index.html",
    "title": "Guidelines I follow when giving talks",
    "section": "",
    "text": "As with everything in life, there are no hard-and-fast rules for giving talks. But, I have a number of conventions I have developed over the years that I think make for clear(er) presentations. If you look at talks I have given you will see many of these things in practice. I started writing up some of these conventions for my Data Communication course, but realized there might be broader interest, so I’m making it a blog post."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#intro-slide",
    "href": "blog/2020-04-16-presentations/index.html#intro-slide",
    "title": "Guidelines I follow when giving talks",
    "section": "Intro slide",
    "text": "Intro slide\nThe intro slide should be eye-catching, because it is often up on the screen for longer than everything else. If you’re giving a talk in person, it is sometimes on the screen for several minutes before you actually get up to give your talk, and for the first moments as you begin speaking.\nI always try to find an engaging image on flickr that is Creative Commons licensed. Google Image search also has a filter for CC licensed images, but I find the collection on flickr to be better.\nMy first image always bleeds across the entire slide. Evergreen Data has a nice explanation of why it’s good to bleed images wherever possible. To make slides look nice in high resolution, I always download the largest possible image from flickr.\nOver the image, I put the title of my presentation, my name, and at least one way to contact me (usually my twitter handle)."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#introducing-yourself",
    "href": "blog/2020-04-16-presentations/index.html#introducing-yourself",
    "title": "Guidelines I follow when giving talks",
    "section": "Introducing yourself",
    "text": "Introducing yourself\nWhen you begin a presentation you are often nervous. I’ve developed a little “patter” I say at the beginning of any talk I give in person. It goes something like this: “I’m Amelia McNamara. I teach at the University of St Thomas in Minnesota, and I tweet at AmeliaMN. That’s a double-entendre, because my last name is McNamara, and I live in Minnesota. I just tweeted a link to these slides, if you want to follow along.” By the time I’ve said that (out of sheer force of habit) usually my nerves have calmed down, and I can begin with my content. (In classes, I find I almost always start with “hi folks! It’s [insert day of the week]” for similar reasons.)"
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#slide-content",
    "href": "blog/2020-04-16-presentations/index.html#slide-content",
    "title": "Guidelines I follow when giving talks",
    "section": "Slide content",
    "text": "Slide content\nAs Edward Tufte so effectively explains in PowerPoint does Rocket Science, many technical presentations get bogged down by having too much information on the screen at once. I try my best to not include formulas on slides unless absolutely necessary, and reduce the number of words on slides as well.\nYou want your audience focusing on what you have to say, more than reading what’s behind you. But, the slides should give them something to look at (maybe a pretty picture!) or an overview of the idea. I mostly present at conferences, and attendees like to take photos of slides and live-tweet them, so a slide that stands alone as a summary is a good way to encourage that.\nMuch like optimizing the data-ink ratio for a data visualization, you can often simplify a slide down to just its most important point by iteratively removing information. If you have an image on a slide, it should probably fill or almost-fill the slide. Same with a data visualization. When I include a ggplot image on a slide, I scale it so the visualization fills as much of the page as possible, often cutting off some of the white margins at the top and bottom. The rule of thirds can make your slides look more professionally-designed.\nEvergreen Data has a summary of the top four mistakes seen in conference presentations. I disagree slightly with some of these mistakes– I tend to include references on each slide, both hyperlinks to my image sources (to credit those CC-licensed image creators!) and links to original papers and other materials. However, I try to use a URL-shortening service whenever possible. Most commonly, I use bit.ly and customize the shortlinks to be readable. For example, I’ve made bit.ly/AllHailTheAlgorithm, which links to a piece of journalism by Al Jazeera called All Hail the Algorithm. It’s a simple-enough link people watching can type it out as they watch, and again, I always distribute slides via twitter before I give a talk. St Thomas also has a link shortener if you want professional links with a St Thomas theme.\nIn general, slide animations are usually distracting, so it’s best not to use them. However, if used carefully, animations can be very effective. One convention I’ve become slightly known for is embedding screen-capture videos into my slides, so I can show how a piece of software works or a piece of code executes (example 1, example 2, example 3). Crucially, while I’m showing a video, I never am hitting play on something with sound. Instead, it’s a silent video and I get to talk over it live during my presentation."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#building-slide-shows",
    "href": "blog/2020-04-16-presentations/index.html#building-slide-shows",
    "title": "Guidelines I follow when giving talks",
    "section": "Building slide shows",
    "text": "Building slide shows\nThere are a number of platforms for making decks of slides. The most common are probably PowerPoint, Keynote, and Google Slides. Personally, I use Keynote because it works the most seamlessly on a Mac.\nIf you want to work more reproducibly, there are options like Beamer (the way to make slides using LaTeX, a typesetting language for math) and variations on RMarkdown that knit to PDF or HTML slide shows. Personally, I don’t think those platforms give me as much control as I want, and are a lot of overhead to learn. I only use them if I need to include a lot of typeset math, or (sometimes) code.\nEven with code, I often use Keynote because it makes it easier to get things exactly the size I want. I do use the reprex venue RTF to get code highlighting to paste into Keynote.\nIn our new world of online education, I’ve found myself doing a lot of video-recorded presentations. I find full-screen presentations with just a voiceover to be pretty hard to stay attentive to, so I try to have a picture-in-picture of myself talking any time I do one of these recordings. At least on my computer, using the widescreen (16:9) format for my slides rather than standard (4:3) format allows me to get a nice picture-in-picture automatically when recording from Zoom, and easily when recording with a tool like OBS as well. I did a short behind-the-scenes video of how I record using Zoom."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#giving-the-presentation",
    "href": "blog/2020-04-16-presentations/index.html#giving-the-presentation",
    "title": "Guidelines I follow when giving talks",
    "section": "Giving the presentation",
    "text": "Giving the presentation\nWhen I’m given the opportunity to present from my own computer, I always use the Keynote file and take advantage of all the settings I can pre-select (I have my presenter view customized, I know what will happen to my videos when I click, etc).\nIf I have to use someone else’s computer, I export my slides to PDF and use the static slides for my presentation. Sometimes I will “print each stage of builds” if I’ve designed some type of buildable slide (text appearing on a slide, for example). This will end up creating more pages for my PDF, but is nicer while I present.\nI also always export to PDF to share slides with others, and for that usecase I do not “print each stage of builds.” People coming to my twitter or website for a copy of my slides are mostly skimming for links and takeaways, they don’t need the entire build-up of the presentation.\nAs part of the fast-talking R mafia, the thing I am reminding myself the most often while I speak is “slow down” (unless I’m giving a lighting talk where I whiz through 30 slides in 5 minutes). It’s good to build in some places to pause as you speak, to give your audience a chance to process. When you finish talking about a slide, that can be a good place to pause. Nicky Case uses water breaks to good effect for this purpose."
  },
  {
    "objectID": "blog/2020-04-16-presentations/index.html#ending-your-talk",
    "href": "blog/2020-04-16-presentations/index.html#ending-your-talk",
    "title": "Guidelines I follow when giving talks",
    "section": "Ending your talk",
    "text": "Ending your talk\nI have a strongly-held opinion that the last words out of your mouth at the end of a talk should always be “thank you.” This makes it clear you are finished speaking, and also gives the audience a non-awkward opportunity to clap. If you say “any questions?” then the audience claps but it’s a little awkward (can you tell I have social anxiety?). Hadley Wickham says the key is to end with a firm statement (ie. no rising intonation) and then pause, but coming up with something to say in a firm way that is not “thank you” is usually beyond me at the end of a talk. Just like having a patter to begin a talk, having one to end your talk also helps free your brain up to think about other things.\nMy last slide always says “thank you” in big letters, to remind me to say this. I also typically repeat the same image background as my original slide, and repeat how to contact me. Jenny Bryan and Mara Averick disagree with me a little on this point, and say it’s better to have a slide at the end with an overview and links. I think that’s probably great, but I often need the “thank you” cue.\nOn that note, that’s all I have to say about giving presentations. Thank you."
  },
  {
    "objectID": "blog/2016-11-16-CensusRant/index.html",
    "href": "blog/2016-11-16-CensusRant/index.html",
    "title": "Census data: A rant",
    "section": "",
    "text": "As usual toward the end of the semester, my students begin working in earnest on their final projects. And as usual, this results in my selective amnesia about census.gov being ruptured yet again. Looking for a way out, I turned to twitter and got some good recommendations. In case it’s useful to other teachers/people, I’ll share my thoughts.\n(The tl;dr is that you probably want students to be using Social Explorer, but read on for the full rant.)\nFirst, some motivation. A group of students in my class has found some interesting data at the state level, which they think may be related to demographic characteristics of states, like population, poverty level, education, racial makeup, etc. This is where my amnesia kicks in. “It should be really easy to find data like that!” I say. “Let’s go to the American FactFinder website and I’ll show you how it works.” We bumble around on the site for a while, clicking on “Data Main” (a page of screenshots and news) “Data tools and apps” (actually has the link to what we want, but buried among other unrelated things), etc. Finally I manage to find FactFinder main page. We select our geography (states) and dismiss the box that is trying to get us to add yet another geography. We search for “poverty” and voila! …the ugliest dataset you have ever seen.\nI always encourage students to work with data programmatically, but even I have to admit that it would be easier to delete columns and add better names in a spreadsheet. The column names are in multiple rows for crying out loud! Plus, what you would like to be observations (states) are actually variables. I am now imagining the gather/spread in my future. I want students to understand data cleaning and wrangling, but I don’t want their first experience to be hellish. Even my heart is sinking, thinking about doing this all myself."
  },
  {
    "objectID": "blog/2016-11-16-CensusRant/index.html#apis",
    "href": "blog/2016-11-16-CensusRant/index.html#apis",
    "title": "Census data: A rant",
    "section": "APIs",
    "text": "APIs\nOkay, so what are the alternatives? The Census does offer an API, so I signed up for an API key and started playing with some R packages recommended by folks on twitter. The most commonly-mentioned package was acs (a CRAN package), although I also heard about acs14lite and censusapi (both on GitHub and installable using devtools). Sadly, none of these packages have vignettes. I’ve been using R for almost 10 years, so yes, I could figure it out from the documentation, but it would take time (which I am short on) and isn’t something I can ask the students I have in this class to do.\nThomas Lumley pointed me to this blog post on working with ACS data in R which was much more readable. I’m not thinking about maps for this particular application, but I can read around that part. I was able to reproduce the code in the post (yeah!) and modify the first retrieval command to get data for states:\nstateDemo &lt;- acs14(geography='state', variable = c('B17021_001E'))\nBut, this isn’t the only variable I’m interested in! So, how do I learn what other variables I want? As far as I can tell, I have to go to the Census API site and read this table. Say I’m interested in race. I can search the page for that word, but I get almost 5,000 results.\n\n\n\nResults of searching for `race’\n\n\nAnd even if I locate a variable I am interested in, the descriptions are pretty hard to parse.\n\n\n\nVariable description\n\n\nThe result of this is that it would be all too easy to think you were getting one thing and actually request another. Plus, when you generate your list of variable strings like B02001_009E, you also need to maintain a corresponding list of human-readable names like TwoOrMoreCount. If I were writing a peer-reviewed research paper, I would be grateful for the huge amount of detailed breakdowns and the API access, but for a semester-long student project in a class focused on modeling, it’s too much."
  },
  {
    "objectID": "blog/2016-11-16-CensusRant/index.html#using-my-network",
    "href": "blog/2016-11-16-CensusRant/index.html#using-my-network",
    "title": "Census data: A rant",
    "section": "Using my network",
    "text": "Using my network\nWhen I asked on Slack, several folks sent me files they had created using the APIs (thanks, Mara and Adam!), but of course they didn’t have all the variables I really wanted. And, while I’m grateful to my supportive social media network, I know not everyone has that!"
  },
  {
    "objectID": "blog/2016-11-16-CensusRant/index.html#user-interfaces",
    "href": "blog/2016-11-16-CensusRant/index.html#user-interfaces",
    "title": "Census data: A rant",
    "section": "User interfaces",
    "text": "User interfaces\nI had been expecting to be able to find a bunch of GitHub repos full of cleaned up data like the stuff Mara and Adam sent to me on Slack. It seems like people don’t really do that, or if they do it’s not documented in such a way that it is findable. But, Ryan pointed me to the Social Explorer, an online tool mostly designed for making pretty maps online, that includes a Tables functionality that is almost exactly what I was looking for!\n\n\n\nSocial Explorer\n\n\nIt allows you to select your geography, then you can search by keyword or scroll through a manageable list of variable names and add them to a custom table. You can preview the data (it looks like states will be the variables, but never fear, this changes in the download), then choose the format you want to export it in. Other than a slight loss of reproducibility, the only drawback in my mind is that the variable names are still unintelligible, so you have to also download a data dictionary, but it makes you a custom dictionary based on the variables you chose so you don’t have to wade through the entire thing!\nIt looks like the Tables functionality is mostly for “premium” users, which is too bad, but Smith (like many other academic institutions) has a site license.\nAre there other resources out there I have missed? Why is this such a hard problem? I’d love to hear from you."
  },
  {
    "objectID": "blog/2014-05-16-That_Paper/index.html",
    "href": "blog/2014-05-16-That_Paper/index.html",
    "title": "That paper I was telling you to read",
    "section": "",
    "text": "I was joking at lunch with some colleagues that I should have a few of my favorite papers linked on the main page of my website, so when I reference them for the n^th time, I can just say “look at my website” instead of promising to email them. While I don’t think it’s reasonable to put this on my front page, here’s the current list of my “have you read [x]?” papers:\n\nEvaluating the Design of the R Language: I’ll admit, I don’t understand everything that’s in this paper. And it certainly wasn’t necessary to sell me on R! But, I interact with a lot of computer scientists who ask me “why R?” and this paper seems to be a much better answer than anything I manage to come up with.\nTechnical Tools of Statistics: Tukey’s vision of the future, from the 1960s. Interesting because we’re still trying to figure this stuff out.\nSix Provocations for Big Data: This article, by danah boyd and Kate Crawford, gives a great response to the buzzword trend of “Big Data.”\nResearch Directions in Data Wrangling: Just a great paper summing up some directions for research/development in the realm of data processes.\nGraphical Inference for InfoVis: Using an array of plots, most of them with “null” data, to determine if patterns you’re seeing are real or just the result of randomness.\n\n\n\n\nPermuted basketball\n\n\nPermuted graphic showing the location of Lakers players when they made 3-point shots. Which do you think is the real data?"
  },
  {
    "objectID": "blog/2019-12-29-Deleting-Facebook/index.html",
    "href": "blog/2019-12-29-Deleting-Facebook/index.html",
    "title": "Deleting Facebook",
    "section": "",
    "text": "I recently took the plunge and deleted my Facebook account. This was a big decision for me, and I know other people are thinking about doing the same. So, I want to detail why I decided to delete my account, how I prepared to do it while also saving my data, and the way I’ve felt since I took the plunge."
  },
  {
    "objectID": "blog/2019-12-29-Deleting-Facebook/index.html#why-i-did-it",
    "href": "blog/2019-12-29-Deleting-Facebook/index.html#why-i-did-it",
    "title": "Deleting Facebook",
    "section": "Why I did it",
    "text": "Why I did it\nI had been thinking about deleting my Facebook account for several years, because I spend a fair amount of time teaching and speaking about data ethics and algorithmic inequality (e.g., my 2017 Otelia Cromwell Day workshop, Algorithmic Accountability). If you are a Facebook user, you are probably at least vaguely aware the company doesn’t always do the right thing. I can’t cover everything they’ve done wrong, but here are a few things that come to mind:\n\nIn 2013, Facebook was letting advertisers take your data (including your likes and profile picture) to include in ads to your friends. I became aware of this when I saw my dad’s photo in an ad. After this was exposed, Facebook gave users the opportunity to opt out of this.\nIn 2014, Facebook ran an experiment to manipulate emotions of users. This experiment was done without the oversight of an Institutional Review Board (the administrative body that protects human subjects in university studies). As someone commented on twitter, “probably nobody was driven to suicide” by this experiment, but we don’t know.\nFacebook allowed Cambridge Analytica to get information on “more than 50 million Facebook users,” which may have helped them manipulate the 2016 election.\nIn 2016, it was shown that Facebook was letting housing advertisers discriminate by race, in violation of federal law. Facebook said they would fix the issue.\nIn 2017, Facebook was still letting housing advertisers discriminate by race. Facebook said they would fix the issue.\nIn 2018, it was shown that Facebook was letting job advertisers target only men, in violation of federal law. Facebook said they would fix the issue.\nAs of March 2019, Facebook says “advertisers can no longer target users by age, gender and ZIP code for housing, employment and credit offers”, but it took years for this change to be made.\nOf course, Facebook is tracking you all over the web, not just while you’re on their site. And, it seems clear they maintain shadow profiles of people who don’t have accounts.\nThe thing that pushed me over the edge was Facebook saying they wouldn’t fact-check political ads. In general, Facebook does fact-check ads, and uses a third-party fact checking service. But, they won’t apply the same policy to political ads.\n\nFacebook is essentially an ad company, and they develop their platform to keep you hooked enough to serve you more ads. Although Mark Zuckerberg already has a net worth of ~$70 billion, he is still valuing making more money over doing the right thing. While most social networks operate on the same principles, other companies have left money on the table in order to do the right thing. For example, Pinterest now blocks anti-vaccination searches.\nSo, for my own personal sanity (less time looking at my phone!) as well as my principles (data companies should do good with data!) I decided to delete my account.\nEven though I knew I was doing the right thing, it was still really hard. I was an extremely active Facebook user. From when I joined the site in 2006 until I deleted my account in 2019, I almost always had a Facebook tab open on my computer, and as soon as I had a smartphone I had the app on my phone. Looking back at RescueTime data, it appears I spent about 15 minutes a day on Facebook most days. I was actually surprised to see such a low number, because it felt like I was on Facebook all day. I would probably click on my Facebook tab at least once an hour.\nBecause I was so attached to Facebook, I didn’t want to delete my account and lose all my data. So, instead of just hitting the button, I embarked on a journey to rescue the data I wanted to keep."
  },
  {
    "objectID": "blog/2019-12-29-Deleting-Facebook/index.html#how-i-did-it",
    "href": "blog/2019-12-29-Deleting-Facebook/index.html#how-i-did-it",
    "title": "Deleting Facebook",
    "section": "How I did it",
    "text": "How I did it\nI didn’t do any sort of tapering to my use. I posted a warning to my Facebook two weeks before I deleted my account, saying I was leaving the platform and giving people the opportunity to subscribe to my newsletter. The text of what I posted is on my GitHub if you want to see what I wrote.\nThroughout those two weeks I continued to use Facebook on both my computer and my phone. In fact, I might have been on Facebook even more during that time, as I worked my way through my friend list, reaching out to people and getting other contact methods.\nMy first move was to download all the data Facebook makes available to users through their Download Your Information tool. Facebook has instructions on how to do this. My main piece of advice is to think carefully about whether you want .html or .json files.\nHTML is probably the most appropriate for most (non-programmer) people, because the HTML file structure makes it easy to browse through your data on your computer. The files kind of mimic the Facebook experience, so you can click on a file called your_photos.html and see a page with links to each of your Facebook photo albums. If you click through to an album, you will get a page with each photo and the associated comments on the picture. It’s a nice interface.\nHowever, the HTML file structures are nasty. I didn’t see the JSON option, so I thought my only option was to use the HTML files. In order to parse out my friend list to a useful csv file, I had to use rvest. My code is available on GitHub in case you find yourself in that same situation. But, to avoid this, I recommend getting at least some of your information in JSON format. Eventually, I downloaded all my data twice, once in each format.\nOnce I had the parsed csv file, I turned it into a Google Sheet. This is both a backup of my data and a centrally-accessible place to store it. (A side note– I have put my Facebook data all over the cloud. This might seem counter-intuitive, but I’m not worried about an individual hacker accessing my data, I wanted to remove myself from Facebook using my data in their ad machine.)\nOver the course of several days, I worked my way through my entire friend list (625 friends) and recorded some meta-data about them. I used Facebook as a sort of external brain for my social network. What is that person’s married name? Facebook knows. Which of the two people with the same first name did I go to college with, and which one is the partner of my colleague? Facebook knows. I knew that once I lost the ability to go to friends’ profiles to see their pictures and our list of mutual friends, I would lose a lot of my “memory” of them.\nFriends’ pictures and mutual friends don’t come with your Facebook data download. I decided I didn’t need all that data (which I could have scraped) but that I would go through and categorize each person with a “rank,” a general way I knew them, and then any additional notes I would need in order to recall who they were.\nMy rank variable was a value from 1 to 4, with 1 being my very best friends and close family, and 4 being people I probably should have unfriended because we had almost no connection. It felt a little scummy to rank people like that, but I know Facebook probably had an algorithmically-generated score in the background already, to determine whose posts I saw most often. I went through my friend list in reverse order of becoming Facebook friends, and it was such an interesting exercise. A sort of Marie Kondo of the network. (As a side note, I loved Tokimeki Unfollow, which is exactly that for your Twitter follows.)\nAfter going through all my friends, here is the distribution of my friend rankings:\n\n\n\nDistribution of friend ranks\n\n\nSo interesting! It looks linear, and the numbers in each category seem to follow Dunbar’s number. I have 18 people I characterized as 1s (my support clique and sympathy group), 86 who are 2s (my active network), and 202 who are 3s (my personal network).\nIt was also interesting to see how I knew people,\n\n\n\nDistribution of how I know people\n\n\nI was homeschooled as a child, so that covers the elementary and middle school years. Cincinnati was my first year of college, Macalester was where I transferred and finished my BA, UCLA was my PhD. I like the big categories of “friend of a friend” and “family of a friend.” I used forcats::fct_lump to lump together small categories, because there were a lot of little categories that I’m less comfortable sharing with the wider internet! But again, cool insight.\nOnce I had my friend list in an accessible place, and notes to myself about how I knew people, I felt pretty comfortable with my data. But, there were still two more big things I wanted from Facebook that didn’t come in their data download:\n\nfriends’ birthdays\ntagged photos of me\n\nFacebook used to let users download their friends’ birthdays to a local calendar. However, they recently changed their policy, and that feature has been removed. I dug around online, and found a python scraper by Mo Beigi on GitHub. As he warns, using this script made facebook temporarily suspend my account because of suspected security issues. However, this was a good reminder to change my Facebook password (something which happened repeatedly as I was working on scraping data). The script worked so well that I don’t have a clear memory of it anymore!\nOnce I ran the scraper, I had a .ics file with all 625 Facebook friends’ birthdays. However, I didn’t want to add all of them to my calendar, just my 104 close friends. So, I had to parse the .ics file and filter out just those birthdays. This process was tricky, mostly because I couldn’t make the calendar package work on the ics file I got from the scraper. Again, my code is on GitHub, scroll down to get to the useful stuff.\nOne interesting insight from this process was that 21% of my close friends didn’t have their birthdays on Facebook. I’d guess this is in response to the feeling that Facebook is too invasive with their data collection. So now I have a list of people who I need to ask for their birthday!\nThe most challenging part of my data-rescue endeavor was rescuing my tagged photos. Again, I was a very active Facebook user with hundreds of friends. Over the 13 years I was on the site, I posted a lot of photos (1781, in fact). It was nice to get them in my Facebook data download. But, I would guess that I have almost all those files elsewhere, on my computer or my phone. The Facebook albums give me a curated selection of photos, and they’re batched into folders to make them easier to see. And of course, I also got the comments on my photos. But for the files themselves, not so useful.\nWhat Facebook does not give you in your data download are your tagged photos. It considers them to be your friends’ data, which I guess is fair. But, I wanted those photos! I had 878 tagged photos on Facebook, so going through them one by one and right-clicking to download didn’t make sense. I looked into scrapers online, and found several in python. However, I wasn’t able to get any of those scrapers to work. My suspicion is that Facebook changed their page structure to thwart those particular scrapers, and I’m not good enough at python to debug that.\nSo, I wrote my own scraper in R. This scraper was very likely not the most efficient– it used RSelenium to drive a browser, a for loop to click through each picture because I couldn’t think of how to vectorize it, and the whole thing happened in real time (the browser was up and I got to watch it go through all the photos). In RSelenium I just grabbed the image urls, because I couldn’t figure out how to do the download right there, and I used another for loop and the curl library to download the images after the fact. However, this scraping code has the advantage of being (I think) very easy to read, so if Facebook changes their page structure I’d guess it will be possible to adjust the code to work again.\nOnce I had my 1781 uploaded photos and 878 tagged photos on my computer, I decided I wanted some of them physically printed. Somehow, when those photos were on Facebook they felt “safe” and like I would have access to them forever. But once they were on my computer, even though they were backed up, they felt insecure. Of those ~2500 photos, I chose ~500 I wanted printed, and uploaded them to Shutterfly.\nI did my initial order before I deleted my Facebook account, and was able to use Shutterfly’s Facebook Connect feature to import some photos directly from my photo albums. However, tagged photos are again not available there, so I had to upload those manually.\nWhen I got my order, I was pretty shocked to see what ~500 photos looks like in a stack.\n\n\n\nStack of 500 photos\n\n\nImagine 2500 photos! I bought myself a photo album with those plastic sleeves, and have been working on getting the photos in there. I’ve got a few more to go, mostly because Shutterfly sent my prints in random order. I uploaded them chronologically, but that is not how the order arrives. Luckily, they do automatically print metadata on the back of their photos, so some of my prints came with Facebook captions, some came with dates, and some came with file names on the back. I haven’t spent enough time examining the pattern to be able to determine how those different pieces of metadata were chosen. But, they were all helpful when trying to remember when a photo was from.\nAs I’ve gone, I’ve tried to label the backs of photos with names and dates. However, this has proven to be pretty difficult! If I got to go back and do this again, I’d print and organize the photos before I deleted my Facebook account, because it became difficult to remember some names. If I’d still had my profile, I could have looked at who was tagged in a photo, or used my “external brain” to make connections with people who weren’t tagged (“she’s friends with Katie, let’s go look at Katie and my mutual friends”). As it was, I ended up messaging people on other platforms to see if they could help me. Actually a pretty fun social activity, but it added to the challenge.\n\nThe exercise of printing and organizing 500 photos made me want to do a larger art project and physicalize all my Facebook data– print all 2500 photos, and make all my Facebook comments and Messenger messages into a book, a la Nicholas Felton. Do you remember when he cataloged all communication for a year, and would print a book of all his communications with a particular person?\nI think this is the one of communications with his girlfriend:\n\n\n\nNicholas Felton’s book of communications\n\n\nI was awed by the physicality of that data, but I’m sure my Facebook data would be even larger when printed.\nWe’re getting a little out of order now, but one of the most important parts of the process is of course to delete you actual account! If you are leaving Facebook for the same reasons I was (abuse of data in support of ad revenue), you need to actually delete your account rather than deactivate it. If you deactivate your account, Facebook gets to keep using your valuable data. (Of course, they might actually keep using it after you delete your account, but that would go against their Terms of Use.)\nI posted my initial announcement that I was leaving Facebook on October 8. The following two weeks were a flurry of activity, much of it documented above (plus, my job and normal life stuff…). On October 22, I went through the deletion process.\nThe first thing Facebook asks is if you really want to delete your account, or just deactivate it. The default is deactivate. I chose delete.\n\n\n\nDo you want to deactivate or delete your account?\n\n\nThen, you’re given a warning, with the option to keep Messenger and to download your information. I had already decided to delete Messenger and downloaded my information twice (once as html and once as json). But, this step might slow you down again.\n\n\n\nPermanently delete account\n\n\nYou have to enter your password to confirm your identity.\n\n\n\nEnter password\n\n\nOne final warning,\n\n\n\nConfirm again\n\n\nand you’re deleted! (Well, not really, because they give you 30 days to change your mind, and if you log in during that time you have to restart the whole process!)\n\n\n\nDeletion email\n\n\nI was somewhat surprised that this was the last email I got from Facebook. I was expecting a flurry of “are you sure??” emails as the date of permanent deletion approached. But perhaps they have studied user behavior and realized that if you get this far you aren’t going to change your mind."
  },
  {
    "objectID": "blog/2019-12-29-Deleting-Facebook/index.html#the-way-i-feel",
    "href": "blog/2019-12-29-Deleting-Facebook/index.html#the-way-i-feel",
    "title": "Deleting Facebook",
    "section": "The way I feel",
    "text": "The way I feel\nIt’s been more than two months since I have logged in to Facebook (and a month since my account was actually deleted) and I have been shocked by how little I have missed Facebook. There have been a handful of times I’ve been somewhere and felt bored, anxious, or awkward, and have reached for my phone hoping to open Facebook. But, that’s about it. Unsurprisingly, I’ve been able to find other apps for that little twitch (something to work on in 2020!), so the lack of Facebook in particular hasn’t bothered me. I do know that I’m missing out on things. I miss being able to see regular posts of my friends’ kids (and, the kids of acquaintances who I probably can’t reach out to asking for photos!). I miss seeing what my far-flung family is up to. I know I’ve missed a couple event invites, and certainly the “events around town” feature that exposed me to things I wouldn’t have otherwise heard of.\nIt’s been interesting to see how my social connections have changed. I’ve transitioned one of my best friends to Signal for messaging, after years of using Messenger. Signal isn’t great for sharing photos, and we haven’t found a perfect solution. Right now we’re just sending emails with photo attachments. I’ve been getting more memes texted and emailed to me (several people have said, “I went to share it with you on Facebook and you weren’t there!”). Overall, it seems like my connections have gotten more personal.\nI’ve been talking to lots of people about my decision to delete my account, and a common refrain I hear from people is, “I could never delete my account because [x].” I know there are many great things about Facebook. It connects people with rare diseases across the globe, it gets used by parents to organize school-related events for children, and on and on. In particular, Facebook is a place for emotional labor, so it can be particularly hard for women and mothers to leave it. As Vicki Boykis has pointed out, leaving Facebook is a luxury that not everyone can afford.\nJust because I deleted my account doesn’t mean I’m judging you for not deleting yours. But, I would encourage you to think about whether you actually need it, or whether it just feels scary to go without. The more people who leave Facebook, the bigger a message we send that the platform can’t abuse our desire for social connection to sell more ads at any cost."
  },
  {
    "objectID": "blog/2018-02-11-Dagstuhl-reflections/index.html",
    "href": "blog/2018-02-11-Dagstuhl-reflections/index.html",
    "title": "Dagstuhl reflections",
    "section": "",
    "text": "Over the last 10 days, I had the pleasure of attending rstudio::conf and the Dagstuhl seminar on Evidence about Programmers for Programming Language Design. At rstudio::conf, I taught a two-day workshop on Intro to R & RStudio, and at the Dagstuhl I mostly thought about exposing novices to programming in scientific contexts. So, there was a lot of overlap.\n\n\n\nSchloss Dagstuhl\n\n\nSchloss Dagstuhl is a historic castle that hosts workshops on topics related to computer science. The idea is to bring a small group of people (ours was about 35) together for a week to discuss a specific topic, with the goal of advancing the discipline.\nIn this case, what we wanted to advance was evidence about programmers for programming language design. Most programming languages are designed by intuition. But, given that the people who write programming languages are not always the people who use them, and we may be quite different from most users, this is likely not the best strategy. To improve, we’d like to put the “science” in “computer science.”\nWe began the week with presentations from a subset of the attendees (I wrote a separate post about my talk, Scientists Programming). The talks were aimed to ground us in the disparate disciplines we came from, including programming language design, eye tracking, science, and education. Then, we spent the remainder of the time in discussion, both small-group and full-group.\nI was invited to the workshop by Andreas Stefik, the absolute master of Skype networking with other academics. Stefik is known for developing an evidence-based programming language, called Quorum. He was the only person I had interacted with at all before the conference, and our interactions had been limited to Skype and email.\nHowever, I discovered many connections with other folks once I was there.\n\nBrad Meyers - better language goals\nOne of the first presentations of the workshop was by Brad Myers, who spoke about types of studies. However, what stuck out the most to me was his modified version of programming language goals.\n\n\nBrad Myers started his #dagstuhl talk by comparing programming language design goals:(traditional)1. Correctness2. Performance (of code)3. Expressiveness4. Speed of compiling (usability)5. Understandability 6. Ease of reading7. Modifiability8. Learnability\n\n— Amelia McNamara (@AmeliaMN) February 5, 2018\n\n\nThese goals reminded me of my key attributes for a modern statistical programming tool paper.\n\n\nNeil Brown - data on student errors\nNeil Brown, who worked on Greenfoot and BlueJ (IDEs for novices learning Java) was there. He and I talked about a principle I have absorbed from Greenfoot, that novices should never be presented with a blank screen, but instead some scaffolding.\nBrown has data on many students’ Java code from Blackbox, a way to capture data from students using BlueJ.\n\n\nNext up, Neil Brown of @KingsCollegeLon on analyzing programming data. BlueJay is an IDE for Java beginners, Blackbox can capture data about it. They have data on 300 million compilations, which is ~3TB (compressed) of source code.\n\n— Amelia McNamara (@AmeliaMN) February 8, 2018\n\n\nTeachers think they are good at predicting errors, but they actually are not.\n\n\nTeachers are pretty certain they can predict what errors students will make. But, is that actually true?\n\n— Amelia McNamara (@AmeliaMN) February 8, 2018\n\n\nThis and many other great insights are in Brown’s slides.\n\n\nBrett Becker - better errors\nSpeaking of errors, Brett Becker talked about improving compiler errors.\n\n\nBecker: Error messages are accusatory and negative. Do we need to say “terminated” and “illegal”? “I’m going to get killed, and I’m going to jail.”\n\n— Amelia McNamara (@AmeliaMN) February 8, 2018\n\n\n\n\nFelienne - what is programming?\nI was delighted to meet Felienne in person after hearing Jenny Bryan talk about her, and following her on twitter for years. Felienne gave my favorite talk of the workshop, which ranged from her work on spreadsheets as programming to her more recent work teaching students about “code smells” in Scratch.\nFelienne asked “What is programming anyway?”\n\n\nWhat is programming anyway? @Felienne talking about 🧠head vs. ❤️. pic.twitter.com/9qCfMq80KW\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nShe pointed out that while “every programmer ever” says that “Everyone should learn programming,” the community can be quite restrictive about what it considers to be “real programming.” Funnily enough, while everyone seemed to understand what she was saying, I also got pushback about my talk.\nOften, we put users in one category and programmers in the other. But, there are many people who are “swimming in the water” of programming, and don’t even know they’re doing it. People who work deeply with spreadsheets are programmers, but they wouldn’t describe themselves that way. Again, this had reverberations with the scientists programming stuff I talked about.\n\n\nAfter spreadsheets, @Felienne started working on “code smells” in @scratch. pic.twitter.com/Ci9hGnesSh\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nFelienne asked us two questions:\n\n\nTwo dimensions: what is programming for? What is programming like? @Felienne making a case for creativity and self-expression. pic.twitter.com/GzzSRg5Rc0\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nOne answer to “what is programming like?” is writing.\n\n\nOne metaphor @Felienne likes is programming as writing. There are many things that “count” as writing. Can be for yourself, can be for others, can be creative. pic.twitter.com/eMlZhaJQ2D\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nI’ve used it a lot, so I think programming as writing is a pretty robust metaphor.\n\n\nBaker Franke - teachability and learnability\nAnother person I was delighted to meet was Baker Franke of code.org, who it turns out worked on the Exploring Computer Science (ECS) project before I joined it as a graduate student. At some point, I used the phrase “data the students can see themselves in” and he mentioned he’d written something about that in the early ECS curriculum. I’m now certain that’s where this idea (which I have fully integrated into my worldview) came from. It’s always fun to pin down those influences.\nFranke’s talk was also great, and teased out the subtle differences between learnability and teachability of a language.\n\n\nWe’ve been expanding our definition of language properties at #dagstuhl. @bakerfranke adding “teachability,” although he says this might not be a problem for language designers but a problem of pedagogy. pic.twitter.com/b1HlrgUJyH\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nHe echoed my feeling that consistency in teaching is so important (the reason why I teach only one R syntax in my intro classes)\n\n\nFor learners, consistency is good. Flexibility may not be helpful. @bakerfranke pic.twitter.com/g8NokrqC15\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nHe also has an idea about scaffolding in language design,\n\n\nMitch Resnik wanted @scratch to have a low floor, high ceiling, wise walls. @bakerfranke wants a low floor, high ceiling, narrow walls to start, and wider walls as you get closer to the ceiling.\n\n— Amelia McNamara (@AmeliaMN) February 7, 2018\n\n\nThis was just a selection of the talks that related most directly to my interests. If you want to see the rest of them, they are posted on the seminar materials page, and for more context on the people who attended (not all of whom presented), you can read everyone’s introductory slide, and check out the twitter list I made of all the participants who are on twitter.\n\n\nMore than talks\nBut, of course the point of the Dagstuhl was not just talks. It was making connections with other researchers. At every meal, we were assigned random seats to help us meet more people. We had structured time to think about programming language design studies we would like to see (thanks, Baker!) and time to discuss in groups. At the end of the last day, we developed a massive spreadsheet of current projects, future projects, and resources. I can see at least five potential projects I might lead or become involved with. The types of things I think about in terms of teaching statistics and R seemed to fit right in with the questions others at the workshop were asking– a novel feeling!\nI know I’m not the only one who feels this way. Several other folks have already written up their thoughts on the seminar, including Andy Ko, Brett Becker, and of course Felienne’s liveblogging. Johannes Hofmeister captured his ideas on video,\n\n\nMe, trying to capture all my ideas from the Dagstuhl seminar. Not a time lapse, actual speed of thoughts :D pic.twitter.com/kDolr7e8gb\n\n— Johannes Hofmeister (@pro_cessor) February 11, 2018\n\nI can’t wait to see where things go from here."
  },
  {
    "objectID": "blog/2019-08-17-Tidy-Dress/index.html",
    "href": "blog/2019-08-17-Tidy-Dress/index.html",
    "title": "A tidy dress",
    "section": "",
    "text": "That’s right ya’ll, I made myself a R hex logo dress!\n\n\n\nThe hex dress\n\n\nI’ve been thinking about this dress for at least a year, if not longer. But I finally had a summer where I wasn’t moving, and I decided to make it happen.\nThe first step was looking for a good nested hex image. I searched the internet, and came across a way to use magick to nest hexes. I love R and reproducible work, so I played around with this code for a while to see if it would do what I wanted. Pretty quickly, I ran into problems with non-standard hex sizes, as well as errors I struggled to debug. So, I turned away from code. 😭\nYou may or may not know that I started college at art/design school. While I realized design wasn’t for me, I have a year of color theory, art history, 3D design, digital design, etc., under my belt. This means I can kinda-sorta use Adobe Illustrator. I’m maintaining a version of CS6 on my computer to avoid paying for Creative Suite (sorry if that means my files are deprecated!).\nI gathered the RStudio hexes from their GitHub repo, and searched the internet to find a few additional hexes. My final design uses hex logos from blogdown, bookdown, broom, devtools, dplyr, feather, forcats, fs, ggplot2, glue, googlesheets, haven, hms, janeaustenr, knitr, lobstr, lubridate, magrittr, packrat, parsnip, plumber, readr, readxl, reprex, rladies, rlang, rmarkdown, roxygen2, rstudio, scales, shiny, skimr, stringr, testthat, tibble, tidyverse, usethis, visdat, withr, xaringan, and yardstick. Mostly, I picked packages that I personally use.\nThe most time-consuming part of this entire process was laying out the hexes. As I learned, not all hexes are exactly the same size. Some aren’t quite hexagons. And, when you’re going to make fabric, you need the right edge of the design to match up perfectly with the left edge, and the top with the bottom. There was lots of zooming in, fiddling with dimensions, and nudging objects by a pixel one direction or another.\nLuckily for me, I was in the midst of this nudging while I was at the Columbia, MD StatPREP workshop. I ran my idea past Danny Kaplan. He looked at my design and immediately could see it was too busy. I was nesting all these extremely colorful, highly saturated images right next to each other, and it was overwhelming. His initial suggestion was to leave blank spaces in between the hexes, but I really wanted tiles. So then he suggested making many of the hexes less opaque, with just a few at full opacity. This turned out to be genius, because the lighter areas were easier to match together when sewing, and hid some of the issues with the edge matching.\nInitially, I tried using R to generate random 0s and 1s to tell me which to make full opacity. This ended up looking terrible (true randomness doesn’t look random to humans!), so I ended up hand-choosing the opaque hexes. My goal was to emulate the way hexes often look on laptop lids (here’s my laptop on devlids).\n\n\nLaptop stickers spotted in the wild at the Southern MN Stat Chat! https://t.co/3RipGl3tI7 pic.twitter.com/KJVYGr6Vi7\n\n— Amelia McNamara (@AmeliaMN) April 26, 2019\n\n\nOnce I had the Illustrator file, I exported to JPG. I’ve written about making fabric posters on Spoonflower, so I knew about their upload process. You need to upload a TIF, JPG, PNG, or GIF. Can’t say why I chose JPG on this particular occasion. The important thing is to make sure your DPI (dots per inch) is high enough to look good. Spoonflower recommends 150 DPI, but I think I used 300 DPI. More on this later. The Illustrator file and exported JPG are both on my hexfabric GitHub page. If you want to adjust the design, include different hexes, etc., feel free to use and modify those files!\nAlthough I am mostly a procrastinator, I actually took my time on this project. So, once I had uploaded the design I got a “test swatch” printed from Spoonflower. I’m so glad I did, because I didn’t realize how big the hexes initially were! I think this was a result of the DPI, perhaps if I had used the recommended 150 DPI the hexes would have come out how I expected. No problem, I used the “smaller” button on Spoonflower and got the design the way I wanted.\n\n\n\nToo-big hexes\n\n\nIn another shocking bout of preparation, I decided to make a test dress out of a plain fabric before jumping in with the custom fabric. I chose not to use a pattern, but instead pattern off existing garments. I learned how to pattern from existing garments at a sewing makerspace in Northampton, MA while I lived there. Sadly, the makerspace is now defunct, but the owner does have a neat etsy shop! (Fun fact– I got a too-big conference tshirt from OpenVisConf a few years ago, and I used the material from that shirt to re-make a shirt that fit as my class project, patterning off a Shiny tshirt.) If you’re a loyal reader of this blog, you may remember that I get too attached to particular items of clothing, so patterning off existing garments lets me give old things new life. I modified my tshirt pattern from the makerspace class for the bodice, and patterned the skirt off an Old Navy dress I loved but had worn out.\nOf course, the challenge with drafting your own pattern is that you don’t get a nice set of instructions! Luckily, I had my t-shirt instruction booklet from the makerspace class, and a salesperson at my local fabric shop had told me about a pattern that sounded similar to what I was sewing, and suggested looking for a sew-along tutorial to explain how to do the clear elastic gathering for the skirt.\nI had two yards of grey jersey material, so I cut out all the pieces out of that to verify that two yards was a reasonable amount of fabric. For plain material, I think I could have gotten the dress cut from one yard of material, but I knew that with a patterned material I’d need more wiggle room. Satisfied that two yards was enough, I threw caution to the wind and purchased two yards of my new design from Spoonflower.\nIn the interim, I sewed up the grey practice dress. When working with stretchy material (like jersey), it’s best to use a Serger rather than a regular sewing machine. Sergers do “interlock” stitching, which holds together better on stretchy material. Luckily for me, my mom has a Serger, and she was willing to let me use it. Over a couple sewing sessions at her house, the dress came together. I was really glad I chose to do the practice dress, because I made all sorts of mistakes and discovered issues in my pattern. And, most importantly, while I was sewing at her house my mom mentioned POCKETS! My original Old Navy dress didn’t have pockets, so I hadn’t patterned them. But of course the internet knows how to make pockets. Those got added in as I went. With the grey dress done, I was ready when my custom fabric arrived.\n\n\n\nSmaller hex fabric\n\n\nOf course, you want to wash and dry fabric before you work with it, to avoid issues with shrinkage. I washed and dried my fabric and was on to the second-hardest part of the project: cutting out the pieces for the final dress. I wanted everything to be straight, upright, and ideally match up along seams. So hard! The other tricky thing was the neckband, which I knew would look busy if I cut it out of the patterned fabric. I thought of trying to buy some plain white jersey for the neckband, but the store I looked at didn’t have anything that matched the fabric weight. But, when my Spoonflower order arrived, I realized there was a little white edge outside the print area that I could use! Very fortuitous.\nThe sewing went much quicker on the tidyverse dress than the grey one, because I had worked out all the kinks ahead of time. I did my best to match up the pattern wherever possible, but because of the gathering in the skirt and the curved seams on the shoulders, it didn’t often happen. However, those low-opacity hexes proved to be very forgiving for overlap.\nAlthough I had access to a Serger for doing most of the work, I needed a different machine to finish the hems. Apart from the community and the classes, the thing I miss most about the sewing makerspace I used to go to is the coverhem machine. If you look at the hem of almost any commercially-made garment you have (especially if it’s knit), it probably was done on a coverhem. The back looks like Serger stitches, all interlocked, and the front has two rows of parallel stitches that look like they were done on a regular machine. As far as I can tell, there’s nowhere in Minneapolis to use a coverhem machine, so I had to learn to do knit hems on a regular sewing machine. I’ve been following crazyauntpurl since she had a blogspot blog, and she sews with knits on her standard machine, so I knew it was possible. Again, my local fabric shop employee helped me find a good twin needle I could use on my machine, and reminded me to make sure my needle position was centered and my machine was not on zig-zag. The hems turned out pretty well, although they “tunneled” a little, so if I was going to do this again I would find some interfacing to stabilize the seams.\nMy best friend came over for the very end of the sewing process to do some documentation.\n\n\n\nSewing the hems of my dress\n\n\nOnce the dress was finished, I put it on and she did an awesome photoshoot of me in front of all the photogenic walls in my neighborhood!\n\n\n\nThe hex dress in front of a different wall\n\n\nThe dress made its debut at the 2019 noRth conference, where it was a definite hit. I’m excited to see how much it sticks out in group photos!\nOf course, people on twitter immediately wanted their own version. If you’ve read this far into my post, I hope you can understand why I’m not getting into the custom-dressmaking business! I am definitely an amateur when it comes to sewing. As I said on Twitter,\n\n\nIf you want to edit the design, I’ve put the files on GitHub, https://t.co/47FC2uMMCn #rstats #tidyverse\n\n— Amelia McNamara (@AmeliaMN) August 16, 2019\n\n\nOf course, not everyone has the skills to sew their own projects! So, I started looking into some of the online retailers suggested by folks on Twitter. I haven’t seen any of these products yet, so I can’t really vouch for their quality, but if you want to make the leap here are a couple places you can buy things:\n\nTable cloth, cocktail napkins, tea towels, duvet cover, throw pillow, and more housewares (scroll down) available on Spoonflower via Roostery\nA bigger hex version is coming to Spoonflower soon! I think that version looks better on the duvet cover and other large housewares.\nA-line dress, miniskirt, laptop sleeve, and many more items (scroll down and click “Also available on”) on Redbubble.\nSimilar products on Redbubble using large hexes, including a shower curtain, throw blanket, and tote bag. Again, scroll down and click “Also available on” to see all options\nRegular mousepad, gel mousepad, baby burp cloth, bandana, and more on Zazzle.\nOf course, the things most people want are really hard to find quality vendors for. I’ve heard requests for tshirts (working on it!), dresses with pockets (this may be impossible), leggings (working on it! Although this may be a weird design for leggings…), and pants (this seems impossible, too).\nAs above, if you find a site where you can upload a design, feel free to use the files from GitHub to make your own products!\n\nI’m so excited to finally have this physical artifact I’ve been dreaming about, and I would love to see ya’ll get the same kind of joy!"
  },
  {
    "objectID": "blog/2015-01-14-FitbitColors/index.html",
    "href": "blog/2015-01-14-FitbitColors/index.html",
    "title": "Fitbit colors",
    "section": "",
    "text": "I have been a devoted Fitbit user for over a year now, and I think that as a device it is the sort of thing that a statistician would natually enjoy. It produces rich data about my day-to-day life, can be used to identify notable dates in my year, and does a great job of making me more physically active “on the margin.” But, the Fitbit app designers have made very strange color choices.\nI’d appreciate any insight you have about why they might have chosen these colors. Here’s the daily trajectory:\nWhen you get up and start moving, the graphs show your steps in an aqua blue that is very similar to the Fitbit dashboard color.\n\n\n\nAt the beginning of the day\n\n\nThen, as you continue, the colors move from aqua, to yellow, to orange/red.\n\n\n\nA little later\n\n\n\n\n\nGetting close to the step goal\n\n\nFinally, when you hit your step goal the color switches to green,\n\n\n\nGoal reached\n\n\nThis color progression drives me a little crazy. It’s not in rainbow order, and it seems like it should move from red to orange, to yellow, to green/blue. In other words, the colors should indicate something about your health level.\nAnother choice I can imagine for the colors would be to code the progression I mentioned above to the progress you’ve made over the day. The Fitbit knows how long your typical day is, and it knows when you woke up. Maybe it could show green if you were making good progress to your goal for the day (given the time of day it was), yellow if you weren’t, and red if you were really behind. Then, once you hit the goal it could switch to a different color (e.g. purple) to indicate you’d hit it.\nAny ideas about the color trajectory they’ve chosen? Do other fitness trackers use similar colors?"
  },
  {
    "objectID": "blog/2019-11-26-Key-Attributes/index.html",
    "href": "blog/2019-11-26-Key-Attributes/index.html",
    "title": "Key Attributes of a Modern Statistical Computing Tool",
    "section": "",
    "text": "A few weeks ago, I tweeted that the first paper to come out of my dissertation was published. I got my PhD in 2015, and it’s now 2019. That means I’ve been thinking about this work for more than four years (in fact, it’s probably closer to seven!).\nIf you want to read the paper, it’s now published in Volume 73, Issue 4 of The American Statistician. I was given 100 free e-prints, which are available until they run out here. A pre-print of this paper is also available on the arXiv if you want a non-paywalled version.\nI am so delighted to finally have this thing I have been thinking about since 2012 finally off my mental list and into the real world!\n\n\n\nThe print version of this paper!\n\n\nSome of the story of how this paper came to be is written into my dissertation, but I thought I would summarize it briefly here.\nEssentially, as a graduate student I had the opportunity to run some professional development (PD) for high school teachers to help them develop their R skills. The PD was about a week long, and it was for the Exploring Computer Science (ECS) curriculum. Over the course of the week, I managed to prove myself to the PIs of the grant (particularly Jane Margolis) because of my ability to communicate with both technical folks and tech-phobic people. The ECS team immediately found some hours for me to continue work with them, and then I was quickly incorporated into the next grant the team developed– Mobilize. The Mobilize project had some big goals, including developing data science technology and curriculum for the high school level, running PD, research and evaluation of our products, changing educational policy at the national and state level, and more.\nOver the course of the next three years, I ran hundreds of hours of professional development for teachers, mostly teaching them technical skills to support data science curriculum, but also some basic statistical concepts. This was extremely difficult task, because many teachers had never taken a statistics course (or if they had, it was years before), or programmed in any language. Over the course of the grant, we iterated on a number of tools. In order, we tried:\n\nbase R, with the standard R GUI\nDeducer\nformula syntax R, through an RStudio server\n\nThe experience of iterating through tools and attempting to teach them to novices gave me a lot of perspective on what makes a good tool for someone who is just starting out. Since then, I’ve taught R in many other contexts (in undergraduate college courses, at conference workshops for biostatisticians, data journalists, and generally for data practitioners).\nIn parallel to my experience starting to teach R and other technical tools, as a graduate student I had the time and opportunity to read hundreds of papers. This allowed me to think deeply about concepts from human-computer interaction, user experience design, and programming language design, and how these concepts interact with ideas of statistical computing. If you look at the references of my recent paper, I think it’s clear this paper came from a dissertation, because it has almost 100 references. The two most influential pieces to my dissertation (and this derived work) were\n\nThe Technical Tools of Statistics, by John Tukey (1965)\nSoftware for Learning and for Doing Statistics, by Rolf Biehler (1997)\n\nBoth of these papers lay out idealized attributes of statistical tools, and by clearly articulating the way tools should work in the future, they were able to manifest that future. (As Alan Kay says, “the best way to predict the future is to create it.”)\nIn Tukey’s case, his paper clearly influenced his own software work (e.g., prim9, a tool for visualizing high-dimensional data from the 1970s, which you interacted with using a light pen), as well as tools like S, S-Plus, and R.\nBiehler’s paper laid the groundwork for TinkerPlots and Fathom, two tools for teaching statistics that allowed learners to directly interact with their data and build intuition about statistics. Sadly, both TinkerPlots and Fathom are now mostly defunct, although The Concord Consortium is working on a next-generation tool called CODAP, the Common Online Data Analysis Platform (full disclosure, I have consulted with The Concord Consortium).\nI found these papers influential, and I saw an opportunity to reconsider the necessary components for statistical software again. Nearly 20 years had passed since Biehler’s paper, and advances in computer graphics and software in general have made more things possible. Working from the inspiration of Tukey and Biehler, and guided by my on-the-ground experience teaching R as well as my extensive literature review, I developed a list of ten attributes I think are necessary for a modern statistical computing tool. These are:\n\nAccessibility\nEasy entry for novice users\nData as a first-order persistent object\nSupport for a cycle of exploratory and confirmatory analysis\nFlexible plot creation\nSupport for randomization throughout\nInteractivity at every level\nInherent documentation\nSimple support for narrative, publishing, and reproducibility\nFlexibility to build extensions\n\nAs a tl;dr, I’ll summarize those for you here.\n\nAccessibility: The tool should be accessible to lots of people. It needs to work on all major operating systems, be free or cheap, and be accessible to people with disabilities.\nEasy entry for novice users: The tool should be easy to get started using, with little or no guidance. The “threshold” to success should be low.\nData as a first-order persistent object: Data should be the foundational object for a statistical tool. When you work with data, it should always be as a copy (looking at you, Excel).\nSupport for a cycle of exploratory and confirmatory analysis: It should be easy to iterate through the “data cycle” in the tool.\nFlexible plot creation: Statistical tools should allow you to create new visualization types, following the Grammar of Graphics.\nSupport for randomization throughout: As a pedagogical tool and a more flexible method for inference, it is important a tool support simulation methods like randomization and the bootstrap, both for numerical statistics and for graphics.\nInteractivity at every level: An analyst should find it easy to adjust parameters as they work and see the effects of parameters early in their analysis on later pieces of the analysis. Published results should be interactive to allow readers to do the same.\nInherent documentation: The tool should tell (or show) you what it is going to do. Textual languages should aim for function names that are as human-readable as possible, and graphical user interfaces should strive to visualize what is happening in each step.\nSimple support for narrative, publishing, and reproducibility: Whatever you do in your tool should be easily documented, whether as a static document with textual code or a list of operations in a GUI. Narrative and analysis should be encouraged to mix in literate documents.\nFlexibility to build extensions: The complement to 2), the tool needs a high ceiling, so you don’t “experience out” of the tool. It should be possible to implement new functions and extend functionality.\n\nI think these attributes encompass a lot of what is important for a tool. Over the years I’ve been thinking about this, I’ve refined the attributes and tried to make them as comprehensive as possible. But, I’m sure there are pieces that are missing. For example, it has been pointed out that I don’t talk about the correctness of the implementations. To me, that goes without saying– if it’s a tool for statistical computing, it needs to do statistics properly! But of course, we have seen that isn’t always a safe assumption. I explicitly didn’t touch things like computing speed, because I was considering this from a human usability perspective. If you have a good, usable tool, and it allows you to build extensions, someone will fix the speed issue. Much harder to fix are things like inherent documentation, which we have seen Hadley Wickham working through with things like pivot_longer(), the next generation of what was formerly gather(), formerly cast().\nMy goal with writing this paper was to give practitioners and educators a framework for evaluating a tool, and to inspire people who build tools themselves. I hadn’t seen anything like this since 1997, so I decided it was time to re-open the conversation. But, I do want it to be a conversation. What else should we be aiming for as we design the next iteration of statistical computing tools?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Amelia McNamara",
    "section": "",
    "text": "Photo courtesy Mark Brown\n\n\n\n\n\n\n\nMy work is focused on learning what makes it easier to do and to understand statistics, and my research interests include statistics education, statistical computing, data visualization, and spatial statistics.\nRecent notable work of mine includes\nFor a more detailed look at my recent work, see my writings and presentations.\nI have research projects in progress about the impact of R syntax on learning and teaching, the Modifiable Areal Unit Problem in spatial statistics, and the ways in which data analysts check their work. I employ several undergraduate research assistants and I am always willing to work with students on research projects."
  },
  {
    "objectID": "index.html#associate-professor-university-of-st-thomas-department-of-computer-and-information-sciences",
    "href": "index.html#associate-professor-university-of-st-thomas-department-of-computer-and-information-sciences",
    "title": "Amelia McNamara",
    "section": "",
    "text": "My work is focused on learning what makes it easier to do and to understand statistics, and my research interests include statistics education, statistical computing, data visualization, and spatial statistics."
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html",
    "href": "STAT360/rmarkdown/07-DataTypes.html",
    "title": "More Data Types and Syntax",
    "section": "",
    "text": "Look at the R object WorldPhones (by typing its name in your notebook or the Console and hitting enter)."
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-1",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-1",
    "title": "More Data Types and Syntax",
    "section": "",
    "text": "Look at the R object WorldPhones (by typing its name in your notebook or the Console and hitting enter)."
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#vectors-and-matrices",
    "href": "STAT360/rmarkdown/07-DataTypes.html#vectors-and-matrices",
    "title": "More Data Types and Syntax",
    "section": "Vectors and matrices",
    "text": "Vectors and matrices\n\nvec &lt;- c(1, 2, 3, 10, 100)\nvec\n\n[1]   1   2   3  10 100\n\n\n\nmat &lt;- matrix(c(1, 2, 3, 4, 5, 6), nrow = 2)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 2)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nmatrix(c(1, 2, 3, 4, 5, 6), nrow = 3)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#math-with-vectors-and-matrices",
    "href": "STAT360/rmarkdown/07-DataTypes.html#math-with-vectors-and-matrices",
    "title": "More Data Types and Syntax",
    "section": "Math with vectors and matrices",
    "text": "Math with vectors and matrices\n\nvec + 4\n\n[1]   5   6   7  14 104\n\nvec * 4\n\n[1]   4   8  12  40 400\n\nvec * vec # element-wise multiplication\n\n[1]     1     4     9   100 10000\n\nvec %*% vec # matrix multiplication (inner)\n\n      [,1]\n[1,] 10114\n\nvec %o% vec # matrix multiplication (outer)\n\n     [,1] [,2] [,3] [,4]  [,5]\n[1,]    1    2    3   10   100\n[2,]    2    4    6   20   200\n[3,]    3    6    9   30   300\n[4,]   10   20   30  100  1000\n[5,]  100  200  300 1000 10000\n\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nt(mat) # transpose\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#arrays",
    "href": "STAT360/rmarkdown/07-DataTypes.html#arrays",
    "title": "More Data Types and Syntax",
    "section": "Arrays",
    "text": "Arrays\n\narray(c(1, 2, 3, 4, 5, 6), dim = c(2, 2, 3))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    1\n[2,]    6    2\n\n, , 3\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    4    6"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#data-types",
    "href": "STAT360/rmarkdown/07-DataTypes.html#data-types",
    "title": "More Data Types and Syntax",
    "section": "Data types",
    "text": "Data types\nNumeric\n\n1 + 1\n\n[1] 2\n\n3000000\n\n[1] 3e+06\n\nclass(0.00001)\n\n[1] \"numeric\"\n\n\nCharacter\n\n\"hello\"\n\n[1] \"hello\"\n\nclass(\"hello\")\n\n[1] \"character\"\n\n\n\n\"hello\" + \"world\"\n\nError in \"hello\" + \"world\": non-numeric argument to binary operator\n\n\n\nnchar(\"hello\")\n\n[1] 5\n\npaste(\"hello\", \"world\")\n\n[1] \"hello world\"\n\n\nLogical\n\n3 &gt; 4\n\n[1] FALSE\n\nclass(TRUE)\n\n[1] \"logical\"\n\nclass(T)\n\n[1] \"logical\"\n\n\nFactor (danger zone!)\n\nfac &lt;- factor(c(\"a\", \"b\", \"c\"))\nfac\n\n[1] a b c\nLevels: a b c\n\nclass(fac)\n\n[1] \"factor\""
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn\nMake a vector that contains the number 1, the letter R, and the logical TRUE"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#quiz",
    "href": "STAT360/rmarkdown/07-DataTypes.html#quiz",
    "title": "More Data Types and Syntax",
    "section": "Quiz",
    "text": "Quiz\nWhat type of data will result?"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#lists",
    "href": "STAT360/rmarkdown/07-DataTypes.html#lists",
    "title": "More Data Types and Syntax",
    "section": "Lists",
    "text": "Lists\n\nlst &lt;- list(1, \"R\", TRUE)\nclass(lst)\n\n[1] \"list\""
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#data-frames",
    "href": "STAT360/rmarkdown/07-DataTypes.html#data-frames",
    "title": "More Data Types and Syntax",
    "section": "Data frames",
    "text": "Data frames\n\ndf &lt;- data.frame(c(1, 2, 3), \n c(\"R\",\"S\",\"T\"), c(TRUE, FALSE, TRUE))\nclass(df)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#naming",
    "href": "STAT360/rmarkdown/07-DataTypes.html#naming",
    "title": "More Data Types and Syntax",
    "section": "Naming",
    "text": "Naming\n\nnvec &lt;- c(one = 1, two = 2, three = 3)\nnlst &lt;- list(one = 1, two = 2, many = c(3, 4, 5))\n\n\nndf &lt;- data.frame(numbers = c(1, 2, 3), \n          letters = c(\"R\",\"S\",\"T\"), \n          logic = c(TRUE, FALSE, TRUE))\n\n\nnames(ndf)\n\n[1] \"numbers\" \"letters\" \"logic\"  \n\nnames(nvec)\n\n[1] \"one\"   \"two\"   \"three\""
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#subsetting",
    "href": "STAT360/rmarkdown/07-DataTypes.html#subsetting",
    "title": "More Data Types and Syntax",
    "section": "Subsetting",
    "text": "Subsetting"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#the-dplyr-way",
    "href": "STAT360/rmarkdown/07-DataTypes.html#the-dplyr-way",
    "title": "More Data Types and Syntax",
    "section": "The dplyr way",
    "text": "The dplyr way\nTo use the dplyr package, we need to load it.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#beatles-data",
    "href": "STAT360/rmarkdown/07-DataTypes.html#beatles-data",
    "title": "More Data Types and Syntax",
    "section": "Beatles data",
    "text": "Beatles data\nLet’s make a toy dataset to play with.\n\nbeatles &lt;- data.frame(\n  name = c(\"John\", \"Paul\", \"George\", \"Ringo\"),\n  birth = c(1940, 1942, 1943, 1940), \n  instrument = c(\"guitar\", \"bass\", \"guitar\", \"drums\")\n)"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#select",
    "href": "STAT360/rmarkdown/07-DataTypes.html#select",
    "title": "More Data Types and Syntax",
    "section": "Select",
    "text": "Select\nSelect is a way to extract columns from dataframes\n\nselect(beatles, name, birth)\n\n    name birth\n1   John  1940\n2   Paul  1942\n3 George  1943\n4  Ringo  1940"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-2",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-2",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn\nSelect just the instrument column"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#base-r",
    "href": "STAT360/rmarkdown/07-DataTypes.html#base-r",
    "title": "More Data Types and Syntax",
    "section": "Base R",
    "text": "Base R\n\nbeatles[2,3]\n\n[1] \"bass\"\n\nbeatles[ ,\"birth\"]\n\n[1] 1940 1942 1943 1940\n\nbeatles[ ,c(\"name\",\"birth\")]\n\n    name birth\n1   John  1940\n2   Paul  1942\n3 George  1943\n4  Ringo  1940\n\nbeatles$birth\n\n[1] 1940 1942 1943 1940"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-3",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-3",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn\nSelect just the instrument column using brackets\nSelect just the instrument column using a dollar sign."
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#filter",
    "href": "STAT360/rmarkdown/07-DataTypes.html#filter",
    "title": "More Data Types and Syntax",
    "section": "Filter",
    "text": "Filter\n\nfilter(beatles, name == \"George\")\n\n    name birth instrument\n1 George  1943     guitar"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-4",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-4",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#more-than-one-condition",
    "href": "STAT360/rmarkdown/07-DataTypes.html#more-than-one-condition",
    "title": "More Data Types and Syntax",
    "section": "More than one condition",
    "text": "More than one condition\n\nfilter(beatles, birth==1940, instrument == \"guitar\")\n\n  name birth instrument\n1 John  1940     guitar\n\nfilter(beatles, birth==1940 & instrument == \"guitar\")\n\n  name birth instrument\n1 John  1940     guitar"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-5",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-5",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn\nModify the code below to filter out the rows for which birth is 1943 or instrument is drums\n\nfilter(beatles, birth==1940 & instrument == \"guitar\")\n\n  name birth instrument\n1 John  1940     guitar"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#base-r-1",
    "href": "STAT360/rmarkdown/07-DataTypes.html#base-r-1",
    "title": "More Data Types and Syntax",
    "section": "Base R",
    "text": "Base R\n\nbeatles[c(FALSE,TRUE,TRUE,FALSE), ]\n\n    name birth instrument\n2   Paul  1942       bass\n3 George  1943     guitar\n\nbeatles[beatles$birth == 1940, ]\n\n   name birth instrument\n1  John  1940     guitar\n4 Ringo  1940      drums"
  },
  {
    "objectID": "STAT360/rmarkdown/07-DataTypes.html#your-turn-6",
    "href": "STAT360/rmarkdown/07-DataTypes.html#your-turn-6",
    "title": "More Data Types and Syntax",
    "section": "Your turn",
    "text": "Your turn"
  },
  {
    "objectID": "wikipedia.html",
    "href": "wikipedia.html",
    "title": "List of student-authored Wikipedia pages",
    "section": "",
    "text": "Here are some links to Wikipedia pages my students have authored in the past. I wrote about some frustrations with Wikipedia on my blog a few years ago, but I realized it would be better to have a centralized place to update about the pages that have made it! I’ve done the assignment four times now, once as a “rogue professor” and three times using the Wikipedia Education dashboard. I highly highly recommend using the dashboard. For more advice, see this presentation I gave at JSM 2020.\n\nData Communication and Visualization (Fall 2021)\n\nChristo Allegra\nCarmen Batanero\nIda Benedetto\nJeff Bezanson\nTim Hesterberg\nSusie Lu\nJan Malcolm\nChi Nguyen\nJonathan Schwabish\nJulia Stewart Lowndes\nApril Soetarman\nNoam Ross\nChristobal Valenzuela\n\n\n\nData Communication and Visualization (Spring 2021)\n\nRuth Agbakoba\nNick Horton\nJessica Hullman\nChantilly Jaggernauth\nJosh Katz\nJen Lowe\nThomas Lumley\nAyodele Odubela\nTawana Petty\nKarthik Ram\nAndreas Stefik\nLuke Tierney\nShirley Wu\n\n\n\nData Commuication and Visualization (Spring 2020)\n\nIngrid Burrington\nNadieh Bremer\nKarl Broman\nAndreas Buja\nJen Christiansen\nCatherine D’Ignazio\nAdam Harvey\nKyle McDonald\nRegina Nuzzo\nHilary Parker\nRoger Peng\nDavid Robinson\nRobert Simmon\nAntony Unwin\nColin Ware\n\n\n\nData Journalism (Spring 2018)\n\nMeredith Broussard\nJenny Bryan\nLynn Cherny\nAmanda Cox\nLena Groeger\nMark Hansen\nJanaya Khan\nJeff Leek\nGiorgia Lupi\nStefanie Posavec\nKim Rees\nJulia Silge\nVictoria Stodden\nJer Thorp"
  },
  {
    "objectID": "STAT220labs.html",
    "href": "STAT220labs.html",
    "title": "STAT 220 labs",
    "section": "",
    "text": "In Spring 2020 and Fall 2020, I embarked on an “experiment” of sorts to compare the formula and tidyverse syntaxes for teaching introductory statistics.\nPrior to this experiement, I had taught a full-semester introductory statistics course soley using formula syntax, as well as a full-semester course in tidyverse syntax. Both semesters were at my previous institution (Smith College). However, I had never compared the two syntaxes head to head. This made it difficult to compare, because groups of students vary so much semester to semester. In addition, when I wrote the instructional materials for a tidy semester (for example) I would be implicitly thinking of tasks that would be easy to do in tidy syntax.\nAt St Thomas, the STAT 220 are written to be software-agnostic (they are used across all the STAT 220 labs, including labs taught in Excel, Minitab, JMP, and SPSS), which allowed me to produce materials without being influenced as much by the tasks that would easy in a particular syntax. I developed pre-labs for each of the 11 labs in each of the two syntaxes.\nThe pre-labs are written and provided to students in RMarkdown format, and walk students through the code necessary to accomplish the tasks in the real lab for the week. Some code is provided in the document, but there are also blank code chunks and questions throughout the pre-lab. The pre-labs for the two syntaxes are typically similar in text content, but different in the code they display, although differences in the syntax mean that they occasionally deviate.\nWhen I teach in person, I guide students through th pre-lab material live, stopping periodically to give them time to work on questions. So, for the first several weeks of this experiment I was able to do this sort of live teaching. When the pandemic forced courses online mid-Spring 2020, I began providing the pre-lab material as YouTube videos students could watch asynchronously. In the Fall 2020 semester, each pre-lab has been provided as a YouTube video. During synchronous classtime, I help students get started on the real lab, and stay on Zoom to answer questions from students as they arise."
  },
  {
    "objectID": "STAT220labs.html#formula-syntax",
    "href": "STAT220labs.html#formula-syntax",
    "title": "STAT 220 labs",
    "section": "formula syntax",
    "text": "formula syntax\nI provide students with a cheatsheet entitled [All the R you need for intro stat](), which gives them an overview of the most important code for the semester.\n        Describing data. RMarkdown, YouTube video\n\n        Categorical variables. RMarkdown, YouTube videos\n\n        Quantitative variables. RMarkdown, YouTube videos\n\n        Correlation and regression. RMarkdown, YouTube videos\n\n        Bootstrap intervals. RMarkdown, YouTube videos\n\n        Randomization tests. RMarkdown, YouTube videos\n\n        Inference for a single proportion. RMarkdown, YouTube videos\n\n        Inference for a single mean. RMarkdown, YouTube videos\n\n        Inference for two samples. RMarkdown, YouTube videos\n\n        ANOVA. RMarkdown, YouTube videos\n\n        Chi-square. RMarkdown, YouTube videos\n\n        Inference for Regression. RMarkdown. YouTube videos"
  },
  {
    "objectID": "STAT220labs.html#tidy-syntax",
    "href": "STAT220labs.html#tidy-syntax",
    "title": "STAT 220 labs",
    "section": "tidy syntax",
    "text": "tidy syntax\nI provide students with a cheatsheet entitled All the R you need for intro stat which gives them an overview of the most important code for the semester. Each week, I post RMarkdown documents on Canvas (also hosted on GitHub for sharing), and accompanying videos on the YouTube playlist for the class.\nDescribing data. RMarkdown, YouTube video\nCategorical variables. RMarkdown, YouTube video 1, video 2\nQuantitative variables. RMarkdown, YouTube video 1, video 2\nCorrelation and regression. RMarkdown, YouTube video\nBootstrap intervals. RMarkdown, YouTube video 1, video 2\nRandomization tests. RMarkdown, YouTube video\nInference for a single proportion. RMarkdown, YouTube video\nInference for a single mean. RMarkdown, YouTube video\nInference for two samples. RMarkdown, YouTube video 1, video 2\nANOVA. RMarkdown, YouTube video 1, video 2\nChi-square. RMarkdown YouTube videos\nInference for Regression. RMarkdown YouTube videos"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html",
    "href": "STAT360/rmarkdown/09-Case-Study.html",
    "title": "Case Study: Friday the 13th Effect",
    "section": "",
    "text": "library(fivethirtyeight)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#task",
    "href": "STAT360/rmarkdown/09-Case-Study.html#task",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Task",
    "text": "Task\nReproduce this figure from fivethirtyeight’s article Some People Are Too Superstitious To Have A Baby On Friday The 13th:"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#data",
    "href": "STAT360/rmarkdown/09-Case-Study.html#data",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Data",
    "text": "Data\nIn the fivethiryeight package there are two datasets containing birth data, but for now let’s just work with one, US_births_1994_2003. Note that since we have data from 1994-2003, our results may differ somewhat from the figure based on 1994-2014."
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-1",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-1",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 1",
    "text": "Your Turn 1\nWith your neighbour, brainstorm the steps needed to get the data in a form ready to make the plot.\n\nUS_births_1994_2003\n\n# A tibble: 3,652 × 6\n    year month date_of_month date       day_of_week births\n   &lt;int&gt; &lt;int&gt;         &lt;int&gt; &lt;date&gt;     &lt;ord&gt;        &lt;int&gt;\n 1  1994     1             1 1994-01-01 Sat           8096\n 2  1994     1             2 1994-01-02 Sun           7772\n 3  1994     1             3 1994-01-03 Mon          10142\n 4  1994     1             4 1994-01-04 Tues         11248\n 5  1994     1             5 1994-01-05 Wed          11053\n 6  1994     1             6 1994-01-06 Thurs        11406\n 7  1994     1             7 1994-01-07 Fri          11251\n 8  1994     1             8 1994-01-08 Sat           8653\n 9  1994     1             9 1994-01-09 Sun           7910\n10  1994     1            10 1994-01-10 Mon          10498\n# ℹ 3,642 more rows"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#some-overviews-of-the-data",
    "href": "STAT360/rmarkdown/09-Case-Study.html#some-overviews-of-the-data",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Some overviews of the data",
    "text": "Some overviews of the data\nWhole time series:\n\nggplot(US_births_1994_2003, aes(x = date, y = births)) +\n  geom_line()\n\n\n\n\nThere is so much fluctuation it’s really hard to see what is going on.\nLet’s try just looking at one year:\n\nUS_births_1994_2003 %&gt;%\n  filter(year == 1994) %&gt;%\n  ggplot(mapping = aes(x = date, y = births)) +\n    geom_line()\n\n\n\n\nStrong weekly pattern accounts for most variation."
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#strategy",
    "href": "STAT360/rmarkdown/09-Case-Study.html#strategy",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Strategy",
    "text": "Strategy\nUse the figure as a guide for what the data should like to make the final plot. We want to end up with something like:\n\n\n\n\n\n\n\nday_of_week\navg_diff_13\n\n\n\n\nMon\n-2.686\n\n\nTues\n-1.378\n\n\nWed\n-3.274\n\n\n…\n…"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-2",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-2",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 2",
    "text": "Your Turn 2\nExtract just the 6th, 13th and 20th of each month:\n\nUS_births_1994_2003 %&gt;%\n  select(-date) \n\n# A tibble: 3,652 × 5\n    year month date_of_month day_of_week births\n   &lt;int&gt; &lt;int&gt;         &lt;int&gt; &lt;ord&gt;        &lt;int&gt;\n 1  1994     1             1 Sat           8096\n 2  1994     1             2 Sun           7772\n 3  1994     1             3 Mon          10142\n 4  1994     1             4 Tues         11248\n 5  1994     1             5 Wed          11053\n 6  1994     1             6 Thurs        11406\n 7  1994     1             7 Fri          11251\n 8  1994     1             8 Sat           8653\n 9  1994     1             9 Sun           7910\n10  1994     1            10 Mon          10498\n# ℹ 3,642 more rows"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-3",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-3",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 3",
    "text": "Your Turn 3\nWhich arrangement is tidy?\nOption 1:\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\ndate_of_month\nday_of_week\nbirths\n\n\n\n\n1994\n1\n6\nThurs\n11406\n\n\n1994\n1\n13\nThurs\n11212\n\n\n1994\n1\n20\nThurs\n11682\n\n\n\nOption 2:\n\n\n\n\n\n\n\n\n\n\n\nyear\nmonth\nday_of_week\n6\n13\n20\n\n\n\n\n1994\n1\nThurs\n11406\n11212\n11682\n\n\n\n(Hint: think about our next step “Find the percent difference between the 13th and the average of the 6th and 12th”. In which layout will this be easier using our tidy tools?)"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-4",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-4",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 4",
    "text": "Your Turn 4\nTidy the filtered data to have the days in columns.\n\nUS_births_1994_2003 %&gt;%\n  select(-date) %&gt;% \n  filter(date_of_month %in% c(6, 13, 20))\n\n# A tibble: 360 × 5\n    year month date_of_month day_of_week births\n   &lt;int&gt; &lt;int&gt;         &lt;int&gt; &lt;ord&gt;        &lt;int&gt;\n 1  1994     1             6 Thurs        11406\n 2  1994     1            13 Thurs        11212\n 3  1994     1            20 Thurs        11682\n 4  1994     2             6 Sun           8309\n 5  1994     2            13 Sun           8171\n 6  1994     2            20 Sun           8402\n 7  1994     3             6 Sun           8389\n 8  1994     3            13 Sun           8248\n 9  1994     3            20 Sun           8243\n10  1994     4             6 Wed          11811\n# ℹ 350 more rows"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-5",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-5",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 5",
    "text": "Your Turn 5\nNow use mutate() to add columns for:\n\nThe average of the births on the 6th and 20th\nThe percentage difference between the number of births on the 13th and the average of the 6th and 20th\n\n\nUS_births_1994_2003 %&gt;%\n  select(-date) %&gt;% \n  filter(date_of_month %in% c(6, 13, 20)) %&gt;%\n  spread(date_of_month, births) \n\n# A tibble: 120 × 6\n    year month day_of_week   `6`  `13`  `20`\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  1994     1 Thurs       11406 11212 11682\n 2  1994     2 Sun          8309  8171  8402\n 3  1994     3 Sun          8389  8248  8243\n 4  1994     4 Wed         11811 11428 11585\n 5  1994     5 Fri         11904 11085 11645\n 6  1994     6 Mon         11130 10692 11337\n 7  1994     7 Wed         13086 12134 12378\n 8  1994     8 Sat          9336  9474  9646\n 9  1994     9 Tues        11448 12560 12584\n10  1994    10 Thurs       12017 11398 11876\n# ℹ 110 more rows"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#a-little-additional-exploring",
    "href": "STAT360/rmarkdown/09-Case-Study.html#a-little-additional-exploring",
    "title": "Case Study: Friday the 13th Effect",
    "section": "A little additional exploring",
    "text": "A little additional exploring\nNow we have a percent difference between the 13th and the 6th and 20th of each month, it’s probably worth exploring a little (at the very least to check our calculations seem reasonable).\nTo make it a little easier let’s assign our current data to a variable\n\nbirths_diff_13 &lt;- US_births_1994_2003 %&gt;%\n  select(-date) %&gt;% \n  filter(date_of_month %in% c(6, 13, 20)) %&gt;%\n  spread(date_of_month, births) %&gt;%\n  mutate(\n    avg_6_20 = (`6` + `20`)/2,\n    diff_13 = (`13` - avg_6_20) / avg_6_20 * 100\n  )\n\nThen take a look\n\nbirths_diff_13 %&gt;% \n  ggplot(mapping = aes(day_of_week, diff_13)) +\n    geom_point()\n\n\n\n\nLooks like we are on the right path. There’s a big outlier one Monday\n\nbirths_diff_13 %&gt;%\n  filter(day_of_week == \"Mon\", diff_13 &gt; 10)\n\n# A tibble: 1 × 8\n   year month day_of_week   `6`  `13`  `20` avg_6_20 diff_13\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1  1999     9 Mon          8249 11481 11961    10105    13.6\n\n\nSeem’s to be driven but a particularly low number of births on the 6th of Sep 1999. Maybe a holiday effect? Labour Day was of the 6th of Sep that year."
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#your-turn-6",
    "href": "STAT360/rmarkdown/09-Case-Study.html#your-turn-6",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Your Turn 6",
    "text": "Your Turn 6\nSummarize each day of the week to have mean of diff_13.\nThen, recreate the fivethirtyeight plot.\n\nUS_births_1994_2003 %&gt;%\n  select(-date) %&gt;% \n  filter(date_of_month %in% c(6, 13, 20)) %&gt;%\n  spread(date_of_month, births) %&gt;%\n  mutate(\n    avg_6_20 = (`6` + `20`)/2,\n    diff_13 = (`13` - avg_6_20) / avg_6_20 * 100\n  ) \n\n# A tibble: 120 × 8\n    year month day_of_week   `6`  `13`  `20` avg_6_20 diff_13\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1  1994     1 Thurs       11406 11212 11682   11544   -2.88 \n 2  1994     2 Sun          8309  8171  8402    8356.  -2.21 \n 3  1994     3 Sun          8389  8248  8243    8316   -0.818\n 4  1994     4 Wed         11811 11428 11585   11698   -2.31 \n 5  1994     5 Fri         11904 11085 11645   11774.  -5.86 \n 6  1994     6 Mon         11130 10692 11337   11234.  -4.82 \n 7  1994     7 Wed         13086 12134 12378   12732   -4.70 \n 8  1994     8 Sat          9336  9474  9646    9491   -0.179\n 9  1994     9 Tues        11448 12560 12584   12016    4.53 \n10  1994    10 Thurs       12017 11398 11876   11946.  -4.59 \n# ℹ 110 more rows"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#extra-challenges",
    "href": "STAT360/rmarkdown/09-Case-Study.html#extra-challenges",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Extra Challenges",
    "text": "Extra Challenges\n\nIf you wanted to use the US_births_2000_2014 data instead, what would you need to change in the pipeline? How about using both US_births_1994_2003 and US_births_2000_2014?\nTry not removing the date column. At what point in the pipeline does it cause problems? Why?\nCan you come up with an alternative way to investigate the Friday the 13th effect? Try it out!"
  },
  {
    "objectID": "STAT360/rmarkdown/09-Case-Study.html#takeaways",
    "href": "STAT360/rmarkdown/09-Case-Study.html#takeaways",
    "title": "Case Study: Friday the 13th Effect",
    "section": "Takeaways",
    "text": "Takeaways\nThe power of the tidyverse comes from being able to easily combine functions that do simple things well."
  },
  {
    "objectID": "blog/2016-04-08-participatory_sensing/index.html",
    "href": "blog/2016-04-08-participatory_sensing/index.html",
    "title": "What’s wrong with being data-collecting pigeons?",
    "section": "",
    "text": "As you might know from my blog or twitter, I’ve been a Fitbit person since December 2013. I’m also very interested in participatory sensing and citizen science, and I’ve been thinking about the ways that data can be used for good, both on the personal and societal level.\n\nFitbit\nIn June 2014, I set up an IFTTT recipe to save my step data into a google spreadsheet every day. So, I have almost two years worth of data to play with.\n\n\n\nMy step habits\n\n\nAs you can see, I get an average of 10,000 steps a day, with quite a bit of variance. I hadn’t realized this until recently, but it seems like since I moved from LA to Northampton (August 2015) I’ve been getting progressively fewer steps. I’m trying to turn that around, but it hasn’t quite been captured by the loess line yet! I’ve certainly used the Fitbit data to make micro changes on the day scale, but this is my first time using longer-term data to try to make behavior changes.\nAfter years of hearing me talk about how useful the Fitbit feedback was, my dad recently purchased one, and he loves it. Apparently he has already changed his behavior to hit 10,000 steps a day. On the phone, he instructed me to &gt; invent two of three more things like that\nmeaning simple sensor tools to help you live a better life. That’s a big proposition (thanks, Dad!) but I can see some interesting developments in that space lately.\n\n\nWeMo Insight switch\n One is the WeMo insight switch, which I heard about on Nathan Yau’s blog. You should go read his telling of it, but it sounds like it (or tools like it) could help people keep track of their energy use and learn to use less. That seems to count as a sensor tool to help you live a better life!\n\n\nPigeon Air Patrol and Plume\n\n\n\nPigeon Air Patrol\n\n\nThen, I heard about the idea of personalized pollution sensors on the Not So Standard Deviations Podcast. Roger and Hilary were discussing how the Pigeon Air Patrol project strapped mini air pollution sensors to trained pigeons and then let them loose in London.\nObviously, this kind of procedure could be used to produce much more data than the official governmental sensors like those maintained by the EPA or DERFA, as is the case in the UK. From what I can see, it seems like the Pigeon Air Patrol was more of a publicity stunt than anything else, and the team has moved on to having humans carry the sensors in their daily lives. They successfully crowdfunded a campaign to create a small sensor device that can be fastened onto a person’s bag or bike to tract the air quality wherever they go.\n\n\nUsing participatory sensing data\nThere are two main ways that I see participatory sensing or citizen science data collection projects being motivated.\n\nFor the user’s own edification. This is how I see Fitbit. The company is probably using my data for their own purposes, but my reason for collecting step data is so that I can know more about myself.\nFor the use of researchers. In this paradigm, people who participate in the project are doing it to generate data for some type of study. They may not get any direct use out of the data themselves, but they are happy to help. This is how the pigeons (presumably) felt about collecting air quality data.\n\nIt’s not clear where in this system the Plume sensor will fit. Will people be paying attention to their own personal “pollution dashboard” to see how much exposure they’re getting? Or are they just data-collecting pigeons helping scientists learn more about air quality? Of course, the best projects manage to do a little of both.\n\n\nOldschool citizen science\n The Audubon Society probably deserves the award for longest-running citizen science project, for their Christmas Bird Count, which has been ongoing since 1900.\nVolunteers go out in the month of December, observe the bird species in their area, and report back to the Audubon. In a similar vein, the Cornell Lab of Ornithology organizes eBird, a year-round, volunteer-driven data collection method to track birds.\nIn this paper (cited almost 300 times!) the authors describe their commitment to good user-rewards for eBird participants. Although the people collecting data are doing so to help the researchers, the data collection process needs to feel fulfilling to them in order to motivate them to collect and input data. The researchers observed that their data submissions went up when they instituted more user-rewards, including data visualizations on the site that allow the citizen scientists to see their data in action.\nIn pretty stark contrast, the EPA has a toolbox for citizen scientists who are interested in gathering their own air quality data. However, it’s not clear if the process involves feeding the data back to the EPA for use, so it is just for the user’s own edification. I’m not sure there’s enough incentive there.\n\n\nData quality\n Of course, if you are going to have many people collecting cheap data for scientific purposes, you have to worry about data quality. On the NSSD podcast, that was one of the main things Roger and Hilary discussed about the Plume project.\nAs far as I know, this is an open problem, but I’ve seen some interesting work on data augmentation that looks promising. For example, this paper found that they could scale models created with eBird data to get very similar predictions to those produced by models trained on high quality standardized data. Another paper about data produced by Mechanical Turk workers suggests the use of the EM algorithm to produce probabilistic labels that have more reliable quality. Another awesome paper (by Andrew Gelman, among others) does election forecasting using non-representative polls taken on the Xbox using multilevel regression and poststratification.\nAll in all, I’m really excited about the possibility of more inexpensive sensor data to change the world! Certainly using tools like the Fitbit has changed my life, and I think there are things that could be done with the data (perhaps necessitating some more statistical research) that could benefit us all. As much as I love the idea of data-collecting pigeons, I want to be completely involved with the data collection process and getting something back from it myself."
  },
  {
    "objectID": "blog/2017-01-13-Amanda/index.html",
    "href": "blog/2017-01-13-Amanda/index.html",
    "title": "What are the chances my name is Amanda?",
    "section": "",
    "text": "I get called Amanda a lot. This tends to drive me crazy, because I think my real name is much more interesting. But, I realized recently that given the prior probabilities, it’s actually a very reasonable thing to call me.\n[Note: this blog post has some interactive elements, so it is probably more fun to read on my shiny server. And of course, if you want to see what I did, the code is on GitHub.]\nTo investigate this using data, I am using the babynames R package, which has data from the Social Security Administration. It includes all names that had at least 5 uses for a particular gender in a given year. Obviously, that leaves some people out, but actually this data includes most (documented) people in the United States. For more about this, see my data appendix.\nThe basic idea is to take my (approximate) age and see how likely it is that my name is Amanda. I look incredibly young, but given that I have a PhD and am a statistics professor, there’s a lower bound on how young I could really be. Lets assume that people talking to me believe I was born between 1980-1989, inclusive. So the question is, given that I was born then, what are the chances my name is Amanda?\nIn a blog post at the beginning of last semester, Mine Cetinkaya-Rundel linked to a 538 article from 2014 that approaches the problem from the other side– How to tell someone’s age when all you know is her name. Interestingly, although using this method would help confirm people’s suspicions that I’m a child (the Age|Name method would estimate I’m about 13 years old), that’s not how people tend to think about names and ages.\nInstead, people seem to be taking an approximate age and then just grabbing a name out of the hat. In other words, they are using the Name|Age method. With that in mind, let’s see how likely Amanda really is. I’m focusing just on girls names for this analysis, mostly because Amelia isn’t a common boy’s name (see my data appendix).\nIn the eighties, numbers for Amanda and Amelia were as follows:\nArmed only with the information that I was born between 1985 and 1989, there’s a 2% chance my name is Amanda. That’s actually pretty incredible! (In contrast, there’s just a 0.05% chance my name is Amelia.) And, we can figure most people remember at least the first letter of a name. So, what if we add the fact that my name starts with an A?\nIn the eighties, numbers for Amanda and Amelia (out of all A names) were as follows:\nNow, there’s a 15.7% chance my name is Amanda (and a 0.4% chance my name is Amelia). The only A name that was more popular than Amanda in the eighties was Ashley, which made up 19.5% of the female names starting with A. Of course, Ashley doesn’t have as similar of a sound to Amelia as Amanda does. I didn’t go this far, but we could also look at the names starting with Am– I think this would serve to solidify Amanda as the much more likely choice.\nSo, for those of you that have felt bad about getting my name wrong in the past– the data supports you!\nI’m not sure if doing this analysis has made me feel better about being called Amanda– for that, I just tell myself “they’re probably mixing me up with Amanda Cox.”"
  },
  {
    "objectID": "blog/2017-01-13-Amanda/index.html#your-name",
    "href": "blog/2017-01-13-Amanda/index.html#your-name",
    "title": "What are the chances my name is Amanda?",
    "section": "Your name",
    "text": "Your name\nDoes this trend hold up with the name people are always calling you? Are you a Jacob that always gets called Jason? A Kirstin that gets called Kristen? (I’m guilty of that one.)\n[This is the part of the post that is responsive, so you probably want to head to my shiny server.]"
  },
  {
    "objectID": "blog/2017-01-13-Amanda/index.html#data-appendix",
    "href": "blog/2017-01-13-Amanda/index.html#data-appendix",
    "title": "What are the chances my name is Amanda?",
    "section": "Data appendix",
    "text": "Data appendix\nI can get off-track when doing analyses, so here are a couple more thoughts.\n\nHow full is the data?\nOne thing I was worried about was how well the data really represented the population. Uncommon names are excluded for privacy purposes, and I thought maybe people were getting more (or less) creative with names over time. It turns out that may be the case, but only slightly.\n\n\n\nMissing data\n\n\n\n\n\n\n\nMin.\n\n\n1st Qu.\n\n\nMedian\n\n\nMean\n\n\n3rd Qu.\n\n\nMax.\n\n\n\n\n\n\n2.462\n\n\n3.535\n\n\n5.592\n\n\n5.406\n\n\n6.793\n\n\n9.374\n\n\n\n\n\nOn average, only 5% of people are missing from the data. That feels pretty good to me. The article mentioned above, 538 estimated that only 1% of the data was missing, which I’m not sure how they estimated.\n\n\nHow many baby boys are named Amelia?\nAnswer– not many. In 2004, the year with the most male Amelias, 14 baby boys were named Amelia. Or, someone checked the wrong box on a birth certificate.\n\n\n\n\n\nyear\n\n\nsex\n\n\nname\n\n\nn\n\n\n\n\n\n\n2004\n\n\nM\n\n\nAmelia\n\n\n14\n\n\n\n\n\n\n\nCreativity in naming\nOf course, there’s a lot deeper I could dig on this analysis. Just by scrolling through the data I noticed there are many other ways to spell Amanda, which didn’t get taken into account. (Maybe the creative spellings of Amelia balance it out.)\n\n\n\n\n\nname\n\n\nnumber\n\n\nproportion\n\n\n\n\n\n\nAamanda\n\n\n10\n\n\n0.0000006\n\n\n\n\nAmamda\n\n\n178\n\n\n0.0000104\n\n\n\n\nAmanda\n\n\n369690\n\n\n0.0215329\n\n\n\n\nAmandah\n\n\n16\n\n\n0.0000009\n\n\n\n\nAmannda\n\n\n23\n\n\n0.0000013\n\n\n\n\n\n\n\nA is for eighties\nYou can actually see the eighties pretty clearly when you look at plots of letters names start with. Girls’ names starting with A had been on the rise since the 1960s, but you see a local maximum in about 1984 and then a small decline before continuing to rise.\n\n\n\nLetters over time\n\n\n\n\n\nA over time"
  },
  {
    "objectID": "blog/2010-09-27-Stochastic_as-shit/index.html",
    "href": "blog/2010-09-27-Stochastic_as-shit/index.html",
    "title": "Stochastic as shit",
    "section": "",
    "text": "xkcd\n\n\nI’m really into this comic, and am going to try to use the phrase “stochastic as shit” in a serious academic context at some point in my graduate school career. It’s good to have goals, right?"
  },
  {
    "objectID": "blog/2018-02-11-More-guys/index.html",
    "href": "blog/2018-02-11-More-guys/index.html",
    "title": "More guys",
    "section": "",
    "text": "One incidental piece of the Dagstuhl is I thought more about the use of the term “guys.” While we were going around and doing our initial 3-minute introductions, I heard a lot of uses of “guys” that rubbed me the wrong way, so when I introduced myself I asked people to think about their gendered word choices throughout the seminar.\nI regret doing this, because I think it marked me as someone who cares mainly about gender issues (I was there for the PL design stuff!) and I don’t think anyone really understood my point. People did become more aware of when they were saying it (and we used “squirrels!” as an interjection when someone did it), but not in a way that fostered inclusiveness. I’ll try to outline my personal opinion on the term here.\nLike the results from the survey I linked in my previous post, I find it gender-neutral when people use the second person phrasing of “you guys.” This means it’s okay to address a mixed-gender group by saying “hey guys, what’s up?” “okay guys, let’s focus” “it’s time for us to go to lunch, guys,” etc. (That being said, I would encourage you to work toward saying something more inclusive, like “folks”, “peeps”, “ya’ll”, “everyone”, etc.)\nThe problem comes when you talk about people in the third person. In the survey, phrases like “we’re going to need to hire a Python guy” or “I met a great Erlang guy” were perceived by the majority not to be gender-neutral. This is the useage that people were abusing in the Dagstuhl. I often heard people say, “I was talking to the google guys,” “I know the guys who worked on that,” “the guys who study these things,” etc. If the speaker realized they had used the word guys, they would justify by saying “and they’re actually all men,” as if that made it okay. That is actually worse.\nAs a rule of thumb, I told someone at the Dagstuhl “if you can see the people in front of you, saying”guys” is okay. If you can’t see them, don’t use it.” This is perhaps an oversimplication, but it gets at that second-person (“you guys”) versus third-person (“those guys”) distinction.\nWe know that women are underrepresented in STEM, and perhaps particularly in computer science (the Dagstuhl seminar I attended was about 20% female, which seems to be the norm). Ed Yong outlines some of the issues in the beginning of his piece on increasing the diversity of scientists he quotes. To quote Ed,\n\nWomen in science face a gauntlet of well-documented systemic biases. They face long-standing stereotypes about their intelligence and scientific acumen. They need better college grades to get the same prestige as equally skilled men, they receive less mentoring, they’re rated as less competent and less employable than equally qualified men, they’re less likely to be invited to give talks, they earn less than their male peers, and they have to deal with significant levels of harassment and abuse.\n\nWhen you say “the guys at google,” you are erasing the women who work there, even if the people you happen to know are all male.\nI know lots of cool men, but when I talk about teams to my students (who are mostly female, because I work at a women’s college) I try to speak in gender neutral terms. If my students have heard me talk about “the people I know at google,” “the people I know at facebook,” “the people who work on the tidyverse,” etc., they can project themselves into that group. When they come to me for a contact at a company, and I put them in touch with a man, that feels incidental. I know lots of people, that contact just happened to be male. Plus, in between when I make those statements in class and when people follow up, teams often diversify and I meet more folks. So by starting with a gender-neutral, I’m leaving the door open for more diverse teams in the future.\nI don’t want to hear about gender, I want to hear about the work. If you are going to reference a specific person, of course it is okay to indicate their gender and use their pronouns. But, if you’re going to speak about a group of two or more people in the third person, please try to remove the word “guys” from your vocabulary. We’re all people.\n\n\n\nMany diverse face sketches\n\n\n[Image via prestosketching]"
  },
  {
    "objectID": "blog/2015-11-01-NEISM/index.html",
    "href": "blog/2015-11-01-NEISM/index.html",
    "title": "20th New England Isolated Statisticians Meeting (NEISM)",
    "section": "",
    "text": "Now that I am at Smith, I am technically an “isolated” statistician. It’s a funny term, because I don’t feel isolated. I have great colleagues in the Statistical and Data Sciences program (Ben Baumer, Jordan Crouser and Katherine Halverson), although I am the only person with a PhD in statistics. However, there is a great mailing list for isostats and a few local meetings, so I am happy to accept the designation if it means I get to participate in these great discussions, many of which are teaching-oriented.\nEarlier this month, I got to go to my first NEISM (New England Isolated Statisticians Meeting). It was great fun, and as always I got more out of the experience by livetweeting it.\nWe talked about many current issues that statisticians are facing, including the importance of p-values, technology in statistics, and what skills were necessary in the context of big data (surprisingly, that conversation turned to communication, not databases). We also talked about issues more specific to teaching the introductory statistics class, including the GAISE report, which promotes multivariate analysis but doesn’t take a particular stand on technology.\nA couple discussions got a bit heated.\n\nTables\nAt the meeting, we spent some time debating the importance of tables (e.g. the standard normal table) in teaching.\n\n\nWhat about tables? There's a range of responses- 3 days on tables, only technology, or something in between. I'm in between. #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\nInterestingly, some folks in the room were insistent that tables not be used, while @rudeboybert and I were both on the side of showing the tables in conjunction with technology.\n(If you are looking for a good way to integrate technology when teaching about normal distributions, the mosaic package functions xpnorm and xqnorm are fantastic. That’s where the preview image on the blog came from.)\n\n\nThe question of whether or not to abandon normal tables in intro stats led to vigorous debate #NEISM20\n\n— Albert Y. Kim (@rudeboybert) October 17, 2015\n\n\n\n\n@StatsbyLopez @rudeboybert Not taking a hard line. De-emphasize tables but don't ban them.\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\n\n\nResources\nWe spent some time collating conferences we liked,\n\n\nWe love the 'COTS. USCOTS, ECOTS, ICOTS. #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\nand journals,\n\n\nPopular journals include @signmagazine, @JStatEd, Chance, TISE. #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\n\n\nCommunication\nWe also discussed the importance of communication in statistics and “data science.” I mentioned the SDS program at Smith, and folks on twitter jumped in to tout the benefits of communication.\n\n\n@AmeliaMN @smithcollege I was in a program called Integrated Science and Technology that required frequent teamwork and presentations.\n\n— Data Science Renee (@BecomingDataSci) October 17, 2015\n\n\n\n\n@AmeliaMN I think this is so important. Students SBAT communicate results clearly to anyone, inc those without statistical knowledge.\n\n— Amy Hogan (@alittlestats) October 17, 2015\n\n\nWhile most of the isolated statisticians teach at small colleges, we also thought about the challenges of incorporating communication in large classes, and the peer-reviewed assignments in the JHU data science specializationn were mentioned.\n\n\nWhat to do about communication when you have hundreds of students? Maybe peer-review is the answer. #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\n\n\nGroup work\nAnother discussion that got heated, particularly on twitter, was the question of group work. We had come around to the idea that group projects were probably the best way to include communication in classes. It lets students do real analysis and present the results. Grading a few team projects is more manageable for the instructor than reading many individual papers. But, we acknowledged that there is no optimal way to select groups. One participant asked,\n\n\nHOW do you make groups work in classes? Assigning alphabetically doesn't work, nor by seating, nor students choosing. #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\nThere were many suggestions, including\n\n\n@AmeliaMN I ended up doing most the work no matter who I got in group. Maybe assign based on % of work done on previous. High %ers together?\n\n— BlinkPop (@emilyeifler) October 17, 2015\n\n\n\n\n@AmeliaMN i randomize from the class list, re-do weekly. really like the results from doing so. everyone encouraged to get to know everyone\n\n— Dianne Cook (@visnut) October 19, 2015\n\n\nand\n\n\n@AmeliaMN idea: students choose partner, then pairs randomly grouped? Benefit of comfortable w someone on team plus forced to work w new ppl\n\n— Data Science Renee (@BecomingDataSci) October 17, 2015\n\n\nHadley Wickham immediately piped up with a link to a paper called Effective Strategies for Cooperative Learning, which includes many of the strategies we discussed both in person and electronically. The executive summary of the paper is that you have to assign groups, you might want to think about outnumbered minorities, particularly at the beginning of a class, and you should take into account peoples’ schedules.\nThe paper also discusses the importance of peer ratings to assess participation, and there was a suggestion in the meeting to do this rating as part of the final.\n\n\nWhen evaluating groups, ask them to rate group participation (inc. themselves) & answer simple Qs about their analysis on the final #neism20\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\nBy doing this, you are able to weed out people who really did not participate. If they can’t list their explanatory and response variables without referring to notes or teammates, they didn’t participate fully in the project.\nHowever, there was some pushback on twitter about whether forcing students to work in groups is the right answer. As the paper says, “As we tell our students, we’re sorry if they’re unhappy about having to work in teams but the truth is that our job is not to make them happy— it is to prepare them to be professionals.” @KCombinator disagreed,\n\n\n@AmeliaMN why is your assumption that group work is crucial? Why not just let loners work alone if they wish? I do it for life.\n\n— Lambda Kansas City (@KCombinator) October 17, 2015\n\n\nThey recommended a few resources by [Paul Graham](http://www.paulgraham.com/,\n\n\n@AmeliaMN read @paulg. Particularly http://t.co/gwS2CvZT2O http://t.co/1faV6YPcl8 http://t.co/WY88ZX1HS5 http://t.co/yBzlwgwCys\n\n— Lambda Kansas City (@KCombinator) October 17, 2015\n\n\nMy question was,\n\n\n@KCombinator this is getting pretty philosophical! Are there enough jobs for everyone who would prefer not to work with others to have them?\n\n— Amelia McNamara (@AmeliaMN) October 17, 2015\n\n\nWhat do you think? Tell me in the comments."
  },
  {
    "objectID": "blog/2014-10-27-New-tools-for-data/index.html",
    "href": "blog/2014-10-27-New-tools-for-data/index.html",
    "title": "New tools for data analysis and journalism",
    "section": "",
    "text": "While the Computation + Journalism Symposium continues to be filled with fascinating inlets to journalistic projects and innovative computational approaches (like using drones to map garbage dumps) the component that always catches my attention is the demoing of new tools for data analysis.\nThis year’s symposium was hosted at the Brown Institute and was no exception– I saw many projects that approached elements of my theory about the future of statistical programming from different directions.\nMany of the data analysis projects were discussed on a panel about journalistic platforms. The panelists were\n\nGideon Mann, of Bloomberg, speaking about coLaboratory\nKareem Amin, of NewsCorp, talking about a forthcoming tool that uses custom HTML tags\nBrian Abelson, of enigma.io and the Tow Center, on streamtools from the NYT R&D labs\n\nAll the tools spoke to the ideas I am engaging with in my dissertation, which I found very encouraging. The context that I have been thinking most deeply about is the high school data science classroom, much like the Mobilize project, which I formerly worked on.\nHowever, data journalism is the other context that I had been hoping to find relevance in. The fact that the panelists were engaging with the same issues, of being able to get started quickly but develop new modules as needed, of publishability and the need for interactivity, suggests that the two seemingly-different contexts (high school students/teachers, and data journalists) might not be that different in their needs after all.\n\ncoLaboratory\nI had asked my twitterverse about coLaboratory in September,\n\n\nSaw this article on CoLaboratory, http://t.co/MylVNfZckI but would love to hear from people who use it. @hmason?\n\n— Amelia McNamara (@AmeliaMN) September 26, 2014\n\n\nbut got no response except for Hilary Mason responding that she didn’t know the project and didn’t know why her photo was used on the article.\nSo, it was great to hear about the project straight from the horse’s mouth. coLaboratory seems to be part of project Jupyter, and it offers an iPython notebook that allows you to collaborate with others. Sadly, all their documentation seems to be technical, without any of the “so what” that I look for when learning about a tool. I did find a google research blog post which gives a little more context and a few examples.\nEssentially, coLaboratory can produce interactive reports (meaning, with sliders and buttons for users to interact with). It also has some focus on making it easy to develop the interface, using simple commands to invoke interactive controls. However, once a user modifies a control they need to re-click on the “run” button to see the updated information. This is my main complaint with iPython notebooks in general, so it’s too bad that the version for creating interactive documents doesn’t manage to jump that hurdle. The other problem is that the source code really isn’t that readable when you get down to it.\n\n\nThelma\nThe tool that Amin was talking about had different advantages. First, it is a click-driven interface, so you click on data inputs and outputs and see the results in the browser. By allowing users to build analysis in this way, you make it easier for novices to get started easily while still making nice results. The product he was talking about is based on custom HTML tags, using Polymer web components.\nI don’t think that Amin wanted the name of the project to be made public yet, but he showed a demo and several people tweeted out the link,\n\n\nAs you do data analysis it updates the webpage: http://t.co/isWky1uNAV -@kareemamin #cj2014\n\n— Nick Diakopoulos (@ndiakopoulos) October 24, 2014\n\n\nand it appears to be called Thelma.\nObviously, Thelma needs a lot more work before it’s really made public. It’s not clear how to begin if you want to do something that doesn’t use the data and chart types already on the demo page, and even those aren’t exactly transparent. But, there’s a lot of potential here. The resulting charts are somewhat interactive, and updating happens as soon as you modify the inputs/outputs.\n\n\nstreamtools\nThen, Brian Abelson spoke about streamtools. Abelson was not involved with the development of streamtools, but he found it useful enough that he was willing to be its advocate on the panel. streamtools is a dataflow programming environment for working with live streaming data. Rather than waiting for data to be presented as a flat file, the lab wanted to allow users to deal directly with data as it streams in. Since I work with the Communications Design Group, it immediately made me think of Dan Ingalls’ fabrik (a visual programming environment in which users “wired” components together), but of course there is a rich history of this sort of blocks programming environments.\nThe tool does a great job of making explicit the links between components in the system, and giving a visual hint to the data flowing through the system. However, it seems that the intended user of the system is someone who knows quite a bit about data analysis and programming. The blocks do not cue you toward their behavior, and the “parts bin” (to steal a Lively term) is full of consise but not extremely descriptive labels.\n\n\nIEEE programming language rating\nThe other project (not from the panel) that caught my eye was Nick Diakopoulos’ interactive ranking of programming languages for IEEE Spectrum. Sadly, it seems like the interactive version is behind a paywall, but it’s worth the 99¢ to see it.\nThe ranking is based on a number of factors, such as position in google results, number of github repositories, and questions on stackoverflow. The interactive tool lets you see four different versions of the rating: IEEE Spectrum, Trending, Jobs, and Open. You can also filter the languages shown to only display Mobile, Web, Enterprise, or Embedded languages.\nBut the real beauty in this project is that you can manipulate the parameters used in the rankings to create your own version. If you don’t believe that google page rank is a good measure of a programming language’s ubiquity, you can drag that slider down or even zero it out. This lets you modify the procedure used to create the ranking, and it allows you to perform ad hoc sensitivity analysis. Many changes to parameters do not change the rankings, which could make a reader more confident in the ranking as provided by the IEEE.\nIn fact, the list is so robust to parameter manipulation that it was hard for me to tell if the ranking was being updated dynamically, or if you need to click the “Save as Custom” button to see the results! However, they do provide a great “Make a Comparison” button, which shows a sort of slopegraph/bumps chart (see Tufte) displaying which languages were displaced and to where.\nThis article/interactive can’t really be described as a “tool,” per se, but I loved seeing an example of analysis where the underlying data work was exposed. This is the sort of “interaction all the way down” that I think is the future of data analysis. Not only should creators of data products be able to interact with their work as they produce it, consumers of those products should have the opportunity to deeply critique the process by viewing a transparent representation of the analysis process.\n\n\nOverall\nI am so invigorated by everything that I saw at the symposium. It’s such a treat to be around people who are thinking about some of the same issues that I am.\nIf you want to learn more about any of these projects, they are almost all forkable on github, which is another one of the principles that I think all good software should adhere to! What other interesting projects have you seen recently?"
  },
  {
    "objectID": "blog/2014-10-01-KidPix/index.html",
    "href": "blog/2014-10-01-KidPix/index.html",
    "title": "Kid Pix",
    "section": "",
    "text": "The talk was a great mix of nostalgia and inspiration for me. I loved hearing about the principles that guided Hickman as he worked, like the idea of contextual menus, so kids never had to look in the menu bar. I was also brought back to my childhood, creating artwork, menus for pretend restaurants, birthday cards, notes, and much more, all in Kid Pix.\nHickman referenced two papers that inspired him– Walt Disney and User Oriented Software and Alan Kay’s Computer Software. Since I work with Alan, I’ve read Computer Software several times, and love the ideas about software being flexible enough to do things that the designer didn’t foresee, and the importance of computer users “creating” rather than just “using” (something that comes up a lot in the Mobilize project).\nHowever, I hadn’t seen the article on Disney before. When I read it, I was surprised to discover I had heard most of the anecdotes before from people in Alan’s group. The article is full of connections between the principles of Disney animators and how they could be applied by software developers. For example, “prepare the audience.” A Disney character will show you what it’s going to do before it begins the action– for example, by bending its knees before a jump. Software should do the same. The author suggests that a slow in/slow out approach to changing things on the screen can help a user be prepared for what is happening, and not get lost in a transition.\nKid Pix accomplished a lot of the goals outlined in these two papers. You could see the tools available to you on the left hand side, and you always knew to expect relevant contextual options to appear at the bottom of the screen. Everything was modifiable, including the Susan Kare-designed font stamps. Hickman explained that he’d loved the hieroglyphic-style font Cairo, designed by Kare for the original Macintosh, so he just “put it back” as the stamps you could use in Kid Pix. As a child, I loved the fact that the stamps were modifiable, and I think the goal of a program that is manipulable all the way down is wonderful for any age level.\nOne of the unique qualities of Kid Pix, compared to a more standard paint program, is that the process matters as much or more than the final product. Although this had been my experience as a kid, I hadn’t realized that it was explicitly intended to be that way. Listen to Hickman explain,\n{% soundcloud_sound 170212262 %}\nThe original version of Kid Pix was black and white, but Hickman soon created a second version with the software company Broderbund, which was in color and included sounds. This is the version I have the strongest memories of, although I think we had the black-and-white version first.\nI was struck by David Pogue’s review of commercial version, which Hickman showed us.\n\nKid Pix 1.0\nPros: Brilliant; hilarious; innovative; inexpensive.\nCons: None.\n\nSo that gives you something to aspire to, doesn’t it?\nIf you ever used Kid Pix, its sounds will immediately bring you back. Here is Hickman demoing some of the tools, including the popular eraser bomb tool that cleared the entire screen: {% soundcloud_sound 170212261 %}\nIf you want to try out the original (black and white) Kid Pix, James Friend has created a Mac emulator that runs it. For Hickman’s complete talk, see this video from Media Leaders or Hickman’s page on Kid Pix, which goes through a lot of the material from his talk. Thanks again to Cross Campus for hosting such a great event!"
  },
  {
    "objectID": "blog/2017-12-19-On-Microaggressions/index.html",
    "href": "blog/2017-12-19-On-Microaggressions/index.html",
    "title": "On microaggressions",
    "section": "",
    "text": "I was glad to see Karl Broman’s blog post on a troubling conference talk. In the post, Karl describes how the speaker repeatedly used the word “guys” to describe people involved in statistics, and how he and Hadley Wickham reached out to him in an effort to get him to change his ways.\nIf you were in the room during the talk, I’m sure the experience is still viscerally available to you, but for those who were not, I think Karl’s post falls far short of the necessary description to underscore the problem. As I said in a recent tweet, it’s hard to convey in words how icky the talk was. But, I’m going to make an attempt.\nThe talk was part of a session honoring Bill Cleveland. Other talks in the session were fantastic– in particular, Di Cook’s talk, where she replicated Cleveland and McGill’s findings live (!!). But this particular speaker, who has now come under scrutiny for sexual harassment, gave a talk that was broadly derisive and dismissive of anyone who is not a white man pursuing statistics. I have a record of many of the comments he made as part of email threads from the time.\nIn a talk that included “data science” in the title, he was dismissive of data science, making the comparison that statistics is like architecture (accompanied by a photo of the Sydney Opera House) while data science is like cookie-cutter tract housing (accompanied by a photo of a suburban development), managing to be derisive of both manual labor and the efforts of many of the people in the audience. He disparaged useR, saying that you should only go if you want to hear about “guys who made a nice plot.” The entire tenor of the talk was that work with data is less important than statistics, a completely bizarre take in a talk supposedly honoring Bill Cleveland.\nBeyond that, the speaker made it clear by his references, language, and tone, that he only considers contributions by men. Every researcher he mentioned by name was male (even though he was speaking shortly after Di Cook, clearly eminent in the field). This might seem like a justification for his gendered language, but I think it speaks to a deeper issue– he simply does not see contributions by women.\nThe most obvious indicator of his dismissal of women was his repetitive use of the word “guys,” as Karl has mentioned. The usage become more and more grating throughout the talk, but even on his first use of the word the colleague sitting to my left said “sexist” under her breath at the same time I corrected “people” under mine. On twitter, many people chimed in to Karl’s tweet at the time saying they found the comments jarring as well. In situations where he could easily have said “people,” “statisticians,” “data scientists,” “programmers,” or many other gender-neutral terms, the speaker always used the word “guys.”\nWhat makes this difficult to adequately describe is that in some contexts, “guys” is almost gender-neutral. However, the survey that Dave Harris linked suggests that “guys” is only gender-neutral if you say “you guys.” Phrases like “we’re going to need to hire a Python guy” or “I met a great Erlang guy” were perceived by the majority not to be gender-neutral. As Karl responded to someone saying she didn’t care about the use of “guys” as long as she was not excluded,\n\n\n\nin this context, you would have cared.\n\n— Karl Broman (@kwbroman) August 3, 2016\n\n\n\nThat’s what is so insidious about these types of microaggressions. When you try to describe them, they sound insignificant, even though for those who were present it was clear there was a pattern of dismissal. It was so blatant that\n\n\n\nsteam was literally coming out of @AmeliaMN’s ears\n\n— Hadley Wickham (@hadleywickham) August 3, 2016\n\n\n\nand,\n\n\n\nI’m still shaking with rage. Literally.\n\n— Karl Broman (@kwbroman) August 3, 2016\n\n\n\nAfter the talk, it was clear that some action needed to be taken. Through email, Karl, Hadley, Hilary Parker (looped in for her interest in the issue, although she wasn’t present at the talk) and I discussed that while the talk offended us personally, it wasn’t a clear-cut violation of JSM’s Code of Conduct. This left making a comment to him, either publicly or privately. I knew that I (as a junior, female, data science-y person) would get no respect from him, so I encouraged Karl and Hadley to call him out. The speaker had mentioned Hadley in his talk, and since both of them are more senior, male, and ASA Fellows, I hoped their words would carry some weight. Karl, Hadley, Hilary and I went back and forth via email working on drafts.\nInterestingly, we ended up softening the language in successive drafts. A first draft said people “were taken aback, to the point of being offended,” while the final version just says “were taken aback.” Karl initially mentioned that he “might also talk about the pedestrian carpenter data scientist vs the soaring architect applied statistician, but two criticisms without any positive comments seems too much.”\nWithin a week, the email Karl reproduced on his blog was sent, and after a couple months I followed up with Karl to see if the speaker had responded. He hadn’t, and we dropped the issue.\nI regret that I didn’t feel comfortable writing publicly about this at the time, and that I didn’t push harder for follow-up when I heard he had ignored the email. We now know this particular person has a (alleged) history of sexual harassment, and perhaps publicizing this talk would have opened the issue up earlier. As Mara tweeted,\n\n\n\nReason number 247 “just send an email, we’re all adults here” is operationally disastrous advice. Also, and a whole host of @RLadiesGlobal can back me up on this, sometimes communicating privately is the OPPOSITE of a good idea!\n\n— Mara Averick (@dataandme) December 19, 2017\n\n\n\nWith the #metoo movement in general and the recent attention on harassment in statistics, data science, and machine learning in particular, I am reminded of the Brandeis quote that “Sunlight is said to be the best of disinfectants.” Of course, this is only true if people believe the stories that are voiced, but we seem to be at a watershed moment. I regret not shining more light on this at the time, and hope that we can be more open about violations of community norms, even if they seem “minor.” They could be indicative of a larger pattern."
  },
  {
    "objectID": "blog/2014-03-04-Migration/index.html",
    "href": "blog/2014-03-04-Migration/index.html",
    "title": "Migrated posts",
    "section": "",
    "text": "I decided to hand-migrate the posts from my now-depreciated blogger blog, so while this blog is new, some of the posts are old. Hopefully that makes sense!"
  },
  {
    "objectID": "blog/2014-07-22-Coded/index.html",
    "href": "blog/2014-07-22-Coded/index.html",
    "title": "Code + Ed",
    "section": "",
    "text": "Last month, I had the opportunity to attend the eyeo festival as a student volunteer. For me, one of the most useful parts of the festival was the Code+ed Summit, which took place on the day before the official conference opening.\nThis was the second annual meeting of the summit (which I also attended last year) and it’s organized as an un-conference/wiki-style conference, meaning that there’s no set schedule when you arrive. Instead, the participants build the schedule as they arrive, writing down topics they want to talk about themselves or things they’d like to hear others discuss.\nHere’s what the board looked like when we arrived:\nWhat the board looked like when we showed up. (Shoutout to the awesome Intermedia Arts space!)\nYou can see that there are a number of different physical spaces available at all times, with sizes that accommodate from 10 people to “a lot.” The time is broken into four sessions, two before and two after lunch. I didn’t get a shot of the full schedule, but here’s what it looked like as people started arriving and filling it up.\n1Filling it in\nAs people started filling in the board.\nThis shot isn’t much better, but maybe you have an OCR algorithm that can catch all the names:\nTega Brain started a collaborative “hackpad” (hadn’t heard of this site before) so people could contribute notes, and those are available here. Uncharacteristically, I wasn’t taking good notes or tweeting during the summit, I think because I was so engaged in the material! With that said, here are some of my takeaways, in what I think is chronological order. If you want to know more about the summit, I really recommend the hackpad. There are many differing perspectives there!"
  },
  {
    "objectID": "blog/2014-07-22-Coded/index.html#big-data-and-data-science-for-beginners",
    "href": "blog/2014-07-22-Coded/index.html#big-data-and-data-science-for-beginners",
    "title": "Code + Ed",
    "section": "Big data and data science for beginners",
    "text": "Big data and data science for beginners\nDuring the first session, Jer Thorpe, Golan Levin and I led a session on teaching big data and data science to beginners.\nWe were talking about the skills that beginners really need. Do they need to understand SQL? Web scraping? Complex joins? Spinning up Amazon EC2 instances? I don’t know if we got all the way to this in the discussion, but I think these questions really point to a need for a second course. Do whatever you want in the first course (I don’t even think it needs to be the “easy” stuff, as long as it’s interesting) and resolve to cover the other stuff in the next one. This takes the pressure off fitting it all in!\nI believe this was the session in which we were talking about responsible web scrapers. Sometimes, scraping looks like “hacking,” especially if you’re someone who doesn’t know much about hacking. To alleviate this tension, someone (probably Jer) has his students include some text with their scaper, saying something like “we’re students in Jer Thorpe’s data art class. If you want to know what we’re doing with your data, feel free to contact us at this email address.” This simple act can change the way people view the scraper, and afterward they got overwhelmingly positive responses.\nFrom my experience, if you’re going to teach “data science” to beginners, you need to make sure that the vocabulary that you’re teaching is very limited. Obviously, this can expand as the students gain experience, but it’s crucial not to overwhelm them at first. I got a laugh with something like, “we’ve been very specific about the syntaxes we use with the students. Syntaxii? Syntaxonomies?” (Pretty sure it’s “syntaxes”)"
  },
  {
    "objectID": "blog/2014-07-22-Coded/index.html#new-medium-for-education",
    "href": "blog/2014-07-22-Coded/index.html#new-medium-for-education",
    "title": "Code + Ed",
    "section": "New medium for education",
    "text": "New medium for education\nI didn’t totally know what this theme meant, but we certainly had some interesting discussions. The two I remember most distinctly were focused on the iPad.\nI mentioned the “navigate/drive” strategy that is often useful when people are learning at the computer, especially if equipment is limited. One person (the navigator) is more of the thinker, deciding what the pair should be doing. The other (the driver) actually has their hands on the keyboard/mouse. After a set period of time, participants switch roles. This can be a bit freeing, because the driver doesn’t need to be focusing on the strategy as much, just the technical difficulties, and the navigator doesn’t get caught up in syntax issues. However, it takes a certain type of person to enjoy this breakdown. Personally, I know that when I’m talking about computer tasks with someone (often a student) I am just itching to get my hands on the keyboard. However, it’s definitely a better pedagogical strategy to have them do it themselves!\nIn the same vein, one participant in the “new medium” discussion said that his significant other, who is a teacher, uses an iPad in her elementary school classroom. At any given time, one student is chosen as “The Finger,” which means that they are the one who gets to touch the iPad. This is a fun role for the person who gets to be The Finger, and it really focuses the attention of the class.\nAnother participant had worked with one of the Federal executive departments, and was on a team that had something to do with daily briefings of top officials*. Typically, these briefs are created as pdfs and distributed as printed documents. But, a much more engaging way to display the information is to have an iPad app so that the official can ask questions and be shown answers on-the-fly. This department changed their briefing to iPads, with the option to still receive paper briefings if the officials preferred that. One particular official* is very tech savvy and loves the iPad, but was not being briefed on the iPad. When they investigated, the department learned that it wasn’t the official’s preference, but rather the briefer’s. With a paper report, both official and briefer can have a copy of the brief, and the briefer can keep a respectful distance from the official. The iPad requires a much more intimate connection, both heads bent over the iPad, manipulating the screen. It came out that the briefer felt this was disrespectful to the official.\nI thought this was an interesting intersection of culture and technology. From friends who work with the US Army, I understand that it’s engrained in army culture for a presenter to provide paper copies of presentation slides (e.g. PowerPoint) and pay attention to the most senior official in the room. When they turn the page on their document, everyone else turns the page, and the presenter is expected to move on to the next slide. The iPad example similarly comes from a desire for the official to retain their power, rather than giving the briefer the same (or more) agency. However, it’s not just culture– the iPad (or the app) could have been programmed to allow two devices to sync, so that the briefer could maintain a respectful distance but still “drive” the briefing.\n* I know the names of the department and the official, just trying to maintain some modicum of anonymity"
  },
  {
    "objectID": "blog/2014-07-22-Coded/index.html#language-and-teaching",
    "href": "blog/2014-07-22-Coded/index.html#language-and-teaching",
    "title": "Code + Ed",
    "section": "Language and teaching",
    "text": "Language and teaching\nThe biggest takeaway I got from this session was the importance of concreteness, both in terms of language and in terms of making tangible representations. The woman who made this point was from IDEO, and was talking more specifically about design meetings. She said that in her experience, you can talk about “a pen” but it’s always better to talk about “this pen.” This reminds me of Megan Erin Miller’s tactile design kit. This is a free (!) printable download that allows you to do web design using physical pieces. I saw Megan speak about this at eyeo 2013, and I was impressed with all the creativity-enhancing limits it places. For example, you can clearly see when things won’t all fit on a particular page, or when your buttons will overflow without some sort of collapsing menu. It would be fun to make something similar for statistics education, or even just use this same concept when planning computational projects.\nWe also spent some time talking about the difference between a prototype and a provocation. I think the difference is that a prototype is a proof of concept of something you could actually make. A provocation is something more conceptual, that you wouldn’t actually put into production. But, having it (again, physically) in front of a group can stimulate good conversation that wouldn’t happen otherwise. When you create provocations, you can make many very different iterations, instead of the many similar iterations that you might create as prototypes. Again, this is something I’m trying to bring into my own work. Instead of trying to conceptualize prototypes, I want to think more in terms of provocations."
  },
  {
    "objectID": "blog/2014-07-22-Coded/index.html#teaching-coding-in-high-school",
    "href": "blog/2014-07-22-Coded/index.html#teaching-coding-in-high-school",
    "title": "Code + Ed",
    "section": "Teaching coding in high school",
    "text": "Teaching coding in high school\nWe talked about “what doesn’t work” for quite a while in this session. I think this was a cathartic thing to do (there’s a lot that doesn’t work) but it probably would have been more useful to talk about what did work. Luckily, we did transition there eventually.\nI was glad to hear that most schools are moving away from things like Java and C++ towards simpler scripting languages, at least outside of AP Computer Science. Several people in the session were teaching Processing (even in high school geometry), and I also heard mention of JavaScript. I mostly teach high school teachers, and (of course) I’m teaching them R within RStudio.\nWe all seemed to agree that for effective computer science teaching, you need to find a balance between open-endedness and unpreparedness. It’s important to know that the problem can be solved, or that there’s something interesting there, but you don’t want to have come up with “the answer,” otherwise it won’t feel creative and interesting to students. We all also agreed that finding these interesting prompts is pretty tricky. One successful coding exercise that a teacher set was creating a “choose your own adventure” narrative using HTML pages. Students could make the narrative as complicated or as simple as they wanted, so it scaled well, and they could customize to their interests. The teacher who told this anecdote admitted that his example story was pretty simple and uninteresting, but that his students really ran with it.\nI was talking about the “recipe” idea that I’ve seen come up in some research on teaching/learning Java. Greenfoot (a learning environment for Java) uses the philosophy that students should never start with a blank page. Instead, they should be able to modify one line and then run something and have a result. This is like a recipe. “Starting from a blank screen requires design, and is an advanced exercise. It is something students encounter later, but not as the first contact” (Michael Kölling).\nAnother universal is that humans tend to view any change as bad change. So, if you’re going to show a few different tools you really need to scaffold the transition. It needs to seem like, “oh, we’ve reached the edge of the capabilities of this tool. I really wish we could do [X]” and then give them the tool to do [X]. But, you need to build up. If the new tool can’t do the things that the old tool could, it’s going to feel like a downgrade. A teacher gave the example of starting with Processing and then trying to sell students on R. As you might expect, that transition didn’t feel good to the students. Similarly, I’ve found that people will complain about any tool you give them, but if you try to change it they will complain! Similarly, any time the interface to a social networking site changes, all you see are complaints. Humans hate change!\nThe pitch\nOnce again, the Code+ed summit gave me tons of inspiration for my teaching practice and dissertation. I love the eyeo festival, but I think I get more actionable ideas from the summit. If I haven’t already made the pitch to you, allow me to do it now: the summit has been free to attend so far, and you can register separately from the eyeo festival. The eyeo festival sells out within minutes every year, but the Code+ed summit fills at a more civilized rate. And, there are often a few people who don’t show up, so you can also test your luck by just showing up at Intermedia Arts on the day of the summit and asking if they have space.\nAnd the best part is because it’s an unconference, you can make sure that it fits your interests. People have asked me, “was there a session on [X]?” and even if the answer is no, there could be next year! It’s a very casual event, and great for networking. If you work anywhere in the space of education and computation, I recommend attending!\nP.S., it was only as I was writing this post that I noticed that the name “Code+ed” can be read as “Code and Education” or “coded” (which itself has two meanings; either programmed or as in “coded language”). Nice job with the naming, organizers!"
  },
  {
    "objectID": "blog/2018-02-11-Scientists-Programming/index.html",
    "href": "blog/2018-02-11-Scientists-Programming/index.html",
    "title": "Scientists Programming",
    "section": "",
    "text": "At the Dagstuhl seminar, Andreas Stefik asked me to speak about “Scientists Programming.” I’m not sure whether he did this on purpose, but by assigning people their talk titles, he drew us a little outside our comfort zone and got us to think more deeply. “Scientists Programming” wasn’t a talk that I had in my pocket, and wasn’t what I would have proposed, but it was fun to pull together. It made me realize that scientists programming is actually one of my primary interests, although I’ve never written that anywhere before. Felienne wrote up a nice summary of my talk, but I wanted to write down some thoughts of my own.\n\n\n\nFemale lego scientist\n\n\nIn the talk, I broke scientists’ programming tasks three categories:\n\nmaking models to generate data\ngenerating data to make models\napplication programming.\n\nThat first category would include any scientist who has a theory about the world, and using modeling to try to represent some relationship, then runs the model to generate data. Once you have that data, you compare it to what you’ve seen in the real world to see how good your model is. I’ve seen this done in ecology with predator/prey relationships, but I’m pretty sure physicists, chemists, and many others use this same technique.\nThe second category is the one I think about the most, because it’s more statistical. In this paradigm, scientists generate data (through experimentation, surveying, observation, etc), and use those data to generate a model, which is likely statistical. Then, they assess how well the model does at representing the real world. I see these first two paradigms as two sides of the same coin.\nThe final category, application programming, is the one I know the least about. I know some scientists do things like program a Raspberry Pi to do some specific sensor activity, or program a robot to explore in a deep cave. However, I think that this category is usually in service of one of the above two (scientists are usually trying to observe something about the world). Since application programming is outside of my expertise, and I knew it would be the most familiar to the group at the Dagstuhl, I didn’t discuss this in my talk.\nYou can see my slides if you want to know everything I covered, but the meat of the talk was digging into the paper by Julia Stewart Lowndes, et al, “Our path to better science in less time using open data science tools”. This paper is a perfect case study of a group of scientists who “We thought [they] were doing reproducible science” by using Excel, writing up their process, keeping versions of files by changing filenames, etc. But, they were able to transition to a more robust workflow using git/GitHub, R/RStudio, and RMarkdown. This has made them more accurate, and reduced the amount of time they spend on analysis.\nTo me, this is crucial. Science is the source of a lot of the “ground truth” we rely on as a society. It determines which treatments get used by doctors, what interventions we try on the environment, and just our general knowledge of how the world works. If science is wrong, there are big implications. One common example are Excel errors in genomics research.\nI love that Lowndes’ group has made their code available on GitHub. This is useful on so many levels\n\nAnyone wanting to check their calculations can do so\nIf you are a scientist, you can use it as a reference for coding\nProgramming language designers can examine how scientists authentically code.\n\nIn my talk, I showed some arbitrarily-chosen code from their repo to display how the tidyverse syntax makes code more human-readable.\nmonitoring_indicator_mean_loc= monitoring_indicator_mean %&gt;% \n        left_join(., select(coastal_fish_loc, station_cleaned, lat, lon), \n                                    by=c(\"monitoring_area\" = \"station_cleaned\"))\n\ncoastal_fish_loc = coastal_fish_loc %&gt;%\n                    mutate(lat = DECWGSN,\n                           lon = DECWGSE) %&gt;%\n                    mutate(lat = ifelse(station_cleaned== \"ICES SD 31\", 64.207281,\n                                 ifelse(station_cleaned== \"ICES SD 30\", 61.679815,\n                                 ifelse(station_cleaned== \"ICES SD 29\", 59.705518,\n                                 ifelse(station_cleaned== \"ICES SD 32\", 60.108130,\n                                 ifelse(station_cleaned== \"Rectangle 23 & 28\", 63.505817,\n                                        lat))))),\n                           lon = ifelse(station_cleaned== \"ICES SD 31\", 23.511161,\n                                 ifelse(station_cleaned== \"ICES SD 30\", 21.269112,\n                                 ifelse(station_cleaned== \"ICES SD 29\", 21.264879,\n                                 ifelse(station_cleaned== \"ICES SD 32\", 25.772410,\n                                 ifelse(station_cleaned== \"Rectangle 23 & 28\", 21.351243, \n                                        lon)))))) %&gt;%\n                    mutate(station_cleaned = as.character(station_cleaned),\n                           station = as.character(station))\ntemp_long = coastal_fish_scores %&gt;% \n            select(monitoring_area,core_indicator,taxa, score1,score2,score3) %&gt;%\n            group_by(monitoring_area, core_indicator,taxa) %&gt;%\n            gather(score_type,score,score1,score2,score3) %&gt;%\n            ungroup()\n\nslope2 = slope2 %&gt;%\n        group_by(basin_name, core_indicator) %&gt;%\n        mutate(slope_mean_basin_indicator = mean(slope_mean_basin_taxa))%&gt;%\n        ungroup()\n\nbasin_n_obs = coastal_fish_scores_long %&gt;% \n                            filter(score_type==\"score1\") %&gt;% \n                            select(Basin_HOLAS) %&gt;%\n                            count(Basin_HOLAS)\nInterestingly, the programming language designers found this code hard to read and confusing. Although I’ve heard computer scientists complain about R many times before, I wasn’t anticipating this. I thought that the piped sequence of function calls would be pretty readable. Maybe it was the %&gt;% for the pipe rather than |, or maybe they would have preferred to see it as separate steps.\nI wish I had shown this example, taken from Garrett Grolemund’s Master the Tidyverse materials (which I also showed in my Intro to R & RStudio workshop).\narrange(select(filter(babynames, year == 2015, \n  sex == \"M\"), name, n), desc(n))\nboys_2015 &lt;- filter(babynames, year == 2015, sex == \"M\")\nboys_2015 &lt;- select(boys_2015, name, n)\nboys_2015 &lt;- arrange(boys_2015, desc(n))\nboys_2015\nbabynames %&gt;%\n  filter(year == 2015, sex == \"M\") %&gt;%\n  select(name, n) %&gt;%\n  arrange(desc(n))\nI think with this comparison, it’s easier to see why the piped approach is better for people. With the nested function calls, you have to do a lot of checking in your head to make sure the parentheses are matched, and it’s hard to follow the logic. The incremental approach is good, but you type the name of the data many times. In the last example, the data flows through the piped sequence in a clearer way.\nOne question that came up in my talk was “if we fixed the problems with R as a language, would you appreciate that?” and I didn’t have a great answer. As a user, I (and I believe most scientists) don’t feel the language issues that frustrate computer scientists. It just works for what I need to do. In fact, my talk introduced yet another idea that we came back to again and again– some people who use programming languages don’t think of themselves as programmers. I know many people who use R, but consider it a tool to do data analysis rather than something to program in.\nI think the thing I exposed people to that they were most excited about was RMarkdown. While it seemed most people were familiar with the idea of iPython/Jupyter notebooks, they hadn’t seen knitr before. The idea that they could create a fully reproducible journal article interweaving text and code was very appealling. I showed a simple example with HTML output including text, code, and code output, but over the next few days I demoed more complex (Rnw) examples from my own work that produce publication-ready documents. For example, this pdf of a paper for Computational Statistics was generated from this LaTeX/knitr file. While this group of computer scientists work on experiments surrounding programming language design, and therefore do the same types of data analytic tasks as many scientists, they didn’t have this piece of their workflow."
  },
  {
    "objectID": "blog/2016-04-25-OpenVisConf/index.html",
    "href": "blog/2016-04-25-OpenVisConf/index.html",
    "title": "Do you know Nothing when you see it?",
    "section": "",
    "text": "I’m currently at OpenVisConf and in awe of my fellow presenters. I did my presentation, “Do you know Nothing when you see it?” before lunch and have been slowly unwinding since then. A couple people asked for links to my slides and resources, so here they are!\nI saved my Keynote slides as HTML and posted them on my website. They seem even lower quality than in Keynote (how is that possible?!) but you can access them here.\n title slide image from flickr: hungry_i\nResources: [May be updated to be more descriptive if I get the energy.]\nRandomization and the bootstrap:\n\nDavid M Diez, Christopher D Barr, Mine Çetinkaya-Rundel. Introductory Statistics with Randomization and Simulation.\nJonathan Stray. “Solve Every Statistics Problem with One Weird Trick.” NICAR 2016.\nTim Hesterberg. “What Teachers Should Know About the Bootstrap.“\n\nGraphical inference:\n\nHadley Wickham, Dianne Cook, Heike Hofmann, and Andreas Buja. (2010). Graphical Inference for Infovis. IEEE Transactions on Visualization and Computer Graphics, 16(6).\n\nR packages:\n\nmosaic\nggplot2\nnullabor\n\nModifiable Areal Unit Problem and Gerrymandering\n\nAn Introduction to the Modifiable Areal Unit Problem\nChristopher Ingraham. This is the best explanation of gerrymandering you will ever see. The Washington Post.\n\nExploring parameter space:\n\nAran Lunzer and Amelia McNamara. (2014). It ain’t necessarily so: Checking charts for robustness. In IEEE Vis 2014.\nLivelyR introductory video\nSpatial Aggregation Explorer\n\nThank you so much to everyone who offered kind words about my presentation!"
  },
  {
    "objectID": "blog/2012-02-12-Whats_the-shashistic/index.html",
    "href": "blog/2012-02-12-Whats_the-shashistic/index.html",
    "title": "What’s the shashtistic on me?",
    "section": "",
    "text": "One of the major perks of becoming a statistician is hearing people (mainly drunk) who mispronounce the name of my field, earnestly repeating it over and over again in the hopes that I will tell them about “shashtistics,” or recite “the shashtistic” on them (generally, probability 1 that they are wasted)."
  },
  {
    "objectID": "blog/2016-06-23-Listservs/index.html",
    "href": "blog/2016-06-23-Listservs/index.html",
    "title": "Worth adding to your inbox",
    "section": "",
    "text": "For the most part, I get my data news from the web (blogs like flowingdata, simply statistics, stats chat) and twitter (check out the people I follow). However, there are a few email mailing lists I’ve joined that are worth the addional lines in my inbox. Here they are:\n\nData Is Plural by Jeremy Singer-Vine. A great source of open, interesting data sets.\nAbove Chart by Scott Klein. Historical data visualizations from newspapers.\nO’Reilly Data Newsletter. News about data (often skews toward computer science)."
  },
  {
    "objectID": "blog/2018-02-11-Syntax-comparison/index.html",
    "href": "blog/2018-02-11-Syntax-comparison/index.html",
    "title": "R syntax comparison",
    "section": "",
    "text": "For my rstudio::conf working, Intro to R & RStudio, I finally finished my cheatsheet on R syntaxes. I’ve been working on this for at least a year (😱 ), so I’m glad to see it out in the wild.\nWhen I posted the cheatsheet online, I got some critical feedback, which I would like to address in the form of a FAQ.\n\n\n\nR syntax comparison cheatsheet\n\n\nQ: Why didn’t you include data.table syntax?\nA: Mostly, because I don’t know data.table, so I wouldn’t have done a good job. Also, because I don’t know of anyone who is teaching data.table to absolute novices. However, if someone wanted to generate all the analogous tasks (summary statistics, plots, wrangling) in data.table and share them, I would be very interested.\nQ: Why did you use the word “syntax” when that’s not quite precise?\nA: Because that is a word commonly associated with programming languages. While these three paradigms may not be precisely three different syntaxes, they are certainly syntactic sugar or idioms within the language. I often refer to them as like accents within a human language. If you were learning English, you wouldn’t try to learn it with a British accent, an accent from the American South, and an accent from Australia. You’d learn it one way, and be able to recognize other accents.\nQ: Why didn’t you express [x] in this more efficient way?\nA: This usually comes up because of my use of\nmean(mtcars$mpg[mtcars$cyl==4])\nmean(mtcars$mpg[mtcars$cyl==6])\nmean(mtcars$mpg[mtcars$cyl==8])\ninstead of\ntapply(mtcars$mpg, mtcars$cyl, mean)\nand\nmtcars &lt;- mtcars %&gt;%\n    mutate(efficient = if_else(mpg&gt;30,  TRUE, FALSE))\ninstead of\nmtcars &lt;- mtcars %&gt;%\n    mutate(efficient = if_else(mpg&gt;30))\nIn both of these cases, I was erring on the side of\n\n\nmore verbosity\n\n\na more general pattern\n\n\nPersonally, although I learned R long before the tidyverse existed, I never got adept at the apply() family of functions. Every time I needed to use one, it was a fall down a rabbithole of “tapply? sapply? mapply?” (In fact, trying to create that tapply call for this post included just such a search.) While I would encourage people coding within the base R paradigm to learn these functions, I think the three-line version is easier to think through, and represents a much more common “style” that you will see on places like StackOverflow. In many cases, there is no apply equivalent, and you end up doing this sort of repetitive writing of the dataset name and nesting of statements.\nWith the mutate() call, even though it was unnecessary to include the TRUE and FALSE, I knew that it would provide a better pattern for students who wanted to make their own variables. If you want efficient to instead be “yes” and “no” character strings, you can just sub “yes” for TRUE and “no” for FALSE, without having to think about the magic that goes with logicals.\nQ: Why didn’t you show ggformula or the formula-based base R plots?\nA: I did! They’re just on the back of the sheet. I don’t feel like ggformula is a full-fledged graphics library yet, so I don’t want to present it as such. base graphics, lattice graphics and ggplot2 graphics are all full libraries, so I used those on the front of the sheet. As for formula-based base R plots, I think they are much less common online. My main use case for this cheatsheet is to give to students who I am teaching one particular syntax. For example, I used to teach completely in the formula syntax, but have switched to tidyverse syntax. However, sometimes when I demo code I switch to another syntax because it feels more natural for the task. Or, students google something and find code from another syntax. The cheatsheet serves as a grounding device to say, this is just another way to express the same idea I am familiar with. Since I (almost) never teach base R, I include it on the cheatsheet mostly as a foil to my argument that the tidyverse is more human-readable.\nDo you have other questions? Ask me in the comments!"
  },
  {
    "objectID": "blog/2012-02-08-The-Postal-Service-is-into/index.html",
    "href": "blog/2012-02-08-The-Postal-Service-is-into/index.html",
    "title": "The Postal Service is into data visualization",
    "section": "",
    "text": "I was just listening to “Nothing Better” by The Postal Service on Pandora and was amused to hear for the first time the lyric,\n\nI’ve made charts and graphs that should finally make it clear. I’ve prepared a lecture on why I have to leave.\n\nHopefully I can find more songs that reference data visualization and prepare a playlist!"
  },
  {
    "objectID": "blog/2014-08-01-Yes-And/index.html",
    "href": "blog/2014-08-01-Yes-And/index.html",
    "title": "Yes, and…",
    "section": "",
    "text": "One of my primary teaching techniques is the improv-inspired “yes and…” move. People who have been trained in improvisational comedy know that the first rule is always to build on what your fellow performers say. If someone says, “now we’re sitting in a cafe, having coffee” you should never respond, “no, we’re outside playing frisbee!” Instead, you should build, “yes, and the waitress is a robot!”\nWhen I was in college, I had an instructor whose response to any student comment would always start with “no.” Over the course of the semester, this became a running joke with the students in the class, because it didn’t matter how right your answer was.\nA typical example would be the instructor asking, “What programming construct am I using here?” A student would answer, “you’re using a for loop” And the response would come back, “No. Well, I guess it is a for loop. But I wanted you to say a control statement.” That one’s actually pretty tame, because both the student’s response and the instructor’s expected response could be considered correct. But this instructor would go through verbal gymnastics to always start with no, even if the student’s response was precisely correct and there was no alternative.\nAs a result, many students shut down completely. Even though we joked about the instructor’s love of “no” many people became (consciously or unconsciously) scared off from responding. By the end of the course, I was essentially the only one brave enough to raise my hand, knowing that no matter what I said I would get a negative response.\n1Yes And\nOf course, most teachers don’t go to this extreme. But, I know how easy it is to slip into a frame of mind where you want one particular response and no one is “getting it.” So, I make a concerted effort to always start my response to students with “yes” or something else similarly positive. “Sure, that model has some of the qualities I’m describing here. That’s a nice connection to the stuff we were talking about last week. I was looking for one of the models we’ve been discussing today. Does anyone have another idea?” In fact, I try to start with something positive, even if the answer is partly/mostly wrong. For each incorrect response, I try to put myself into my student’s frame of mind and figure out what the connection is that made them say it.\nIf I go through several responses and no one is saying what I expected, I’ll eventually say something like, “oh boy, here I go again, expecting you guys to read my mind!” Usually, this gets a laugh from the class. And by explicitly calling myself out when I’m asking unreasonable questions, it helps me reduce that behavior. Sometimes students can even be frustrated by my positivism, because they want to know the answer to the question. Since I’ll talk my way through almost any response, it detracts from the idea of a correct response. While I try to strike a balance, I think that the idea of many correct responses is very statistical."
  },
  {
    "objectID": "blog/2015-11-19-context_notes/index.html",
    "href": "blog/2015-11-19-context_notes/index.html",
    "title": "Contextual notes",
    "section": "",
    "text": "Because I tend to select books by how thick they are (super-fast-reader problems) and I am a glutton for punishment, I have been slowly working my way through two of David Foster Wallace’s books concurrently. I am reading a paper copy of Infinite Jest, but the Kindle edition of The Pale King. These contrasting experiences got me thinking about contextual notes, particularly in electronic media.\nFor Infinite Jest, I have two bookmarks, one for my place in the main text and one for my place in the endnotes. In fact, I sometimes have to employ a third bookmark because many of the endnotes have their own footnotes, which can run on for pages.\nIn contrast, the Kindle edition of The Pale King lets me click on a hyperlink to the endnote, and if there is a footnote in the endnote I can click that link, and on and on. Once I have reached the end of the rabbit hole, I can use the back button to step my way out of the layers and eventually find my way back to the text, just where I left off.\nEspecially for an electronic edition, where I couldn’t keep my thumb in the back of the book, being able to travel through the footnotes in this way has been invaluable. But, the paired experiences made me think more about how context is provided in electronic mediums.\nIn my dissertation, I wrote in an offhand way that\n\ninteractive articles on the web can be updated depending on parameter manipulations, and some news outlets have begun including contextual information in their articles. Rather than hyperlinks taking the user away from the page they were reading, New York Magazine includes ephemeral popovers, and Medium has instituted in-context comments linked to a particular paragraph or sentence.\n\nIn this quote, I was trying to refer to the sorts of contextual information that can be access unobtrusively if a reader wants to see it, or ignored if they don’t. Of course, I didn’t include a citation, and when I went back looking for these features, I found it hard to track them down. In fact, at one point my googling returned my own dissertation.\nI’ve now discovered that while Medium has retained in-context comments they have changed the default for these notes to be private. This really reduces the usefulness of the feature, and I was baffled when I went back to look for it. Finally, I saw a tweet from Memo Akten confirming that they used to have such a feature, but recently removed it.\n\n\nI guess y'all didn't notice @medium made side-notes private? Little notes to clarify, provide insight now not visible to public. how lovely.\n\n— Memo Akten (@memotv) November 5, 2015\n\n\nFor more about the original functionality, see this 2013 post on Medium. In it, the author (and CEO of Medium?!) writes that the notes can be used “as footnotes.” However, with the defaults changed, that’s not true until the original author goes back and changes the settings one by one.\n\n\n@xululululuuum @Medium no! I have to delete all my current 'side notes' and recreate them as 'comments'! No automatic way of doing it. #bad\n\n— Memo Akten (@memotv) November 5, 2015\n\n\nMemo and I aren’t the only ones noticing this. Once I found the original blog post, I noticed that most of the comments were actually from the past month and bemoaning the feature change. I was happy to see some references to other places where this is done well, including Tufte CSS which implements Tufte-style sidenotes in an HTML environment.\nThe New York Magazine feature seems to have survived, although I haven’t been able to find whatever article I was reading when I wrote about the popovers in my dissertation. I did track down this Niemen Lab piece which talks about an interview with Jon Stewart that uses the feature. Again, I don’t know what article it was that caught my eye while I was writing, and it was hard to locate a more recent piece with the feature.\nAnother place I have seen side notes used well is in Bret Victor’s explorable essays, like Learnable Programming. Leaving aside the fact that these essays are completely interactive, they do a nice job of balancing the information given to you in the stream of the narrative, and what is included in a Tufte-style side note.\nBret is really a font of knowledge, as you can see when you look at (for example) the contextual notes he provides alongside presentation slides on his website. If you want to watch him talk about the future of programming you can do that, but if you want to dig deeper you can look at the slides and side notes.\nWhen I started writing about this, I did some reading about David Foster Wallace and electronic endnotes, and came across this beautifully designed piece on The Atlantic. In the intro to the article, they note that when it was published in print they did a sort of static popover design, but they recently took the opportunity to redo the article with nested electronic endnotes.\n\n\n\nDFW atlantic\n\n\nSo my question is, if it feels so satisfying to be able to dig deeper into information if we need it, or gloss over the parts we don’t, why isn’t this type of feature more common? And what other features could be included in electronic media to give us more of the same sort of experience?"
  },
  {
    "objectID": "blog/2010-09-26-A-Counting_Problem/index.html",
    "href": "blog/2010-09-26-A-Counting_Problem/index.html",
    "title": "A counting problem",
    "section": "",
    "text": "I was going to title this post something like “day 3” or “day 23” but it’s hard to say where I should start counting. Whatever the number, it’s clear that I’m a fresh graduate student, and barely know what I’m in for.\nWe had our first day of classes on Friday, and I’m just sitting down to my first couple of light readings. So far, I’m delighted. Our upcoming seminar speaker is involved with just the sort of data visualization I am interested in (mostly spatial, geographic visualizations) so I’m very excited to hear his talk and get to meet him at the session afterward. The motivational readings for my programming class are discussing what constitutes a “document” and what “information” really means. Everything meshes with my liberal arts sensibility. I know that grad school is going to be full of hard math, but they’re easing us into it so nicely!\nAs you may have guessed, the name of this blog is a reference to Ian Stewart’s Letters to a Young Mathematician, which I’m re-reading at the moment. I’m hoping my blog will contain the same sort of useful insights about a career in the mathematical sciences, without succumbing to the tactic of a narratee."
  },
  {
    "objectID": "blog/2014-04-08-NICARthoughtsPt2/index.html",
    "href": "blog/2014-04-08-NICARthoughtsPt2/index.html",
    "title": "Thoughts from NICAR 2014: data, where does it come from?",
    "section": "",
    "text": "One of the strands I followed most closely at NICAR was the way people get and use data. Here are some of the things that surprised me or intersect with the stuff I’m thinking about for my dissertation.\nData resources\nI was surprised by the data resources that I hadn’t heard about before. NICAR maintains a list of databases that are available for purchase. These datasets don’t seem to be that expensive, and are pretty interesting looking. Anything from boat registration to something called the Social Security Death Master File, which sounds varying degrees of sinister depending on where you put the emphasis.\nFOIA requests\nThe most useful thing I came away with was the realization that I, as a private citizen, am entitled to make FOIA (Freedom of Information Act) requests. If the government, especially the Federal government, is keeping data, I can request and receive it. I’ve been given to believe that this can be a difficult and slow process, but I’m looking forward to trying it out. One of the tools that people were talking about excitedly at NICAR was the FOIA machine which automates your FOIA requests.\nWhen you make FOIA requests, it’s best if you have some idea of what you’re looking for, but nothing too specific. If it’s clear the story you’re pursuing when you make a request, it might be denied. But, you need to know that the data exist to be able to ask for them. Apparently this is very “journalism 101,” but blank government forms can give a good hint as to what data the government is collecting. Additionally, you can FOIA a “records layout” for a database, so that you know what the variables are before you make your official request for the data.\nPopulation, not a sample\nOne statistical idea that I hadn’t connected the dots on is that journalists often are working with data that is not a sample. This is obviously a problem that many “data scientists” encounter, because data are coming from sources like the internet, but in the case of government data, if you get the entire data set, it should be the entire population, not a sample.\nObviously, this changes the sorts of analysis tasks you might want to do. If you have the population, do you really need statistics?\nData are created by humans\nSpeakers seemed to understand that all data had been touched by human hands “at least once in the process,” but I thought that their understanding could have gone deeper. If you think that some data you receive was created completely by a machine, you’re mistaken! Even if it’s an automated process, humans designed the hardware, the software, the programming language, the data collection algorithm, the data types for collection, etc.\nPhysical metaphors for data analysis\nI also went to a session on “When Data Don’t Exist” where several journalists talked about situations where they weren’t able to find existing datasets and had to create them themselves. There were a couple examples given in the session, especially by Sarah Cohen, that felt like a physical metaphor for data analysis more generally. Several times through her career, Cohen had to go through boxes of paper to turn them into a dataset. At first, she was going through the boxes consecutively, but she learned that you have different questions of the data once you’ve completed the first pass. Instead, she recommends trying to go through the boxes randomly. This will help you get a sense of the data. Use 2-3 weeks for your first stab, and only then make an estimate of how the rest of the process will take.\nThis is a great metaphor. It drives home the idea that going through data changes how you think about data. And usually, the first time you collect data (whether by going through boxes or deploying a participatory sensing campaign) you won’t get your collection schema exactly right. You’ll look back at your data and say, if only I had [blank]. By making it explicit that you’re doing a first pass, not trying to do the whole thing at once, you might be able to escape the sinking feeling that comes after you’ve put too much time into something that’s not going to work.\nData that journalists encounter\nIt came up again and again that much of the data that journalists encounter comes in the form of a pdf. Apparently, it’s a common tactic for an agency to respond to a FOIA request by sending a pdf of data instead of the raw data file (csv, json, xls, etc). This makes it much harder to use the data, because you need to either go through it manually and re-enter the data (making pdfs the computer equivalent of Cohen’s boxes) or use some sort of technological solution. I heard OCR (optical character recognition) being thrown around quite a bit, but OCR really only helps you get text that was scanned into a digital format. Past that, the solution that had people talking at NICAR was tabula. Tabula turns digital data that seems unstructured to humans into structured data tables, saving you the work of typing it in yourself.\nThis is the second in a series of posts about NICAR. The first is here and the next is here."
  },
  {
    "objectID": "press.html",
    "href": "press.html",
    "title": "Press",
    "section": "",
    "text": "I was one of the women profiled by AmstatNews for their “Celebrating Women in Statistics” feature.\nThe essay Aran Lunzer and I wrote about histograms was mentioned in The Data Science Roundup\nNathan Yau of FlowingData posted about the histogram essay.\nEmily Kund of the Tableau Wannabe Podcast covered my OpenVisConf talk, How Spatial Polygons Shape Our World.\nI am occasionally mentioned in the Smith College “People News.” For example, my participation in the Youth, Learning, and Data Science Summit, the Women in Statistics and Data Science Conference (San Diego) and the Women in Data Science Conference (Boston) have received writeups.\nRoger Peng had me on Not So Standard Deviations as a guest for a special podcast episode. \nThe American Statistical Associated interviewed me for Who inspires you?, in AmstatNews. \nDavid Robinson mentioned me several times in his post useR and JSM 2016 conferences: a story in tweets. \nRobert Kosara wrote about my talk in his OpenVisConf summary blog. \nSiena Duplan mentioned me in her post on 10 takeways from 22 data visualization practitioners at #OpenVisConf. \nI was quoted in the KCET article on Introduction to Data Science: An Authentic Approach to 21st Century Learning. \nMy work was mentioned in this blog post about Lively R by Rob Gould. \nI appear briefly in this promotional video about the rOpenSci Hackathon. \nMacalester College recorded video interviews with me several times, about Pursuing a PhD in the sciences and how to Prepare for the unpredictable. \nMacalester College published a profile of me and my undergraduate research in Macalester Today."
  },
  {
    "objectID": "writing.html",
    "href": "writing.html",
    "title": "Writing",
    "section": "",
    "text": "Here are some links to recent writing I’ve done, both in peer-reviewed publications and on blogs.\n\nPublications\n\nMcNamara, A. Teaching modeling in introductory statistics: A comparison of formula and tidyverse syntaxes. Under review. pre-print.\n\nCetinkaya-Rundel, M., Hardin, J., Baumer, B., McNamara, A., Horton, N.J., Rundel, C. An educator’s perspective of the tidyverse Technology Innovations in Statistics Education, Vol. 14, Issue 1. (pre-print).\n\nMcNamara, A. Community engagement and subgroup meta-knowledge: Some factors in the soul of a community. Computational Statistics, Vol. 34, Issue 4 (pre-print and code)\nMcNamara, A. Key attributes of a modern statistical computing tool, https://www.tandfonline.com/doi/full/10.1080/00031305.2018.1482784”&gt;The American Statistician, Vol. 73, Issue 4. (pre-print, blog post) \nMcNamara, A., and Horton, N. J. Wrangling categorical data in R, The American Statistician, Vol. 72, Issue 1 (pre-print in PeerJ Collection)\nMcNamara, A., Horton, N.J., Baumer, B. Greater data science at baccaleureate institutions, Journal of Computational and Graphical Statistics, Vol. 26, No 4.\nMcNamara, A. Considering the gap between learning and doing statistics. International Handbook of Research in Statistics Education. In chapter 15, The Future of Research in Statistics Education. (2015)\nMcNamara, A. Barriers and opportunities for nurturing statistical thinking before college. CHANCE [Vol. 28, Issue 4](http://amstat.tandfonline.com/doi/full/10.1080/09332480.2015.1120126]. (2015)\nMcNamara, A. and Hansen, M. Teaching Data Science to Teenagers. Proceedings of the 9th International Conference on Teaching Statistics (2014).\nMcNamara, A. Dynamic Documents with R and knitr (book review). Journal of Statistical Software, Vol. 56, Book Review 2. (2014).\nCatllá, A.J., McNamara, A. and Topaz, C. M. Instabilities and Patterns in Coupled Reaction-Diffusion Layers. Physical Review E, Vol. 85, Issue 2. (pre-print). (2012).\n\n\n\nBlog posts\n\nMcNamara, A. Data visualisation by hand: drawing data for your next story. DataJournalism.com, March 24 2021.\nMcNamara, A. Counting commits and peer code review. Teach Data Science, July 28, 2019."
  },
  {
    "objectID": "pastcourses.html",
    "href": "pastcourses.html",
    "title": "Past courses",
    "section": "",
    "text": "Here are some links to courses I have taught in the past.\n\nUniversity of St Thomas courses\n\nSTAT 220: Statistics I. Fall 2018, Spring 2019, Fall 2019, Spring 2020, Spring 2021\nSTAT 220: Statistics I R labs. Spring 2020, Fall 2020\nSTAT 320: Applied Regression Analysis. Fall 2018, Fall 2019, Fall 2020, Fall 2021.\nSTAT 360: Advanced Statistical Software. Spring 2019\nSTAT 336: Data Communication and Visualization. Spring 2020, Spring 2021, Fall 2021.\n\n\n\nSmith courses\n\nSDS/MTH 291: Multiple Regression Spring 2016, Fall 2016\nSDS/MTH 220: Introduction to Probability and Statistics Fall 2015, Spring 2017, Fall 2017\nSDS 136: Communicating with Data Fall 2017\nSDS 236: Data Journalism Spring 2018\n\n\n\nMassMutual short courses\n\nMassMutual Data Visualization seminar Summer 2016\nMassMutual Data Visualization seminar Summer 2017\n\n\n\nUCLA courses\n\nSTAT 98t: Data Visualization Spring 2015\nSTAT 101a: Introduction to Data Analysis and Regression Fall 2013\nSTAT 102b: Introduction to Computation and Optimization Winter 2014\nSTAT 101c: Introduction to Regression and Data Mining Spring 2014\n\n\n\nMobilize\n\nProfessional development, 2011-2015"
  }
]