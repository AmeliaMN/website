I"•<p>One of the things I was most interested in learning about at NICAR was the processes and tools that journalists use when they‚Äôre doing data analysis. And the conference did not disappoint. I feel like I came away with a bunch of insight about how things are typically done, and a couple great places where crossover could be even more useful.</p>

<!--more-->

<p><strong>Data diaries</strong></p>

<p>Throughout the conference, I heard several people emphasize the need for a ‚Äúdata diary‚Äù to document the process of creating data-driven journalism. This is great! We should all strive to document our work to capture our own process and remember what we do. However, it seems like journalists have a workflow something like, as you do your analysis in Excel, write down in Word what you did. Others write longhand on a piece of paper. But, it would be much better to have the documentation of the process actually <strong>be</strong> the process. For example, if you do your analysis in a language like python or R, you don‚Äôt have to write out ‚ÄúI edited the third row, second column to say 10 instead of 10000.‚Äù Rather, the code both does the action and documents what happened. Obviously, commenting is important so that you remember what all your hacks were doing, but using a language rather than a graphical user interface makes the process so much more documented. In fact, you can wrap your code and comments in a narrative, using tools like the <a href="http://ipython.org/notebook">iPython</a> notebook or <a href="http://www.rstudio.com/ide/docs/authoring/using_markdown">R Markdown</a>.</p>

<p>In fact, one of my dreams for data journalism has been that all data-driven stories would be accompanied by a version of the data cleaning and analysis process. This documentation would be available to readers, who could modify and ‚Äúaudit‚Äù the code/process to see what their changes do. In my vision, you could also view the raw data, to see where everything started. However, hearing it from the journalistic prospective, that‚Äôs probably an unreasonable vision. Sarah Cohen was saying that if you wanted to open-source data, you would need to verify <strong>every cell</strong> in the data set, and have an editor go through it, too. It‚Äôs hard for me to imagine ‚Äúediting‚Äù a data set like that, but I can see why it‚Äôs necessary. If you did something wrong in your data set, it may not make any impact on the final result (that one value probably isn‚Äôt throwing off your mean or plot if it‚Äôs not an outlier) but if the public could inspect the data cell by cell, it could be a big deal to have a number wrong. I need to think more about this and see if there‚Äôs a solution. If you think of one, let me know in the comments!</p>

<p><strong>Working headlines</strong></p>

<p>Another piece of process that I heard referenced several times in the sessions on ‚ÄúThe Data-Driven Story‚Äù was the conversation starter, ‚ÄúTell me what your working headline is. Tell me what your first two paragraphs are.‚Äù I work with a lot of education experts, who like the strategy of ‚Äúsentence starters‚Äù which are phrases that help get students speaking in the language of the discipline, while guiding the conversation. This is similar. It‚Äôs a simple strategy, but it would be easy for an editor, even one not experienced with data, to understand the response from their journalist. It takes the conversation out of the realm of the data and back into the context. I think it could be a useful conversation (or <a href="http://en.wikipedia.org/wiki/Rubber_duck_debugging">rubber-duck debugging</a> topic) for statisticians and data analysts to work through.</p>

<p><strong>Lightning talks</strong></p>

<p>On a more meta level, I thought the exercise of lightning talks was extremely effective. The space was large enough for almost every attendee of the conference to be included, the talks were short enough that they could only tantalize, never bore, and the material was surprisingly complex. The event was kicked off with some stand-up comedy by <a href="http://www.andymboyle.com/">Andy Boyle</a>, who I‚Äôve come to discover is just as funny on <a href="https://twitter.com/andymboyle">twitter</a> talking about journalism.</p>

<p>For me, the best three lighting talks were by <a href="http://lenagroeger.com/">Lena Groeger</a>, <a href="http://www.chasedavis.com/">Chase Davis</a> and <a href="http://palewi.re/who-is-ben-welsh/">Ben Welsh</a>. Lena talked about ‚Äúwee things,‚Äù which I thought was a nice contrast to ‚Äúbig data.‚Äù Some of the wee things she discussed were small multiples, tiny text, mini maps,  iity-bitty images, and sparklines. The entire talk seems to be available <a href="http://vimeo.com/91429794">online</a> if you want to check it out yourself. She also recommended the book <a href="http://www.amazon.com/The-Information-History-Theory-Flood/dp/1400096235">The Information</a> which I‚Äôm really looking forward to getting into. Chase essentially ran through my entire statistics graduate education in 5 minutes, explaining five algorithms, including Latent Dirichelt Allocation and the Naive Bayes Classifier. The <a href="https://github.com/cjdd3b/nicar2014/tree/master/lightning-talk">code</a> is on github, although the presentation isn‚Äôt working for me. He apparently gave a similar talk the previous year, so if you want to catch up with <strong>everything</strong> I‚Äôve learned in my Phd, check out his <a href="https://github.com/cjdd3b/nicar2013">code</a> and learn about Principal Component Analysis, Random Forests, and more. Ben did a talk called ‚ÄúYou Must Learn,‚Äù although what stuck with me the most (more than what I was supposed to learn, oops) was the graphic styling of his presentation. Each slide was like a comic encapsulation of what he was talking about, and not in a dumb clipart way. The video of the talk is <a href="http://palewi.re/talks/">up</a> too.</p>

<p>I‚Äôm on the planning committee for useR! 2014, and I‚Äôm hoping that our lightning talks can be even half as awesome as the ones from NICAR. You heard me, statisticians, step it up!</p>

<p><strong>Lyra</strong></p>

<p>More of a tool than a process, but I was thrilled to be able to attend a live demo of <a href="http://idl.cs.washington.edu/projects/lyra/">Lyra</a> with <a href="http://arvindsatya.com/">Arvind Satyanarayan</a>, one of the creators of the project. Lyra is fascinating to me because it‚Äôs open source, it‚Äôs working with an existing framework, and it just makes extremely powerful data visualization tools available to people without much training necessary. If you haven‚Äôt already seen it, check out the gallery of plots they‚Äôve been able to reproduce with their tool.</p>

<p>*This is my third (and final) post about NICAR 2014. The other two are <a href="http://www.stat.ucla.edu/~amelia.mcnamara/blog/conferences/2014/03/04/NICARthoughtsPt1.html">here</a> and <a href="http://www.stat.ucla.edu/~amelia.mcnamara/blog/conferences/2014/03/04/NICARthoughtsPt1.html">here</a>.</p>

:ET