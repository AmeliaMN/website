\documentclass[10pt]{article}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{fancyhdr,url,hyperref}
\usepackage{graphicx,xspace}
\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{arrows,decorations.pathmorphing,backgrounds,positioning,fit,through}

\oddsidemargin 0in  %0.5in
\topmargin     0in
\leftmargin    0in
\rightmargin   0in
\textheight    9in
\textwidth     6in %6in
%\headheight    0in
%\headsep       0in
%\footskip      0.5in

\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}{Observation}
\newtheorem{lemma}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{answer}{Answer}
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\newtheorem{conjecture}{Conjecture}

\pagestyle{fancy}

\lhead{\textsc{Prof. McNamara}}
\chead{\textsc{SDS/MTH 220: Lecture notes}}
\lfoot{}
\cfoot{}
%\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\ans}{\vspace{0.25in}}
\newcommand{\R}{{\sf R}\xspace}
\newcommand{\cmd}[1]{\texttt{#1}}
\newcommand{\Ex}{\mathbb{E}}

\rhead{\textsc{September 22, 2017}}

\begin{document}

\paragraph{Agenda}
\begin{enumerate}
  \itemsep0em
  \item Mathematics behind linear regression
  \item Strength of Fit
\end{enumerate}

\paragraph{Mathematics behind linear regression}


Looking at the relationship between the $y_i$s, the $\hat{y}_i$s, and $\bar{y}$, we can see that
\begin{eqnarray*}
(y_i -\bar{y}) = (\hat{y_i}-\bar{y}) + (y_i -\hat{y_i})
\end{eqnarray*}
\vspace{1.5in}

We can use the relationship between those quantities to gain some intuition for this:
\begin{eqnarray*}
\sum_{i=1}^n (y_i -\bar{y})^2 &=& \sum_{i=1}^n (\hat{y_i}-\bar{y}) + \sum_{i=1}^n (y_i-\hat{y_i})^2 \\
SST &=& SSM + SSE
\end{eqnarray*}

$r$ is the correlation between two variables

\begin{eqnarray*}
r = \frac{1}{1-n}\sum_{i=1}^n \frac{x_i-\bar{x}}{s_x}\frac{y_i-\bar{y}}{s_y}
\end{eqnarray*}

and conveniently,

\begin{eqnarray*}
\hat{\beta}_1 = \frac{s_y}{s_x}\cdot r
\end{eqnarray*}

Once you know $\hat{\beta}_1$, you can find the intercept ($\hat{\beta}_0$) by plugging in $(\bar{x}, \bar{y})$-- a point that is always on the line. 

\begin{eqnarray*}
y-\bar{y}=\hat{\beta_1}(x -\bar{x})
\end{eqnarray*}

\paragraph{Example: Poverty and Education}

Is there an association between poverty and education among states? The following plot illustrates the relationship between the \emph{poverty rate} and the \emph{high school graduation rate} among all 50 states and the District of Columbia. 

<<eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.show='hide', size='footnotesize'>>=
require(mosaic)
poverty <- read.csv("http://math.smith.edu/~bbaumer/mth241/poverty.txt", sep = "\t")
qplot(data = poverty, x = Graduates, y = Poverty, xlab = "Graduation Rate", ylab = "Poverty Rate") + 
  geom_smooth(method = "lm", se = 0)
@

Use the following summary statistics to calculate the least squares regression line. 

<<size='footnotesize'>>=
favstats(~Poverty, data = poverty)
favstats(~Graduates, data = poverty)
cor(Poverty ~ Graduates, data = poverty)
@


\begin{itemize}
  \itemsep0.5in
  \item Slope:
  \item Intercept:
  \item Interpretation:
  \vspace{0.5in}
\end{itemize}


\paragraph{Measuring the Strength of Fit}

Just as we were able to quantify the strength of the linear relationship between two variables with the correlation coefficient, $r$, we can quantify the percentage of variation in the response variable ($y$) that is explained by the explanatory variables. This quantity is called the \emph{coefficient of determination} and is denoted $R^2$.

\begin{itemize}
  \itemsep0in
  \item Like any percentage, $R^2$ is always between 0 and 1
  \item For simple linear regression (one explanatory variable), $R^2 = r^2$
  \item $R^2 = 1 - SSE/SST = SSM/SST$
\end{itemize}

%\includegraphics[width=9cm]{poverty_scat}
<<fig.width=10, fig.height=4, fig.show='hide', size='footnotesize'>>=
qplot(data = poverty, x = Graduates, y = Poverty, xlab = "Graduation Rate", ylab = "Poverty Rate") + 
  geom_smooth(method = "lm", se = 0, size = 3)
mod <- lm(Poverty ~ Graduates, data = poverty)
n <- nrow(poverty)
SST <- var(~Poverty, data = poverty) * (n - 1)
SSE <- var(residuals(mod)) * (n - 1)
1 - SSE / SST
rsquared(mod)
@
\clearpage
\paragraph{RailTrail example}

Recall the RailTrail example from last time, in which we were trying to understand ridership ($volume$) in terms of temperature ($avgtemp$). We fit two models: a simple model in based strictly on the average volume, and a linear regression model for $volume$ as a function of $avgtemp$. The $R^2$ value for the second model was:

<<message=FALSE, size='footnotesize'>>=
rsquared(lm(volume ~ avgtemp, data = RailTrail))
# rsquared(lm(volume ~ 1, data=RailTrail))
@

\begin{enumerate}
  \itemsep0.5in
  \item What was the $R^2$ for the first model? Which one fit the data better? 
  \item Write a sentence interpretting the $R^2$ for the second model presented above.
  \vspace{0.5in}
\end{enumerate}


%' 
%' \newpage
%' 
%' 
% ' \newpage
% 
%  \paragraph{Solution to Fitting the Line}
% 
%  Do it by hand.
% 
%  <<fig.height=4, fig.width=10>>=
%  # summary stats to calculate slope and intercept
%  fv.p <- favstats(~Poverty, data = poverty)
%  fv.g <- favstats(~Graduates, data = poverty)
% fv.p
% fv.g
%  x.bar <- fv.g[1, "mean"]
% x.bar
%  s.x <- fv.g[1, "sd"]
% s.x
%  y.bar <- fv.p[1, "mean"]
% y.bar
%  s.y <- fv.p[1, "sd"]
% s.y
%  @
% 
%  <<fig.height=4, fig.width=10, fig.show='hold'>>=
%  xyplot(Poverty ~ Graduates, data = poverty, ylab = "% in poverty", xlab = "% HS grad", pch=19)
%  ladd(panel.abline(v = x.bar, lty=2))
%  ladd(panel.abline(h = y.bar, lty=2))
% 
%  R <- cor(Graduates ~ Poverty, data = poverty)
% R
%  b1 <- (s.y/s.x) * R
% b1
%  b0 <- y.bar + b1 * -x.bar
% b0
% 
%  mod <- lm(Poverty ~ Graduates, data = poverty)
%  coef(mod)
%  ladd(panel.abline(mod, lwd=2, col="goldenrod"))
%  @
% 
% 
%  <<>>=
%  binmod <- lm(volume ~ weekday, data = RailTrail)
%  rsquared(binmod)
%  plotModel(binmod)
%  @
% 
% End of notes

\end{document}
