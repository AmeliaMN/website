```{r, message=FALSE, warning=FALSE, echo=FALSE}
library(knitr)
opts_chunk$set(eval=TRUE)
```

```{r, message=FALSE}
library(mosaic)
library(mosaicData)
library(lmtest)
cols = trellis.par.get()$superpose.symbol$col
```

#### Two-Way Tables and Chi-Squared Tests

Recall the logistic model for $Acceptance$ to medical school that we built previously, based on $Sex$. This is a simple logistic regression model in which **both** the response and explanatory variable are binary. 

```{r}
MedGPA = read.csv("http://www.math.smith.edu/~bbaumer/mth247/MedGPA.csv")
logm = glm(Acceptance ~ Sex, data=MedGPA, family=binomial)
summary(logm)
```

As we can see, the $p$-value for the coefficient of $Sex$ is not significant at the 10% level. 

Since both variables in this case are binary, an essentially equivalent model is to simply compute a 2-way table. 

```{r}
two.way = tally(~Acceptance | Sex, data=MedGPA, format="count")
two.way
```

Note that the logistic model only has two possible imputs (and therefore two possible outputs), and that they match the entries in the 2-way table.

```{r}
fit.accept = makeFun(logm)
fit.accept(Sex="M")
fit.accept(Sex="F")
```

Furthermore, the odds ratio from the 2-way table is the same as the odds ratio from the logistic model. 

```{r}
oddsRatio(two.way)
# Since the coefficients is negative, we add a negative here to match the 2-way table
exp(-coef(logm))
```

When we built the logistic regression model, we obtained a $p$-value for the coefficient of $Sex$. This was helpful, because it gave us some information about the extent of the statistical significance of the relationship between $Sex$ and $Acceptance$. Can we obtain something similar from the 2-way table? Indeed, the $\chi^2$-test gives essentially the same information. 

```{r}
chisq.test(two.way[1:2,1:2], correct=FALSE)
```

#### Classification

We already learned how to compare models using the deviance, but how do we know how well our model works? One techinque for assessing the goodness-of-fit in a logistic regression model is to examine the percentage of the time that our model was "right."

Recall the logistic model that we built for $isAlive$ as a function of $age$ and $smoker$. 

```{r}
Whickham = Whickham %>%
  mutate(isAlive = 2 - as.numeric(outcome))
logm = glm(isAlive ~ age + smoker, data=Whickham, family=binomial(logit))
summary(logm)
```

When we visualize the model, it's hard to assess how often our model is correct. 

```{r, fig.width=10}
plotPoints(jitter(isAlive) ~ age, groups=smoker, data=Whickham, alpha=0.3, pch=19, cex=2,
           ylab="Probability of Being Alive (units)",
           xlab="Age (years)", main="Whickham Study Outcomes",
           sub=paste("Number of Cases = ", nrow(Whickham)),
           auto.key=TRUE)
fit.outcome = makeFun(logm)
plotFun(fit.outcome(age=x, smoker="Yes") ~ x, add=TRUE)
plotFun(fit.outcome(age=x, smoker="No") ~ x, col=cols[2], add=TRUE)
```

Moreover, what does it even mean to be correct? The response variable is binary, but the predictions generated by the model are quantities on $[0,1]$. A simple way to **classify** the fitted values of our model is to simply round them off. Once we do this, we can tabulate how often the rounded-off probability from the model agrees with the actual response variable. 

```{r}
Whickham = Whickham %>%
  mutate(fit.alive = ifelse(logm$fitted.values >= 0.5, 1, 0))
tally(~isAlive | fit.alive, data=Whickham)
tbl = tally(~isAlive | fit.alive, data=Whickham, format="count")
sum(diag(tbl)) / nrow(Whickham)
```

Thus, our model was correct nearly 84% of the time. Is that good? It depends on the application...

We might want to compare this with what would happen if we used the null model, or if we had just flipped a coin instead of using a model at all.

```{r}
mean(~isAlive, data=Whickham)
Whickham = Whickham %>%
   mutate(fit.alive = sample(c(0,1), size=1314, replace=TRUE))
tally(~isAlive | fit.alive, data=Whickham)
```

#### Concordance
A more sophisticated technique for assessing the quality of fit is to examine the $C$-statistic, where $C$ stands for **concordance**. The idea here is to pair up each actual success with each actual failure, and then compute whether the fitted probability for the success was greater than the fitted probability for the failure. The percentage of pairs statisfying this condition makes up the $C$-statistic. 

Note that even if our model had no predictive power, it would still be right about half of the time. Thus, the $C$-statistic for two randomly generated vectors is about 0.5. 

```{r}
X = data.frame(a = runif(10000), b = runif(10000))
library(Hmisc)
rcorrcens(a~b, data=X)
```

In this case our model is much more effective.

```{r}
rcorrcens(isAlive ~ fitted.values(logm), data=Whickham)
```



